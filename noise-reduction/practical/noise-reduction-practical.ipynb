{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn to create a denoising autoencoder. We will be working with the [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist) image dataset. Specifically, we will corrupt the images with a noise, and train a DAE to output a uncorrupted image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "We will need the following libraries:\n",
    "\n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. torch\n",
    "4. torchvision\n",
    "\n",
    "Please follow the instructions [here](https://pytorch.org/) to install the last two libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "PyTorch has simple-to-use functions that downloads and loads the datasets. We will use these functions to streamline our deep learning pipeline.\n",
    "\n",
    "Checkout other image datasets at [torch.datasets](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data in data folder. It will create this folder if it doesn't exist\n",
    "torchvision.datasets.FashionMNIST(root=\"./data/\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The data has already been split into training and testing folders. We will make use of training dataset to train our DAE, and we will use test dataset in the end to check the quality of trained dataset. This way we will be able to compare the model's generalization performance on unseen dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = torchvision.datasets.FashionMNIST(root=\"./data/\", train=True) # only load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the data look like?\n",
    "\n",
    "- What is the type of data?\n",
    "- What does each element of data represent?\n",
    "- What are the constituent parts of each elements?\n",
    "- How is the image represented?\n",
    "- What do we use to plot image?\n",
    "- How do we use the image in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of data is the class\\n\", type(data))\n",
    "print(\"Each element of the data is\\n\", type(data[0]))\n",
    "print(\"A single element is\\n\", data[0],\"first element is the image and the second element is the category\")\n",
    "\n",
    "# what else do you want to know about data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data\n",
    "\n",
    "We know that the image data is present in `PIL.Image.Image` format. We need to convert it to array of pixel values to operate on it. We will also be displaying these images. `matplotlib.pyplot` has a simple function `plt.imshow()` to display matrix as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(data[0][0])\n",
    "print(f\"Raw image to tensor shape: {img.shape}\")\n",
    "\n",
    "# To plot this, we need a 2D array, so we use squeeze_(0) to remove first dimension\n",
    "img = img.squeeze_(0)\n",
    "print(f\"2D image tensor shape: {img.shape}\")\n",
    "\n",
    "# how to display the image \n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corruption model\n",
    "\n",
    "We add noise according to a model. There are several models that can be used. We will be playing with the simple ones in this tutorial.\n",
    "\n",
    "You are free to choose a corruption model, however, for the purpose of this tutorial we will consider the following models \n",
    "1. **gaussian**: add gaussian noise to the image. Checkout `np.random.normal` to do this\n",
    "2. **speckle**: randomly remove a pixel from the image. Checkout `np.random.binomial` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_img(x, noise_model=\"gaussian\"):\n",
    "    \"\"\"\n",
    "    Adds noise to the input\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): input to be corrupted\n",
    "        noise_mode (str): noise model for corruption\n",
    "    \n",
    "    Returns:\n",
    "        (np.array): x corrupted with the noise model \n",
    "    \"\"\"\n",
    "    dtype = x.dtype\n",
    "    if noise_model == \"gaussian\":\n",
    "        # add your noise model\n",
    "        \n",
    "    if noise_model == \"speckle\":\n",
    "        # Add your noise model\n",
    "        \n",
    "    \n",
    "    return x.to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the corrupted images look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "n_samples = len(data)\n",
    "n_vis = 16\n",
    "noise_model = \"speckle\" # \n",
    "\n",
    "nrows = 4\n",
    "ncols = math.ceil(n_vis/nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,6), dpi=100)\n",
    "\n",
    "idxs = np.random.randint(low=0, high=n_samples, size=n_vis)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    ax = axs[i%nrows, i//ncols]\n",
    "    img = transforms.ToTensor()(data[idx][0]).squeeze_(0)\n",
    "    corrupted_img = corrupt_img(img, noise_model)\n",
    "    concat_img = np.concatenate((img, np.ones((img.shape[0], 10)) ,corrupted_img), axis=1)\n",
    "    ax.imshow(concat_img)\n",
    "    ax.axis('off')\n",
    "\n",
    "_ = fig.suptitle(f\"Original FashionMNIST images & {noise_model} corrupted images  \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building our model in PyTorch. This is a standard way to define a model in PyTorch. `nn.Module` is a super class that contains all the functions necessary to build a computation graph and perform backward pass on it. \n",
    "\n",
    "Here we divide DAE into an encoder and decoder. While an encoder learns a lower dimensional representation of the corrupted input, the decoder learns to map it to the original input. These lower dimensional representations are hypothesized to be robust so that different noise on the same input should be mapped to the same point in this space, thereby being decoded to the original input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you choose as layers in your encoder or decoder? \n",
    "\n",
    "Remember that we can always start simple by assuming 28 x 28 image matrix as 784 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising autoencoder; it is conceptually same as a deep autoencoder\n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(DAE, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.in_dim = input_shape[0] * input_shape[1]\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            ## ADD LAYERS \n",
    "            ## Last layer's output will be an input to self.decoder\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            ## ADD LAYERS\n",
    "            ## last layer's output should be in the same range as image's values. \n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input = x.reshape(-1, self.in_dim)\n",
    "        return self.decoder(self.encoder(input)).reshape(-1, self.input_shape[0], self.input_shape[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "PyTorch uses `torch.utils.data.Dataset` class to load data in parallel on multiple CPUs. It enables faster loading of the batch of data. It then uses `torch.utils.data.DataLoader` to combine these loaded data points together into a batch. These batches are then used as an input to the models.\n",
    "\n",
    "For this reason, we need to customize `__len__` and `__getitem__` functions in the class. This will help us do any necessary preprocessing on the images before using them as an input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, noise_model=\"gaussian\"):\n",
    "        self.data = data \n",
    "        self.noise_model = noise_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = transforms.ToTensor()(self.data[index][0]).squeeze_(0)\n",
    "        corrupted_img = corrupt_img(img, self.noise_model)\n",
    "        return (img, corrupted_img)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "We split the training dataset into train and validation. This will help us tune hyperparameters of the model. Note that hyperparameter tuning is not done in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and val\n",
    "x_train, x_val = torch.utils.data.random_split(data, [54000, 6000])\n",
    "val_data = torch.utils.data.DataLoader(Dataset(x_val), batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer \n",
    "\n",
    "We will use the standard Adam optimzer to learn the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your optimizer. It is just a template. we will define it again later just before startig the training.\n",
    "model = DAE(input_shape)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss \n",
    "\n",
    "We will create a function that will take following inputs:\n",
    "\n",
    "1. DAE instance\n",
    "2. `torch.utils.data.DataLoader` instance \n",
    "3. optimizer (optional)\n",
    "\n",
    "This function returns the mean loss on this data. \n",
    "\n",
    "Complete the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device will be defined below.\n",
    "def process(model, data, optimizer=None):\n",
    "    n_samples = 0\n",
    "    running_loss = 0\n",
    "    for img, corrupted_img in data:\n",
    "        # transfer to GPU if avaiable\n",
    "        img = img.to(device)\n",
    "        corrupted_img = corrupted_img.to(device)\n",
    "\n",
    "        n_samples += img.shape[0]\n",
    "        \n",
    "        # forward pass\n",
    "        ## add your code here to\n",
    "        # compute forward pass \n",
    "        # compute loss checkout https://pytorch.org/docs/stable/nn.html#loss-functions for functions to use\n",
    "        \n",
    "        # backward pass \n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            # Add your code here\n",
    "            # perform backward pass on loss\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / n_samples\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Finally, we will be running the training for `n_epochs` number of epochs. \n",
    "Each epoch consists of doing a backward pass on the subset of training data and evaluating the model on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "# fix seed for reproducibility \n",
    "rng = np.random.RandomState(1)\n",
    "torch.manual_seed(rng.randint(np.iinfo(int).max))\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU -> using CPU:\", device)\n",
    "\n",
    "# create a model directory to store the best model\n",
    "model_dir = pathlib.Path(\"./models\").resolve()\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir()\n",
    "\n",
    "model = DAE(input_shape).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "epoch_size=200\n",
    "batch_size=128\n",
    "\n",
    "best_val_loss = np.inf\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "n_epochs = 30\n",
    "no_improvement_cnt = 0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"@ epoch {epoch}\", end=\"\")\n",
    "    \n",
    "    # training loss\n",
    "    idxs = rng.choice(len(x_train), epoch_size * batch_size, replace=True)\n",
    "    train_data = torch.utils.data.DataLoader(Dataset([x_train[idx] for idx in idxs]), batch_size=batch_size, num_workers=4)\n",
    "    train_loss = process(model, train_data, optimizer)\n",
    "    \n",
    "    # validation loss\n",
    "    with torch.no_grad():\n",
    "        val_loss = process(model, val_data)\n",
    "    \n",
    "    # save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_dir / \"best.ckpt\")\n",
    "    else:\n",
    "        \n",
    "        # if there has been no improvement in validation loss, stop early\n",
    "        no_improvement_cnt += 1\n",
    "        \n",
    "        if no_improvement_cnt % 10 == 0:\n",
    "            print(\"\\nEarly stopping!\")\n",
    "            break\n",
    "        \n",
    "    # logging\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"\\ttrain_loss: {train_loss: .5f}, val_loss: {val_loss:.5f}\")\n",
    "\n",
    "print(f\"best val loss: {best_val_loss:.5f}\")\n",
    "\n",
    "# load the best model\n",
    "model = DAE(input_shape)\n",
    "model.load_state_dict(torch.load(model_dir / \"best.ckpt\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress\n",
    " \n",
    "Let's look at how the losses varied over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=100)\n",
    "\n",
    "axs.plot(train_losses, color=\"#BDD9BF\", marker=\"x\", label=\"Train loss\")\n",
    "axs.plot(val_losses, color=\"#A997DF\", marker=\"o\", label=\"Val loss\")\n",
    "axs.set_xlabel(\"Epochs\", fontsize=20)\n",
    "axs.legend(prop={\"size\":15})\n",
    "\n",
    "# tick size\n",
    "for tick in axs.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(15)\n",
    "\n",
    "for tick in axs.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluation\n",
    "\n",
    "Finally, we will evaluate the model on the test dataset. Here we look at the reconstruction loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test losses\n",
    "test_data = torchvision.datasets.FashionMNIST(root=\"./data/\", train=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(Dataset(test_data), batch_size=4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = process(model, test_dataloader)\n",
    "print(f\"Reconstruction loss on test dataset: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed images\n",
    "\n",
    "Let's look at how well our model is able to reconstruct the corrupted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reconstructed images\n",
    "model = model.cpu()\n",
    "n_samples = len(test_data)\n",
    "n_vis = 9\n",
    "\n",
    "nrows = 3\n",
    "ncols = math.ceil(n_vis/nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,8), dpi=100)\n",
    "\n",
    "idxs = np.random.randint(low=0, high=n_samples, size=n_vis)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    ax = axs[i%nrows, i//ncols]\n",
    "    \n",
    "    img = transforms.ToTensor()(test_data[idx][0]).squeeze_(0)\n",
    "    \n",
    "    corrupted_img = corrupt_img(img, noise_model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img = model(corrupted_img.view(-1, *corrupted_img.shape)).squeeze_(0)\n",
    "    \n",
    "    filler =  np.ones((img.shape[0], 10))\n",
    "    concat_img = np.concatenate((img, filler ,corrupted_img, filler, reconstructed_img), axis=1)\n",
    "    \n",
    "    ax.imshow(concat_img)\n",
    "    ax.axis('off')\n",
    "\n",
    "_ = fig.suptitle(f\"Original FashionMNIST & {noise_model} corrupted & recostructed images \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkenv",
   "language": "python",
   "name": "dkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
