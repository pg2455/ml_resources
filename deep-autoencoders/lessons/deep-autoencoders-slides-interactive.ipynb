{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders: Motivation and History \n",
    "\n",
    "* Unsupervised learning:\n",
    "    - uses **only** the inputs $\\mathbf{x}_i$ for learning\n",
    "    - automatically learns *meaningful* features for data (a.k.a **representation learning**)\n",
    "    - makes the best use of unlabeled data (a.k.a **semi-supervised learning**)\n",
    "    - models data generating distribution (a.k.a **Generative modeling**)\n",
    "\n",
    "\n",
    "* Autoencoders are a feedforward network trained to reconstruct its input at the output layer\n",
    "\n",
    "\n",
    "* History:\n",
    "    - Restricted Boltzman Machines (RBM) were quite popular neural networks prior to autoencoders in 2008\n",
    "    - RBMs consist of an input layer (raw input) and a hidden layer (representation to be learned), with the transformation weights from input to hidden layer learned via stochastic sampling methods like MCMC\n",
    "    - Pre-trained weights of RBMs were used to intialize autoencoders' weights, which were then optimized via backpropagation\n",
    "    - *Status Quo*: Autoencoders are fully trained using backpropagation.\n",
    "    \n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\">RBMs</th>\n",
    "        <th style=\"text-align:center\">Autoencoders</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"img/rbm.png\" width=\"500px\"><br><a href=\"https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-1-introduction-model-formulation-1a4404873b3\" target=\"_blank\">(source)</a></td>\n",
    "        <td><img src=\"img/ae1.png\" width=\"500px\"><br><a href=\"https://towardsdatascience.com/unsupervised-learning-part-2-b1c130b8815d\" target=\"_blank\">source</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "* It is a feed-forward network with input $\\mathbf{x}$ and output $\\hat{\\mathbf{x}} \\approx \\mathbf{x}$\n",
    "\n",
    "\n",
    "* We will use $f_{\\theta}: \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_{latent}}$ to denote the **encoder** that maps input to a lower dimension space, and\n",
    "\n",
    "\n",
    "* We will use $g_{\\phi}: \\mathbb{R}^{d_{latent}} \\rightarrow \\mathbb{R}^{d_{in}}$ to denote the **decoder** that reconstructs the *compressed representation of the input in the latent space* back to the input\n",
    "\n",
    "\n",
    "* A typical example of $f_{\\theta}$ (or $g_{\\phi}$) implementing a 1-layer feedforward network is $f_{\\theta} = a(\\mathbf{x}\\mathbf{\\theta})$, where $\\theta \\in \\mathbb{R}^{d_{in} \\times d_{latent}}$ , and $a(.)$ is an activation function of choice\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/ae.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders: Loss function\n",
    "\n",
    "* The objective is to have reconstructed input $\\hat{\\mathbf{x}}_i$ representative of the input $\\mathbf{x}_i$, so we want to maximize conditional data likelihood, i.e.,  \n",
    "\n",
    "$$\\max_{\\theta, \\phi}\\Pi_{i=1}^{N}p(\\mathbf{x}_i | \\hat{\\mathbf{x}}_i)$$\n",
    "\n",
    "\n",
    "* In the standard form, we want to minimize negative log likelihood, i.e., \n",
    "$$\\min_{\\theta, \\phi}-\\sum_{i=1}^{N}\\log p(\\mathbf{x}_i | \\hat{\\mathbf{x}}_i)$$\n",
    "\n",
    "\n",
    "* Therefore, the loss function is $l(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) = -\\log p(\\mathbf{x}_i | \\hat{\\mathbf{x}}_i)$\n",
    "\n",
    "\n",
    "\n",
    "* Depending on the input data type, we can simplify the above loss functions\n",
    "\n",
    "    - if $\\mathbf{x} \\in \\mathcal{R}^{d_{in}}$, we assume a Gaussian model, i.e, $p(\\mathbf{x}_i | \\hat{\\mathbf{x}}_i) \\sim \\mathcal{N}(\\hat{\\mathbf{x}}_i | \\sigma^2 I)$ yielding the **reconstruction loss as $l2$-norm**\n",
    "    $$ l(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) = || \\hat{\\mathbf{x}}_i - \\mathbf{x}_i ||_2^2 $$\n",
    "    \n",
    "    - if $\\mathbf{x} \\in \\{0, 1\\}^{d_{in}}$, we assume a Bernoulli model, i.e, $ p(\\mathbf{x}_i | \\hat{\\mathbf{x}}_i) \\sim \\mathcal{B}(\\hat{\\mathbf{x}}_i) $ yielding the reconstruction loss as the cross entropy loss\n",
    "    \n",
    "     $$ l(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) = -\\sum_{k=1}^{d_{in}} \\mathbf{x}_{i,k} \\log \\hat{\\mathbf{x}}_{i,k} + (1- \\mathbf{x}_{i,k}) \\log (1 - \\hat{\\mathbf{x}}_{i,k}) $$\n",
    "     \n",
    "    - You can adapt to the type of input you have\n",
    "\n",
    "\n",
    "* The parameters $\\theta$ and $\\phi$ are learned by minimizing the reconstruction loss function $\\mathcal{L}$\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} l(\\mathbf{x_i}, g_{\\phi}(f_{\\theta}(\\mathbf{x_i}))) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undercomplete vs Overcomplete Autoencoder\n",
    "\n",
    "* Depending on the dimension of the latent space, an autoconder could be *undercomplete* or *overcomplete*\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\">Undecomplete ($d_{latent} < d_{in}$)</th>\n",
    "        <th style=\"text-align:center\">Overcomplete ($d_{latent} > d_{in}$)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"img/uc_ae.png\" width=\"500px\"> </td>\n",
    "        <td><img src=\"img/oc_ae.png\" width=\"500px\"> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/unsupervised-learning-part-2-b1c130b8815d\" target=\"_blank\">(image source)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undercomplete Autoencoder\n",
    "\n",
    "- $d_{latent} < d_{in}$\n",
    "\n",
    "- doesn't allow neural networks to learn an identity function, thereby discouraging it to memorize input\n",
    "\n",
    "- encoder performs a lossy compression\n",
    "\n",
    "- useful to learn **most important features** or compressed representation of the training data\n",
    "\n",
    "- not useful to represent variations outside of training data (e.g. translations or distortions in images)\n",
    "\n",
    "<img src=\"img/uc_ae.png\" width=\"250px\">\n",
    "\n",
    "* (optional) Linear undercomplete autoencoder\n",
    "\n",
    "    - Linear autoencoder i.e. $f_{\\theta}(\\mathbf{x}) = \\theta \\mathbf{x}$ and $g_{\\phi}(\\mathbf{x}) = \\phi \\mathbf{x}$ with *normalized inputs* and *$l2$-norm as loss* is same as PCA\n",
    "    - Thus, a nonlinear autoencoder is a powerful nonlinear generalization of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcomplete Autoencoder\n",
    "\n",
    "* $d_{latent} > d_{in}$\n",
    "\n",
    "* There are no guarantees that the autoencoder will extract meaningful features unless different *regularization techniques* are used to encourage this\n",
    "\n",
    "* There are two ways to regularize these autoencoders\n",
    "    - Implicit regularization: Loss function is left unchanged e.g. Denoising Autoencoder\n",
    "    - Explicity regularization: Loss function is augmented with a penalty term, e.g., Sparse Autoencoders, Contractive Autoencoder\n",
    "\n",
    "<img src=\"img/oc_ae.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcomplete Autoencoder:  Implicit regularization\n",
    "\n",
    "* **Denoising Autoencoder (DAE)** (refer Module 2, L08 for detailed description)\n",
    "    - Corrupts the input through a noise process \n",
    "    - The task is to reconstruct the uncorrupted input, which is enforced through an appropriate loss function (e.g, l2-norm or cross-entropy loss)\n",
    "    - Encourages the **extraction of higher level features** of the input in the hidden layer\n",
    "    - By enforcing it to reconstruct the original input, the autoencoder is discouraged to learn an identity function\n",
    "    - For example, in the figure below, \n",
    "        - the original input $X$ is corrupted to $\\hat{X}$ such that the feature $X_2$ is dropped out in $\\hat{X}$\n",
    "        - the autoencoder is trained to minimize the reconstruction loss defined between the output $Y$ and the original input $X$\n",
    "        - thus, the autoencoder would need to learn the relation of $\\hat{X}_2$ with other features $\\hat{X}_1, \\hat{X}_3, \\hat{X}_4$\n",
    "    \n",
    "\n",
    "<img src=\"img/denoising_ae.png\" width=\"500px\">\n",
    "<a href=\"https://www.udemy.com/course/deeplearning/\" target=\"_blank\">(image source)</a>\n",
    "\n",
    "[[1] Stacked Denoising Autoencoder](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcomplete Autoencoder: Explicit Regularization\n",
    "\n",
    "* **Sparse Autoencoder**\n",
    "    - Explicitly penalizes hidden layer activations to encourage sprase representation of the input\n",
    "    - Encourages **learning unique statistical features** of the training data by allowing only a few hidden layer neurons to be active at any time\n",
    "\n",
    "<img src=\"img/sparse_ae.png\" width=\"250px\">\n",
    "<a href=\"https://www.wikiwand.com/en/Autoencoder\" target=\"_blank\">(image source)</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcomplete Autoencoder: Explicit Regularization\n",
    "\n",
    "* **Sparse Autoencoder**\n",
    "    - **L1 penalty** on hidden layer activations: The augemented loss function can be written as\n",
    "        $$ l_{aug}(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) = l(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) + \\lambda ||h(\\mathbf{x}_i)||_1 $$\n",
    "        \n",
    "        where $h_i = h(\\mathbf{x}_i) = f_{\\theta}(\\mathbf{x}_i)$ is the latent represetnation of the input $\\mathbf{x}_i$\n",
    "\n",
    "    - **Average activation** of the hidden layer activations *across the training samples*: Let $\\rho_j$ be the average magnitude of hidden layer activations across the training samples. To keep most of the hidden layer neurons inactive, the loss function is imposed with a constraint $\\rho_j = \\rho$, where $\\rho$ is the sparsity hyperparameter\n",
    "        $$ \\hat{\\rho}_j = \\frac{1}{N}\\sum_{i=1}^{N} h(\\mathbf{x}_i)_j $$\n",
    "        \n",
    "        $$\\mathcal{L}_{aug} = \\mathcal{L} + \\sum_{j=1}^{k}KL(\\rho || \\hat{\\rho}_j) = \\mathcal{L} + \\sum_{j=1}^{k} \\Big[\\rho \\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j} \\Big]$$\n",
    "        \n",
    "        where $k$ is the number of hidden layer neurons\n",
    "\n",
    "[[1] Why Regularized Auto-Encoders learn Sparse Representation?](https://arxiv.org/pdf/1505.05561.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcomplete Autoencoder: Explicit Regularization\n",
    "\n",
    "* **Contractive Autoencoder (CAE)**\n",
    "    - Explicitly penalizes gradient of hidden layer activations w.r.t input features\n",
    "    - Encourages **robustness against small perturbations** to the input\n",
    "    - (optional) CAEs are connected to DAEs in the limit of small Gaussian noise. While DAEs learn the robust manifold to project the corrupted input back to its original form, CAEs' extracted features are unaffected to this level of noise\n",
    "    - The augmented loss function can be written as \n",
    "    \n",
    "    $$l_{aug}(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) = l(\\mathbf{x}_i, \\hat{\\mathbf{x}}_i) + \\lambda ||\\nabla_{\\mathbf{x}_i} h(\\mathbf{x}_i)||_F$$\n",
    "\n",
    "    where $F$ represents the Frobenius norm. Note that $\\nabla_{\\mathbf{x}_i} h(\\mathbf{x}_i)$ is the Jacobian matrix representing partial derivatives of *each of the hidden layer activations w.r.t each of the input features*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep / Stacked Autoencoders \n",
    "\n",
    "* In early days, training autoencoders was a non-trivial task. Therefore, Autoencoders were limited to single layer encoders and decoders\n",
    "\n",
    "\n",
    "* The training regime for multi-layered autoencoders was done via **layer-wise training** with the help of RBMs, thereby landing them the name **Stacked Autoencoders** or **Deep Autoencoders**\n",
    "\n",
    "\n",
    "* However, with the recent advances in deep learning like activation functions, normalization layers, etc., training a deep autoencoder has been easy\n",
    "\n",
    "\n",
    "<img src=\"img/deep_ae.png\" width=\"500px\">\n",
    "<a href=\"https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\" target=\"_blank\">(image source)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE)\n",
    "\n",
    "* AEs are used for **data compression** \n",
    "    - latent representations do not have a special spatial meaning to them\n",
    "    \n",
    "    - Sampling a random point in the latent space do not have a meaning unless it is obtained from a point in the original input space\n",
    "    \n",
    "    - The figure shows the clusters obtained by AEs on MNIST dataset. It is evident that the latent encodings do not span the entire subspace, thereby loosing the meaning of interpolation between points (1 and 7 in the illustration)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\">AE on MNIST</th>\n",
    "        <th style=\"text-align:center\">VAE on MNIST</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"img/mnist_ae.png\" width=\"500px\"></td>\n",
    "        <td><img src=\"img/mnist_vae.png\" width=\"500px\"></td>      \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\" target=\"_blank\">(image source)</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE)\n",
    "\n",
    "* Recall that decoder takes a point in the latent space to reconstruct the input\n",
    "\n",
    "\n",
    "* Thus, decoder can be used in isolation to generate new datapoints in the original input space\n",
    "\n",
    "\n",
    "* This requires us to sample points in the latent space\n",
    "\n",
    "\n",
    "* VAEs impose a spatial structure to the latent space, thereby improving **data generation** and **data interpolation** aspect of AEs\n",
    "\n",
    "\n",
    "[[Reference] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE):  Modification 1/2\n",
    "\n",
    "* VAEs impose a spatial structure to the latent space, thereby improving **data generation** and **data interpolation** aspect of AEs\n",
    "\n",
    "\n",
    "* This is done using two modifications to the original AE \n",
    "\n",
    "    - **Sampling the point in the latent space** instead of using the deterministic encodings of an AE\n",
    "    \n",
    "    - Encoder outputs mean $\\mathbb{\\mu} \\in \\mathcal{R}^{d_{latent}}$ and sigma $\\mathbb{\\sigma} \\in \\mathcal{R}^{d_{latent}}$ of a multivariate Gaussian $\\mathcal{N}(\\mathbb{\\mu}, \\mathbb{\\sigma}^2\\mathbf{I})$\n",
    "    \n",
    "    - Decoder takes in as its input a point sampled as per the distribution defined by $\\mathbb{\\mu}$ and $\\mathbb{\\sigma}$\n",
    "    \n",
    "    - Therefore, even though the same input is encoded to the same $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$, decoder sees it as different points in the latent space\n",
    "\n",
    "    - This aspect encourages the decoder to interpret similarity in points surrounding the encoded means, thereby imoroving the **data generation** capability of the VAEs\n",
    "\n",
    "\n",
    "<img src=\"img/sample_ae.jpeg\" width=\"750px\">\n",
    "<a href=\"blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae\" target=\"_blank\">(image source)</a>\n",
    "\n",
    "[[Reference] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE):  Modification 1/2\n",
    "\n",
    "* VAEs impose a spatial structure to the latent space, thereby improving **data generation** and **data interpolation** aspect of AEs\n",
    "\n",
    "\n",
    "* This is done using two modifications to the original AE \n",
    "\n",
    "    - **Sampling the point in the latent space** instead of using the deterministic encodings of an AE\n",
    "    \n",
    "\n",
    "\n",
    "<table style=\"width:500px\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 100px; height:50px\"><strong>Encode</strong></td>\n",
    "        <td style=\"text-align:left\">$\\mu_i = f_{\\theta_1}(\\mathbf{x}_i) \\qquad \\sigma_i = f_{\\theta_2}(\\mathbf{x}_i)$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 100px; height:50px\"><strong>Sample</strong></td>\n",
    "        <td style=\"text-align:left\">$ h(\\mathbf{x}_i) = \\mu_i + \\sigma_i^2 \\times \\mathcal{N}(0, I) $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 100px; height:50px\"><strong>Decode</strong></td>\n",
    "        <td style=\"text-align:left\">$ \\hat{\\mathbf{x}} = g_{\\phi}(h(\\mathbf{x}_i)) $</td>    \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "[[Reference] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE): Modification 2/2\n",
    "\n",
    "* VAEs impose a spatial structure to the latent space, thereby improving **data generation** and **data interpolation** aspect of AEs\n",
    "\n",
    "\n",
    "* This is done using two modifications to the original AE \n",
    "  \n",
    "    - **Imposing the structure to the sampled distribution** via KL divergence \n",
    "    \n",
    "    - Without such penalty, an AE could learn $\\mu$ and $\\sigma$ so far apart that it looses the meaning of points in between those means\n",
    "    \n",
    "    - This results in the problem similar to AEs where latent points in between two clusters loose their meaning\n",
    "    \n",
    "    - Thus, we enforce a normal Gaussian prior $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ on sampled $\\mu$ and $\\sigma$\n",
    "    \n",
    "    - This encourages AE to distribute its latent representation around the origin in the latent space, thereby improving the **data interpolation** aspect of VAEs\n",
    "\n",
    "\n",
    "[[Reference] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoders (VAE): Modification 2/2\n",
    "\n",
    "* VAEs impose a spatial structure to the latent space, thereby improving **data generation** and **data interpolation** aspect of AEs\n",
    "\n",
    "\n",
    "* This is done using two modifications to the original AE \n",
    "  \n",
    "    - **Imposing the structure on the sampled distribution** via KL divergence \n",
    "    \n",
    "    - KL divergence between $q(z) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and $p(z) \\sim \\mathcal{N}(0, 1)$ has a simple form derived in the next slide\n",
    "    \n",
    "    $$ \\mathcal{L}_{VAE} = \\mathcal{L} + \\sum_{i=1}^{N} KL (\\mathcal{N}(\\mu_i, diag(\\sigma_i^2)) || \\mathcal{N}(\\mathbf{0}, \\mathbf{I})) $$\n",
    "    \n",
    "       $$ \\mathcal{L}_{VAE} = \\mathcal{L} + -\\frac{1}{2}\\sum_{i=1}^{N} \\sum_{k=1}^{d_{latent}}\\big(\\log \\sigma_{i,k}^2  +  - (\\mu_{i,k}^2 + \\sigma_{i,k}^2) + 1 \\big) $$ \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    " We will implement the above in our practical session\n",
    " \n",
    " [[Reference] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL(q || p) derivation (optional) \n",
    "\n",
    "Let $q(z) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and $p(z) \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "$$ \\log q(z) \\quad= -\\frac{1}{2}\\log 2\\pi \\sigma^2 - \\frac{1}{2}\\Big(\\frac{z - \\mu}{\\sigma}\\Big)^2$$\n",
    "\n",
    "$$ \\log \\frac{q}{p} \\quad= \\log q - \\log p \\quad= -\\frac{1}{2}\\log 2\\pi \\sigma^2 - \\frac{1}{2}\\Big(\\frac{z - \\mu}{\\sigma}\\Big)^2 + \\frac{1}{2} \\log 2\\pi + \\frac{1}{2}z^2 \\quad= -\\frac{1}{2}\\log \\sigma^2 + \\frac{1}{2} z^2 - \\frac{1}{2}\\Big(\\frac{z-\\mu}{\\sigma}\\Big)^2$$\n",
    "    \n",
    "    \n",
    "$$KL (q || p ) \\quad= \\int q(z) \\log\\frac{q(z)}{p(z)} dz  \\quad= -\\frac{1}{2}\\int  \\log \\sigma^2 q(z) + \\frac{1}{2} \\int z^2 q(z) - \\frac{1}{2} \\int \\Big(\\frac{z-\\mu}{\\sigma}\\Big)^2 q(z)\\quad= -\\frac{1}{2} \\big(\\log \\sigma^2  +  - (\\mu^2 + \\sigma^2) + 1 \\big)$$\n",
    "\n",
    "where the above follows from the following [identity](https://en.wikipedia.org/wiki/Variance#Definition)\n",
    "\n",
    "$$\\int z^2 q(z)dz = E_{q}[z^2] = E_q[z]^2 + Var(z) = \\mu^2 + \\sigma^2$$ \n",
    "For a multivariate independent Gaussian, we get\n",
    "\n",
    "$$ KL(q || p) = -\\frac{1}{2}\\sum_{k=1}^{d_{latent}}\\big(\\log \\sigma_k^2  +  - (\\mu_k^2 + \\sigma_k^2) + 1 \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other VAE \n",
    "\n",
    "- **$\\beta$-VAE [1]** : Adds a penalty on the KL divergence, such that $\\beta > 1$ finds efficient and disentangled latent representation to support better generalization to unseen data\n",
    "\n",
    "\n",
    "- **Vector-Quantized VAE [2]**: Encoder learns a discrete latent variable by the encoder, a more natural fit for problems like language, speech, reasoning, etc. \n",
    "\n",
    "\n",
    "- And many others...\n",
    "\n",
    "[[1] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/forum?id=Sy2fzU9gl)\n",
    "\n",
    "[[2] Neural Discrete Representation Learning](https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)\n",
    "\n",
    "[[3] Generating Diverse High-Fidelity Images with VQ-VAE-2](https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder (Applications)\n",
    "\n",
    "* dimensionality reduction \n",
    "\n",
    "* visualization\n",
    "\n",
    "* feature extraction\n",
    "\n",
    "* anomaly detection \n",
    "\n",
    "* semi-supervised learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
