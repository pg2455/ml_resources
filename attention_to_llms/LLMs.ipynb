{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866ca7e5",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f26148",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- LLMs and Transformers\n",
    "\n",
    "- Characteristics of LLMs: Emergent Properties\n",
    "\n",
    "- Pipeline of building an LLM\n",
    "\n",
    "- Engineering consideratioons "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be7407",
   "metadata": {},
   "source": [
    "### Large Language Models and Transformers\n",
    "\n",
    "- Most LLMs are **decoder-only transformers**, i.e., they autoregressively predict the next word or the set of words\n",
    "    * Use of causal masking to prevent the tokens from looking into the future\n",
    "\n",
    "- **Lots of parameters**: GPT-3 has 175B parameters\n",
    "    * 96 attention layers\n",
    "    * 96x128 dimension heads\n",
    "    * 12,288 dimensional token embeddings\n",
    "\n",
    "- **Lots of data**: GPT-3 is trained on 300B tokens\n",
    "\n",
    "- **Lots of compute**: 1024 GPUs, 34 days, costing USD4.6M\n",
    "\n",
    "- **Lots of research** and hyperparameter choices to make \n",
    "    - Normalization type\n",
    "    - Normalization position\n",
    "    - Activation function\n",
    "    - Learning rate \n",
    "    - ...\n",
    "\n",
    "- **Lots of engineering** to maximize training throughput\n",
    "    * Distributed training \n",
    "    * Memory optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bd574",
   "metadata": {},
   "source": [
    "### Characteristics of LLMs: Scaling Laws for pre-trained models\n",
    "\n",
    "- **Scaling laws**: Empirical investigation of how loss decreases with model parameters ($N$), Dataset size ($D$), and compute ($C$)\n",
    "\n",
    "    * Helps in assessing the model size given dataset size and compute available\n",
    "    * Many laws have been proposed, e.g., Kaplan et al. (2020)\n",
    "    \n",
    "    $$L(N) = \\big(\\frac{N_c}{N}\\big)^{\\alpha_N}, \\alpha_N \\sim 0.076, N_c \\sim 8.8 \\times 10^{13}$$\n",
    "    \n",
    "    $$L(D) = \\big(\\frac{D_c}{D}\\big)^{\\alpha_D}, \\alpha_D \\sim 0.095, D_c \\sim 5.4 \\times 10^{13}$$\n",
    "    \n",
    "    <img src=\"imgs/scaling-laws.png\">\n",
    "    \n",
    "    * General agreement that loss decreases with model size\n",
    "    * Other scaling laws exist as well, e.g., Hoffmann et al. (2022)\n",
    "    \n",
    "- **Caveat**: These laws are observed for decoder-only architecture\n",
    "    \n",
    "[[1]](https://arxiv.org/pdf/2001.08361) Kaplan et al. (2020) Scaling Laws for Neural Language Models\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2203.15556) Hoffmann et al. (2022) Training Compute-Optimal Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c4e9c",
   "metadata": {},
   "source": [
    "### Characteristics of LLMs: Emergent properties\n",
    "\n",
    "- Larger models exhibit properties that they weren't trained on\n",
    "    * No definitive theory as to why it happens\n",
    "\n",
    "- **Instruction tuning** [1, 2]\n",
    "    * Pre-trained language models (PLMs) are fine-tuned on smaller curated dataset with instructions\n",
    "    * All benchmark datasets of NLP are used as an input appended with task instructions\n",
    "    * Results in a smaller loss and a good generalization performance\n",
    "    \n",
    "    <img width=750 src=\"imgs/flan_schematic.png\">\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2109.01652) Wei et al. (2022) Fine-tuned language models are few-shot learners\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2210.11416) Won et al. (2022) Scaling Instruction-Finetuned Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51614d7",
   "metadata": {},
   "source": [
    "### Characteristics of LLMs: Emergent properties\n",
    "\n",
    "- **In-context learning** or **Prompt engineering** (from users' perspective) [1]: \n",
    "    * Zero-shot learning: LLMs can perform tasks that they haven't seen in the dataset\n",
    "    * Few-shot learning: LLMs can learn from the demonstrations of the task\n",
    "    \n",
    "    * This ability unlocks a new paradigm of creating machine learning models, which could have taken months to collect the appropriate data, e.g., spam classification\n",
    "\n",
    "    <img width=750 src=\"imgs/in-context-learning.png\">\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2005.14165) Brown et al. (2020) Language models are few-shot learners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327895d",
   "metadata": {},
   "source": [
    "### Characteristics of LLMs: Emergent properties\n",
    "\n",
    "- **Chain-of-thought reasoning** [1]: \n",
    "    * Complex reasoning tasks are difficult for PLMs\n",
    "    * LLMs can be prompted to reason through its answers, e.g., \"Let's think step by step\"\n",
    "    \n",
    "    <img width=750 src=\"imgs/cot.png\">\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2201.11903) Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75b41d",
   "metadata": {},
   "source": [
    "### LLMs from start to finish\n",
    "\n",
    "<img src=\"imgs/llm-outline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f0cb4",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Data Collection\n",
    "\n",
    "\n",
    "<img src=\"imgs/data-pre-processing.png\">\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a52d2a",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Data Collection\n",
    "\n",
    "\n",
    "- **Corpus selection**\n",
    "\n",
    "<img src=\"imgs/corpus.png\">\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e344be",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Data Collection\n",
    "\n",
    "\n",
    "<img src=\"imgs/data-pre-processing.png\">\n",
    "\n",
    "- **Filter** for good quality documents\n",
    "\n",
    "- **De-duplication**: Remove duplicates at sentence level and document level. \n",
    "    * Important to ensure that there is no leak between training and validation set\n",
    "\n",
    "- **Privacy redaction**: Remove any personally idenitfiable information (PII)\n",
    "\n",
    "- **Tokenization**: Converting raw text into individual tokens that are fed as a sequence into the model\n",
    "    * Several strategies have been proposed, e.g., Byte-Pair Encoding [2], WordPiece[3], Unigram[4]\n",
    "    * It plays an important role in what LLMs can learn\n",
    "    * Tokenization can be learned as well. Modern LLMs learn the tokenization, e.g., WordPiece tokenization\n",
    "    * Library: [SentencePiece](https://github.com/google/sentencepiece) [4]\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models\n",
    "\n",
    "[[2]](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) Byte-pair Encoding tokenization Tutorial\n",
    "\n",
    "[[3]](https://arxiv.org/abs/2012.15524) Song et al. (2020) Fast WordPiece Tokenization\n",
    "\n",
    "[[4]](https://arxiv.org/abs/1808.06226) Kudo et al. (2018) SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2d3d0",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Model Setup\n",
    "\n",
    "- **Transformer architecture**\n",
    "    * Encoder-only BERT [1] was one of the earliest model.\n",
    "    * Very few architectures are Encoder-decoder models, e.g., T5[2]\n",
    "    * Most of the modern LLMs are Decoder-only\n",
    "    \n",
    "- **Attention mechanism**: Memory and fast computations are the major concern here. Several works have been proposed in this line of research, e.g., FlashAttention, Performer, Sparse Attention\n",
    "    * Prefer sub-quadratic mechanism \n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1810.04805) Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "    \n",
    "    \n",
    "[[2]](https://arxiv.org/abs/1910.10683) Raffel et al. (2019) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00d620",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Model Setup\n",
    "\n",
    "- **Positional Encodings**: Words without positional encodings will be treated as sets and not sequences\n",
    "    * Several options to embed positions, e.g., Absolute position embedding, Relative position embedding, Rotary position embedding (RoPE), AliBi\n",
    "    * **Extrapolation**: The ability of LLMs to capture long-term dependencies. \n",
    "    * AliBi is the preferred method, although RoPE and T5 bias have also been shown to exhibit extrapolation\n",
    "\n",
    "- **Loss function**\n",
    "    * Language modeling is the main objective and the most commonly used \n",
    "        $$ \\mathcal{L}_{LM} = \\sum_{i=1}^{N}\\log P_{\\theta}(x_i \\mid x_{<i})$$\n",
    "    * Denoising Autoencoding:  is also used by some LLMs, e.g., T5[1]. Not easy to implement in decoder-only models\n",
    "        $$ \\mathcal{L}_{DAE} = \\log P_{\\theta}(x^{'} \\mid x - x^{'})$$\n",
    "    \n",
    "    * Mixture of Denoising: Uses $\\mathcal{L}_{LM}$, and $\\mathcal{L}_{DAE}$ with different levels of corruption, e.g., PaLM [2]\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1910.10683) Raffel et al. (2019) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2204.02311) Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b022a",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Model Setup\n",
    "\n",
    "- Other choices\n",
    "    * **Normalization**:\n",
    "        * LayerNorm was the proposed method\n",
    "        * RMSNorm [1] and DeepNorm [2] stabilizes the training \n",
    "    * **Position of normalization**: Should the normalization be before or after the feedforward network\n",
    "        * $\\text{Add & Norm}$ was proposed \n",
    "        * $\\text{FeedForward(Norm(x)) + x}$ seems to work better\n",
    "    * **Activation function**: GeLU[3] has been most commonly used activation function\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1910.07467) Zhang et al. (2019) Root Mean Square Layer Normalization\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2203.00555) Wang et al. (2022) DeepNet: Scaling Transformers to 1,000 Layers\n",
    "\n",
    "[[3]](https://arxiv.org/abs/1606.08415) Hendrycks et al. (2016) Gaussian Error Linear Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0352841",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Training\n",
    "\n",
    "- **Batch size**: \n",
    "    * Standard size: 2048 examples or 4M tokens\n",
    "    * Empirically, a schedule has been shown to stabilize the training \n",
    "\n",
    "- **Learning rate**: Warm-up followed by cosine decay\n",
    "\n",
    "- **Optimizer**: Adam, AdamW (GPT-3), Adafactor (PaLM, T5)\n",
    "\n",
    "- Other tricks to stabilize the training\n",
    "    * **Weight decay**: Most LLMs have been trained with a value of 0.1\n",
    "    * **Gradient clipping**: Don't let the absolute value of gradients to go above 1.0\n",
    "    * **Loss spikes**: Prevent these by restarting from the checkpoint where the spike occured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58109da9",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Training\n",
    "   \n",
    "- Scaling up using **3D Parallelism**\n",
    "    * **Data Parallelism**: Replicate the model parameters and optimizer states across multiple GPUs\n",
    "    * **Pipeline parallelism**: Distribute different layers of LLM over multiple GPUs\n",
    "    * **Tensor parallelism**: Decompose the tensors for multiplication over multiple GPUs\n",
    "    * All three can be used, e.g., 8x4x12 parallelism was used for training BLOOM over 384 GPUs [2]\n",
    "\n",
    "\n",
    "Libraries to support all three parallelism: [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Colossol-AI](https://github.com/hpcaitech/ColossalAI), [Alpa](https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html)\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2211.05100) BigScience Workshop (2022) BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d98daa",
   "metadata": {},
   "source": [
    "### Pre-training LLM: Training\n",
    "\n",
    "\n",
    "- **Mixed-Precision Training**\n",
    "    * 32-bit floating-point (FP32) numbers is the default option of training pre-training LMs\n",
    "    * 16-bit floating-point (FP16) reduces memory usage and communication overhead\n",
    "    * FP16 has been shown to result in loss of accuracy\n",
    "    * Brain Floating Point (BF16), with more exponent bits and fewer significant bits\n",
    "    * BF16 is widely used for pre-training LM\n",
    "    \n",
    "- GPT-4 uses **a smaller model to predict if the LLM will be trained successfully**. If not, they kill the process\n",
    "\n",
    "\n",
    "Libraries to support all three parallelism: [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Colossol-AI](https://github.com/hpcaitech/ColossalAI), [Alpa](https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html)\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2211.05100) BigScience Workshop (2022) BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52562b43",
   "metadata": {},
   "source": [
    "### Adaptation: *Instruction* Tuning (Data Collection)\n",
    "\n",
    "- Data collection\n",
    "    * Pre-defined NLP benchmarks formatted with natural language task descriptions\n",
    "    * Collect from human interactions through chat\n",
    "    * Synthetically generate using LLMs\n",
    "    * They need to be balanced so that one type of task do not dominate the dataset\n",
    "    \n",
    "- Considerations\n",
    "    * Too many examples or too few examples for a task could create problems in training LLMs\n",
    "    * Formatting of natural language task description should be considered, e.g., demonstrations for task helps, incorporating suggestions or things to avoid may hurt the performance\n",
    "    * Diversity and quality of instructions is very important\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868395da",
   "metadata": {},
   "source": [
    "### Adaptation: *Alignment* Tuning (Data Collection)\n",
    "\n",
    "- LLMs have been known to show unintended or harmful behaviors such as hallucinating, misleading, or being biased \n",
    "\n",
    "- How can we ensure that LLMs are aligned with human values? Helpful, honest, and harmless\n",
    "\n",
    "- We elicit responses from LLMs and penalize them for producing such responses\n",
    "\n",
    "- **Data Collection**\n",
    "    * Collect LLM's responses for various prompts\n",
    "    * Manually score them\n",
    "        * Ranking approach ranks various responses\n",
    "        * Rating approach let the human annotator score the responses against some criterion\n",
    "    * **Reward Model**: Learn a LM (e.g., encoder-only) that learns the same rating as humans.\n",
    "\n",
    "- Algorithm for fine-tuning: LLMs can be fine-tuned using reinforcement learning algorithms such as PPO [2] to produce outputs that maximize RM's output, i.e., reward\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2303.18223) Zhao et al. (2023) A survey of large language models\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1707.06347) Schulman et al. (2017) Proximal Policy Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d7c9d",
   "metadata": {},
   "source": [
    "### Adaptation: Fine-tuning methods\n",
    "\n",
    "- Full parameter tuning is computationally expensive\n",
    "\n",
    "- Parameter Efficient Fine-Tuning (PEFT) methods are used to fine-tune LLMs by selectively learn the parameters that modify the behavior of original LLM\n",
    "    * Adds a few fine-tunable parameters, thereby drastically reducing the number of trainable parameters\n",
    "\n",
    "- Several techniques have been proposed\n",
    "    * **Prefix tuning [1]**: Task-specific virtual tokens are learned that are added as a prefix to every layer\n",
    "    * **Prompt tuning [2]**: Task-specific prompting tokens are learned\n",
    "    * **Low-Rank Adaptation (LoRA) [3]**: Add low-rank \"update matrices\" to attention blocks. Once they are trained, merge them during the inference\n",
    "\n",
    "    <img src=\"imgs/peft.png\">\n",
    "\n",
    "\n",
    "Library: [PEFT](https://github.com/huggingface/peft) implements all of the fine-tuning methods\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2101.00190) Li et al. (2021) Prefix-Tuning: Optimizing Continuous Prompts for Generation\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2104.08691) Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning\n",
    "\n",
    "[[3]](https://arxiv.org/abs/2106.09685) Hu et al. (2021) LoRA: Low-Rank Adaptation of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4fca7",
   "metadata": {},
   "source": [
    "### Usage: Memory-effecient Model Adaptation\n",
    "\n",
    "- Bigger sized models demand more memory during the inference time\n",
    "\n",
    "- Quantization methods have been proposed to reduce the size of these models\n",
    "\n",
    "- LLMs have been shown to have outliers in their activations. This makes quantization difficult.\n",
    "\n",
    "- Several approaches have been proposed\n",
    "    * Mixed-precision decomposition [1]: *LLM.int8()* is the most-commonly used method to quantize LLMs\n",
    "    * Layerwise quantization [2]\n",
    "    \n",
    "\n",
    "Libraries: [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa), [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2208.07339) Dettmers et al. (2022) LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\n",
    "\n",
    "[[2]](https://openreview.net/forum?id=ksVGCOlOEba) Frantar et al. (2022) Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
