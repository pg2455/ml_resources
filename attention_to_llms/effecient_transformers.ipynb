{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38c60fc",
   "metadata": {},
   "source": [
    "## XFormers\n",
    "\n",
    "- Transformers are behind the most successful LLMs\n",
    "\n",
    "- There is a need to increase the size of the transformers\n",
    "\n",
    "- Scaling laws and Emergent properties\n",
    "\n",
    "- In this session, we will look at various ways that transformers are being optimized for their performance \n",
    "\n",
    "Please refer to [1] for the general survey of the effecient transformers. This session is targeted to give a sense of various ways that attention mechanism can be made effecient. \n",
    "\n",
    "[[1]](https://arxiv.org/abs/2009.06732) Tay et al. (2022) Efficient Transformers: A Survey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ffe8c",
   "metadata": {},
   "source": [
    "### Modes of Transformer\n",
    "- Most common reference to transformers is from Vaswani et al. (2017). \n",
    "\n",
    "<img width=750 src=\"imgs/transformer-modes.png\">\n",
    "\n",
    "- **Encoder-only**: Used for classification tasks \n",
    "- **Decoder-only**: Used for autoregression tasks where next prediction need to be based only on present and the past predictions\n",
    "- **Encoder-Decoder**: Document summarization, language translation, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a18d5",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "- It uses self-attention, with $ \\mathbf{Q},\\mathbf{K}, \\mathbf{V}  \\in \\mathbb{R}^{n \\times d}$. Here, $n$ is the sequence length, and $d$ is the dimension. $n >> d$\n",
    "\n",
    "$$\\mathbf{c} = \\text{Softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^T)}{\\sqrt{d}})\\mathbf{V}$$\n",
    "\n",
    "- Computational complexity: $\\mathcal{O}(n^2)$. \n",
    "\n",
    "<img width=750 src=\"imgs/transformer-complexity.png\">\n",
    "\n",
    "\n",
    "- Most of the engineering advances in attention mechanism **aim to reduce this complexity**: We would want to increase $n$ to a very high value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee38343",
   "metadata": {},
   "source": [
    "### Reducing computational complexity \n",
    "\n",
    "- Reduce the sequence length\n",
    "\n",
    "- Process segment recurrently\n",
    "\n",
    "- Improve the calculation of attention scores\n",
    "\n",
    "- Memory level optimization: Reduce memory access times\n",
    "\n",
    "- Alternatives to attention mechanisms\n",
    "\n",
    "    \n",
    "**Note**: Some mechanisms might not allow decoder where masking is required. As a result, they might not be suitable for auto-regressive applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fa0a4",
   "metadata": {},
   "source": [
    "### Attention mechanisms\n",
    "\n",
    "Tay et al. (2022) Efficient Transformer: A Survey\n",
    "\n",
    "<img width=750 src=\"imgs/attn_complexity.png\">\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2009.06732) Tay et al. (2022) Effecient Transformers: A Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df502990",
   "metadata": {},
   "source": [
    "### Reduce the sequence length\n",
    "\n",
    "- Only compute attention using a segment of the sequence or any smaller representations of this sequence\n",
    "\n",
    "- Several ways to define segments\n",
    "    * **Split the entire sequence into blocks** and **use convolution within each block** to reduce keys and values [1, 2]\n",
    "    \n",
    "    <img width=750 src=\"imgs/mem-compressed-attn.png\">    \n",
    "\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/1801.10198) Liu et al. (2018), Generating Wikipedia by Summarizing Long Sequences\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1802.05751) Parmar et al. (2018), Image Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c1880",
   "metadata": {},
   "source": [
    "### Reduce the sequence length\n",
    "\n",
    "- Only compute attention using a segment of the sequence or any smaller representations of this sequence\n",
    "\n",
    "- Several ways to define segments \n",
    "    * **Do impartial calculations based on pre-defined sparse patterns**,e.g., Sparse Transformers [1], Longformer[2], ETC[3], BigBird[4]. Image: Sparse Transformer\n",
    "    \n",
    "    <img width=750 src=\"imgs/sparse-transformer.png\">\n",
    "        \n",
    "\n",
    "[[1]](https://arxiv.org/abs/1904.10509) Child et al. (2019) Generating Long Sequences with Sparse Transformers\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2004.05150) Beltagzy et al. (2020) Longformer: The Long-Document Transformer\n",
    "\n",
    "[[3]](https://arxiv.org/abs/2004.08483) Ainslie et al. (2020) ETC: Encoding Long and Structured Inputs in Transformers\n",
    "\n",
    "[[4]](https://arxiv.org/abs/2007.14062) Zaheer et al. (2020) Big Bird: Transformers for Longer Sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c426669",
   "metadata": {},
   "source": [
    "### Reduce the sequence length\n",
    "\n",
    "- Only compute attention using a segment of the sequence or any smaller representations of this sequence\n",
    "\n",
    "- Several ways to define segments\n",
    "    * **Low rank projection to a smaller sequence length $k < n$** of the keys and values e.g., Linformer [7], Set Transformer [8]. Set transformer uses ISAB with learnable $\\mathbf{I} \\in \\mathcal{R}^{m \\times d}$ parameters.\n",
    "    \n",
    "    <img width=250 src=\"imgs/ISAB.png\">    \n",
    "\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/2006.04768) Wang et al. (2019) Linformer: Self-Attention with Linear Complexity\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1810.00825) Lee et al. (2019) Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e7586",
   "metadata": {},
   "source": [
    "### Reduce the sequence length\n",
    "\n",
    "- Only compute attention using a segment of the sequence or any smaller representations of this sequence\n",
    "\n",
    "- Several ways to define segments \n",
    "    * **Learn to identify blocks in sequences** based on some mechanism, e.g, Routing Transformer uses clustering [9], Reformer uses locality-sensitive hashing [10]\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/2003.05997) Roy et al. (2020) Efficient Content-Based Sparse Attention with Routing Transformers\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2001.04451) Ketaev et al. (2020) Reformer: The Efficient Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618e110",
   "metadata": {},
   "source": [
    "### Process segments recurrently\n",
    "\n",
    "- Transformer XL [1] processes segments recurrently, passing a hidden representation to the next transformer\n",
    "\n",
    "- Compressive Transformer [2] extends Transformer-XL by maintaining memory to retain past sequences\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1901.02860) Dai et al. (2019) Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1911.05507) Rae et al. (2019) Compressive Transformers for Long-Range Sequence Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1f1d6",
   "metadata": {},
   "source": [
    "### Improve the calculation of attention scores\n",
    "\n",
    "- Improve the attention computation by the use of kernels\n",
    "    * Performer [1] uses random kernels\n",
    "    * Linear Transformer [2] breaks the computation down to $n$ dot products (kernel function)\n",
    "    \n",
    "\n",
    "[[1]](https://arxiv.org/abs/2009.14794) Choromanski et al. (2020) Rethinking Attention with Performers\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2006.16236) Katharapoulos et al. (2020) Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5010e2",
   "metadata": {},
   "source": [
    "### Memory level optimization: Flash Attention [1] \n",
    "\n",
    "- Aims to maximize floating point operations per second (FLOPS) by reducing the times to access memory\n",
    "    \n",
    "<img src=\"imgs/flash-1.png\">\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2205.14135) Dao et al. (2022) FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \n",
    "\n",
    "[[Github Repo]](https://github.com/Dao-AILab/flash-attention) \n",
    "\n",
    "[[Blog]](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) Aleksa Gordic, ELI5: FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae923cd",
   "metadata": {},
   "source": [
    "### Memory level optimization: Flash Attention [1] \n",
    "\n",
    "- Computes attention scores in blocks, thereby reading and writing each block only once\n",
    "\n",
    "<img src=\"imgs/flash-mem.png\">\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2205.14135) Dao et al. (2022) FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \n",
    "\n",
    "\n",
    "[[Github Repo]](https://github.com/Dao-AILab/flash-attention) \n",
    "\n",
    "[[Blog]](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) Aleksa Gordic, ELI5: FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95171d3e",
   "metadata": {},
   "source": [
    "### Memory level optimization: Flash Attention-2 [1] \n",
    "\n",
    "- Flash Attention-2 [1]\n",
    "\n",
    "\n",
    "<img width=500 src=\"imgs/flash_attention.png\">\n",
    "\n",
    "[[1]](https://t.co/E5FZ3j1mDB) Tri Dao (2023) FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n",
    "\n",
    "[[Github Repo]](https://github.com/Dao-AILab/flash-attention) \n",
    "\n",
    "[[Blog]](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) Aleksa Gordic, ELI5: FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6404e",
   "metadata": {},
   "source": [
    "### Alternative architectures\n",
    "\n",
    "- Hyena architecture [1]: \n",
    "    * Subquadratic complexity. \n",
    "    * Uses learnable convolutions and recurrences\n",
    "\n",
    "- MLP Mixers [2]: \n",
    "    * Avoids the use of any CNNs and Attention\n",
    "    * Only uses MLPs \n",
    "\n",
    "- Synthesizers [3]: \n",
    "    * Similar to MLP-Mixers\n",
    "    * Approximates attention scores using MLPs\n",
    "\n",
    "- Many more ... \n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/pdf/2302.10866) Poli et al. (2023) Hyena Hierarchy: Towards Larger Convolutional Model\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2105.01601) Tolstikhin et al. (2021) MLP-Mixer: An all-MLP Architecture for Vision\n",
    "\n",
    "[[3]](https://arxiv.org/abs/2005.00743) Tay et al. (2020) Synthesizer: Rethinking Self-Attention in Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0046383",
   "metadata": {},
   "source": [
    "### Other optimizations\n",
    "\n",
    "- **Reducing the complexity of FFN following MHA**\n",
    "    * FFN is computationally expensive\n",
    "    * Mixture-of-Experts (MOE) approaches that assumes experts in specific region of the inputs, thereby routes tokens to specific experts\n",
    "    * Switch Transformer [1], GShard [2], etc. \n",
    "    \n",
    "- **Weight sharing**:  Sharing the parameters across encoders results in smaller models, e.g., Universal Transformers [3], Albert [4]\n",
    "\n",
    "- **Mixed precision training**: It reduces the memory costs, e.g., Q-BERT [5], Quantization-aware training of transforners [6]\n",
    "\n",
    "- **Knowledge Distillation**: Learning smaller (faster) models from the output of the larger models, e.g., DistilBERT [7], TinyBERT [8]\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2101.03961) Fedus et al. (2021) Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n",
    "\n",
    "[[2]](https://arxiv.org/abs/2006.16668) Lepikhin et al. (2020) GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n",
    "\n",
    "[[3]](https://arxiv.org/abs/1807.03819) Dehghani et. al. (2018) Universal Transformers\n",
    "\n",
    "[[4]](https://arxiv.org/abs/1909.11942) Lan et al. (2019) ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n",
    "\n",
    "[[5]](https://arxiv.org/abs/1909.05840) Shen et al. (2019) Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\n",
    "\n",
    "[[6]](https://arxiv.org/abs/2004.07320) Fan et al. (2020) Training with Quantization Noise for Extreme Model Compression\n",
    "\n",
    "[[7]](https://arxiv.org/abs/1910.01108) Sanh et al. (2019) DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n",
    "\n",
    "[[8]](https://arxiv.org/abs/1909.10351) Jiao et al. (2019) TinyBERT: Distilling BERT for Natural Language Understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
