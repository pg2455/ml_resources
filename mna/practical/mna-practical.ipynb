{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will design and train InceptionNet on [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which is easily available through [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10). \n",
    "\n",
    "\n",
    "*Below is the description of this dataset as on [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) website*\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "<img src=\"img/cifar.png\" width=500>\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will need the following python packages \n",
    "\n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. torch\n",
    "4. torchvision\n",
    "\n",
    "Please follow the instructions [here](https://pytorch.org/) to install the last two libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# fix seed for reproducibility \n",
    "rng = np.random.RandomState(1)\n",
    "torch.manual_seed(rng.randint(np.iinfo(int).max))\n",
    "\n",
    "# it is a good practice to define `device` globally\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU -> using CPU:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "PyTorch has simple-to-use functions that downloads and loads the datasets. We will use these functions to streamline our deep learning pipeline.\n",
    "\n",
    "Checkout other image datasets at [torch.datasets](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data in data folder. It will create this folder if it doesn't exist\n",
    "torchvision.datasets.CIFAR10(root=\"./data/\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "\n",
    "We will be carrying out simple investigations as we did in the practical of denoising autoencoders. \n",
    "Specifically, we are interested in finding out the following :\n",
    "\n",
    "How does the data look like?\n",
    "\n",
    "- What is the `type` of data?\n",
    "- What does each element of data represent?\n",
    "- What are the constituent parts of each element?\n",
    "- How is the image represented?\n",
    "- What do we use to plot an image?\n",
    "- How do we use the image in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = torchvision.datasets.MNIST(root=\"./data/\", train=True) # only load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data is the class\n",
      " <class 'torchvision.datasets.mnist.MNIST'>\n",
      "\n",
      "Each element of the data is\n",
      " <class 'tuple'>\n",
      "\n",
      "A single element is\n",
      " (<PIL.Image.Image image mode=L size=28x28 at 0x7F08DD84E4D0>, 5) \n",
      "\n",
      "first element is the image and the second element is the category\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of data is the class\\n\", type(data))\n",
    "print(\"\\nEach element of the data is\\n\", type(data[0]))\n",
    "print(\"\\nA single element is\\n\", data[0],\"\\n\\nfirst element is the image and the second element is the category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
