{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: InceptionNet\n",
    "\n",
    "In this notebook, we will design and train simplified InceptionNet on [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which is easily available through [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10). \n",
    "Note that InceptionNet was originally trained on ImageNet dataset which is much larger and requires more computational resources.\n",
    "**To prevent overfitting issues, we will size down the InceptionNet as well as introduce data augmentation.**\n",
    "\n",
    "\n",
    "*Below is the description of this dataset as on [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) website*\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "<img src=\"img/cifar.png\" width=500>\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will need the following python packages \n",
    "\n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. torch\n",
    "4. torchvision\n",
    "\n",
    "Please follow the instructions [here](https://pytorch.org/) to install the last two libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# fix seed for reproducibility \n",
    "rng = np.random.RandomState(1)\n",
    "torch.manual_seed(rng.randint(np.iinfo(int).max))\n",
    "\n",
    "# it is a good practice to define `device` globally\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU -> using CPU:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "PyTorch has simple-to-use functions that downloads and loads the datasets. We will use these functions to streamline our deep learning pipeline.\n",
    "\n",
    "Checkout other image datasets at [torch.datasets](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data in data folder. It will create this folder if it doesn't exist\n",
    "torchvision.datasets.CIFAR10(root=\"./data/\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "\n",
    "We will be carrying out simple investigations as we did in the practical of denoising autoencoders. \n",
    "Specifically, we are interested in finding out the following :\n",
    "\n",
    "How does the data look like?\n",
    "\n",
    "- What is the `type` of data?\n",
    "- What does each element of data represent?\n",
    "- What are the constituent parts of each element?\n",
    "- How is the image represented?\n",
    "- What do we use to plot an image?\n",
    "- How do we use the image in our model?\n",
    "- What is the range of input data? Do we need to normalize it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = torchvision.datasets.CIFAR10(root=\"./data/\", train=True) # only load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of referencing, we will create a dictionary mapping the category number to the category label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_MAPPING = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Type of data is the class\\n\", type(data))\n",
    "# print(\"\\nEach element of the data is\\n\", type(data[0]))\n",
    "# print(\"\\nA single element is\\n\", data[0],\"\\n\\nfirst element is the image and the second element is the category\")\n",
    "# print(f\"\\nImage size in tensor form is accessible through data.data[idx].shape e.g.: {data.data[0].shape}\")\n",
    "\n",
    "# what else do you want to know about data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original sized image as displayed in jupyter notebook\n",
    "data[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data\n",
    "\n",
    "We will use `.data` attribute of `torchvision.datasets.CIFAR10` to access the images in tensor format.\n",
    "We will use [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) to appropriately transform the data to be further processed by our CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "\n",
    "img = data.data[idx]\n",
    "\n",
    "# What is the range of data? \n",
    "# print(f\"Min value: {img.min()} \\t Max value:{img.max()}\")\n",
    "\n",
    "# how to display the image \n",
    "plt.imshow(img)\n",
    "print(f\"Category number: {data[idx][1]} \\t Category label: {CATEGORY_MAPPING[data[idx][1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_cat_map = defaultdict(list)\n",
    "for idx, (_, c) in enumerate(data):\n",
    "    reverse_cat_map[c].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "n_samples = len(data)\n",
    "\n",
    "ncats = np.random.choice(list(CATEGORY_MAPPING.keys()), size=5) # number of categories\n",
    "ncols = 10 # images per category\n",
    "fig, axs = plt.subplots(nrows=len(ncats), ncols=ncols, figsize=(12,6), dpi=100)\n",
    "\n",
    "for row, cat in enumerate(ncats):\n",
    "    idxs = reverse_cat_map[cat]\n",
    "    idxs = np.random.choice(idxs, size=ncols)\n",
    "    \n",
    "    # title \n",
    "    axs[row][0].set_ylabel(CATEGORY_MAPPING[cat], fontweight=\"bold\", rotation=0, labelpad=50)\n",
    "    for i in range(ncols):\n",
    "        ax = axs[row][i]\n",
    "        ax.imshow(data[idxs[i]][0])\n",
    "        ax.axis('off')\n",
    "    axs[row][0].axis('on')\n",
    "    axs[row][0].xaxis.set_ticks([])\n",
    "    axs[row][0].yaxis.set_ticks([])\n",
    "\n",
    "_ = fig.suptitle(f\"CIFAR-10 dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Block\n",
    "\n",
    "In this tutorial, instead of building the GoogleNet ditto, we will focus on the basics of InceptionNet. \n",
    "Specifically, we will learn how to build an Inception Block and arrange several of these blocks to form an InceptionNet. \n",
    "\n",
    "As a refresher, the inception block as described by Szegedy et al. consists of several convolutions of size 1,3, and 5 along with max pooling layers. \n",
    "This allows the network to process images at different receptive fields. \n",
    "\n",
    "\n",
    "<img src=\"img/inception_block.svg\" width=1000>\n",
    "\n",
    "[[Szegedy et al. 2014] Going deeper with convolutions](https://arxiv.org/pdf/1409.4842.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a basic inception block \n",
    "    \n",
    "    Args:\n",
    "        c_in (int): Number of channels in the input \n",
    "        c_reduce (dict): dictionary mapping dimension reduction for each of the 3x3 and 5x5 convolutions. Keys are '3x3' and '5x5'.\n",
    "        c_out (dict): dictionary mapping final number of channels for each of the convolutions. Keys are '1x1', '3x3', '5x5', and 'maxpool'\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_reduce, c_out):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        # 1x1\n",
    "        self.conv1x1 = nn.Sequential(         \n",
    "            ### YOUR CODE HERE\n",
    "            ### Use conv, batchnorm, activation\n",
    "        )\n",
    "        \n",
    "        # 3x3 \n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "            ### Use conv, batchnorm, activation along with dimension reduction using 1x1 conv as above\n",
    "        )\n",
    "        \n",
    "        # 5x5 \n",
    "        self.conv5x5 = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "            ### Use conv, batchnorm, activation along with dimension reduction using 1x1 conv as above\n",
    "        )\n",
    "        \n",
    "        # max-pool\n",
    "        self.pool = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        # x_out = \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Shallow) InceptionNet \n",
    "\n",
    "Szegedy et al. used the following configuration for GoogleNet with 7M parameters. ImageNet's images are of the size 3 x 224 x 224.\n",
    "\n",
    "<img src=\"img/inception-net.png\" width=1000>\n",
    "\n",
    "\n",
    "Since we are training on a smaller CIFAR10 dataset with images of size 3 x 32 x 32, we will use a smaller InceptionNet with the following configration\n",
    "\n",
    "[[Szegedy et al. 2014] Going deeper with convolutions](https://arxiv.org/pdf/1409.4842.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR MODEL DETAILS HERE\n",
    "\n",
    "We will **reduce the size of the output channels from the above table by a factor of 4**, and **restrict the depth to 1 for all the blocks**. Resulting in the following configuration (**fill in  ?**) -\n",
    "\n",
    "**Input size: 32 x 32 x 3**\n",
    "\n",
    "\n",
    "|      type     \t| patch size <br>/ stride \t|  output <br>  size  \t| depth \t| # 1x1 \t|  #3x3 <br>reduce \t| #3x3 \t|  #5x5 <br>reduce \t| #5x5 \t| pool<br>proj \t|\n",
    "|:-------------:\t|:-----------------------:\t|:-------------------:\t|:-----:\t|:-----:\t|:----------------:\t|:----:\t|:----------------:\t|:----:\t|:------------:\t|\n",
    "|  convolution  \t|          3x3/?          \t|     32 x 32 x ?    \t|   1   \t|   -   \t|         -        \t|   -  \t|         -        \t|   -  \t|       -      \t|\n",
    "|  convolution  \t|          ?x?/1          \t|     32 x 32 x ?    \t|   1   \t|   -   \t|         -        \t|   -  \t|         -        \t|   -  \t|       -      \t|\n",
    "| inception(3a) \t|            -            \t|     32 x 32 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|   ?  \t|       8      \t|\n",
    "| inception(3b) \t|            -            \t|     32 x 32x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      16      \t|\n",
    "|    maxpool    \t|          3x3/?          \t|    16 x 16 x ?    \t|   -   \t|   -   \t|         -        \t|   -  \t|         -        \t|   -  \t|       -      \t|\n",
    "| inception(4a) \t|                         \t|    16 x 16 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      16      \t|\n",
    "| inception(4b) \t|                         \t|    16 x 16 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      16      \t|\n",
    "| inception(4c) \t|                         \t|    16 x 16 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      16      \t|\n",
    "| inception(4d) \t|                         \t|    16 x 16 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      16      \t|\n",
    "| inception(4e) \t|                         \t|    16 x 16 x ?    \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      32      \t|\n",
    "|    maxpool    \t|          ?x?/2          \t|     8 x 8 x ?     \t|   -   \t|   -   \t|         -        \t|   -  \t|         -        \t|   -  \t|       -      \t|\n",
    "| inception(5a) \t|                         \t|     8 x 8 x ?     \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|         ?        \t|  ?  \t|      32      \t|\n",
    "| inception(5b) \t|                         \t|     8 x 8 x ?     \t|   1   \t|   ?  \t|        ?        \t|  ?  \t|        ?        \t|  ?  \t|      32      \t|\n",
    "|    avg pool   \t|          ?x?/1          \t|     1 x 1 x ?     \t|       \t|       \t|                  \t|      \t|                  \t|      \t|              \t|\n",
    "|  dropout(40%) \t|                         \t|     1 x 1 x ?     \t|       \t|       \t|                  \t|      \t|                  \t|      \t|              \t|\n",
    "|     linear    \t|                         \t|     1 x 1 x ?     \t|       \t|       \t|                  \t|      \t|                  \t|      \t|              \t|\n",
    "|    softmax    \t|                         \t|      1 x 1 x ?     \t|       \t|       \t|                  \t|      \t|                  \t|      \t|              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowInceptionNet(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_args = [c_in]\n",
    "        \n",
    "        # \n",
    "        self.input_conv = nn.Sequential(\n",
    "            ## conv1 \n",
    "            ### YOUR CODE HERE\n",
    "            ### Use conv, batchnorm, activation\n",
    "            \n",
    "            ## conv2\n",
    "            ### YOUR CODE HERE\n",
    "            ### Use conv, batchnorm, activation\n",
    "        )\n",
    "        \n",
    "        # inception blocks \n",
    "        self.inception_stack = nn.Sequential(\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5': ??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 3a\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 3b\n",
    "            nn.MaxPool2d(kernel_size=?? stride=??, padding=??), # maxpool-1\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 4a\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 4b\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 4c\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 4d\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 4e\n",
    "            nn.MaxPool2d(kernel_size=??, stride=??, padding=??), # maxpool-1\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 5a\n",
    "            InceptionBlock(??, c_reduce={'3x3': ??, '5x5':??}, c_out={'1x1':??, '3x3':??, '5x5':??, 'maxpool':??}), # 5b                    \n",
    "        )\n",
    "        \n",
    "        # output module \n",
    "        self.output_softmax = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1,1)), # kernel_size and stride are automaticall inferred: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(??, ??),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_conv(x)\n",
    "        x = self.inception_stack(x)\n",
    "        x = self.output_softmax(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = data.data[0].shape # width, height,  number of channels, \n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write a function to fetch the memory statistics of our model.\n",
    "While building different autoencoder models, it will be important to ensure they have all similar number of learnable parameters. This will ensure fair comparison of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_size(model):\n",
    "    \"\"\"\n",
    "    Get model size in GB (as str: \"N GB\")\n",
    "    \"\"\"\n",
    "    mem_params = sum(\n",
    "        [param.nelement() * param.element_size() for param in model.parameters()]\n",
    "    )\n",
    "    mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs\n",
    "    return f\"{mem / 1e9:.4f} GB\"\n",
    "\n",
    "def num_params(model):\n",
    "    \"\"\"\n",
    "    Print number of parameters in model's named children\n",
    "    and total\n",
    "    \"\"\"\n",
    "    s = \"Number of parameters:\\n\"\n",
    "    n_params = 0\n",
    "    for name, child in model.named_children():\n",
    "        n = sum(p.numel() for p in child.parameters())\n",
    "        s += f\"  • {name:<15}: {n}\\n\"\n",
    "        n_params += n\n",
    "    s += f\"{'total':<19}: {n_params}\"\n",
    "\n",
    "    return s\n",
    "\n",
    "def pp_model_summary(model):\n",
    "    print(num_params(model))\n",
    "    print(f\"{'Total memory':<18} : {mem_size(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model size\n",
    "\n",
    "How many parameters does your model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowInceptionNet(c_in=input_shape[2])\n",
    "pp_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "\n",
    "Standard ways to transform image datasets has majorly involved two components:\n",
    "\n",
    "- **Data normalization**: It is a standard practice to make sure that all input dimensions are on the same scale. We will use [`transforms.normalize`](https://pytorch.org/vision/stable/transforms.html#conversion-transforms) for this purpose.\n",
    "    \n",
    "    \n",
    "- **Data augmentation**: Given that an image category remains same if it is cropped (a little) or rotated, we want the network to be robust to these changes. Not only it makes for a better classifier it also prevents network from overfitting to the original dataset. Therefore, we will be doing random cropping followed by resizing and flipping of the images. This is conveniently handled by [`transforms.RandomCrop`](https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html#RandomCrop) and [`transforms.RandomHorizontalFlip`](https://pytorch.org/vision/stable/_modules/torchvision/transforms/transforms.html#RandomHorizontalFlip). Note that we can use other transformations as well. \n",
    "    \n",
    "**Note**: We don't need to apply the above augmentation transformations on the test dataset. \n",
    "\n",
    "We will define these transforms here for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS =  (data.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (data.data / 255.0).std(axis=(0,1,2))\n",
    "\n",
    "print(f\"data means along three channels: {DATA_MEANS}\")\n",
    "print(f\"data std along three channels: {DATA_STD}\")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(size=(input_shape[0], input_shape[1]), ratio=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=DATA_MEANS, std=DATA_STD)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=DATA_MEANS, std=DATA_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "[`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) conveniently takes these transforms to provide us with a dataloader for iterating through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset with transforms\n",
    "train_data = torchvision.datasets.CIFAR10(root=\"./data/\", transform=train_transforms)\n",
    "\n",
    "# split data into train and val; we will create train_data at the start of every epoch\n",
    "x_train, x_val = torch.utils.data.random_split(train_data, [45000, 5000]) # 10% train-val split\n",
    "val_dataloader = torch.utils.data.DataLoader(x_val, batch_size=256, shuffle=True)\n",
    "\n",
    "# test dataset with transforms\n",
    "test_data = torchvision.datasets.CIFAR10(root=\"./data/\", transform=test_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=256, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, dataloader, optimizer=None):\n",
    "    n_samples = 0\n",
    "    running_loss, running_acc = 0, 0\n",
    "    for batch, labels in dataloader:\n",
    "        # transfer to GPU if avaiable\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        n_samples += batch.shape[0]\n",
    "        \n",
    "        # forward pass\n",
    "        ### YOUR CODE HERE\n",
    "        # Compute model output\n",
    "        # Compute loss\n",
    "        # Compute model predictions\n",
    "        \n",
    "        # backward pass \n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += ### YOUR CODE HERE compute accuracy from predictions\n",
    "        \n",
    "    return running_loss / n_samples, running_acc / n_samples\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.to(device)\n",
    "    \n",
    "    # fix seed for reproducibility \n",
    "    rng = np.random.RandomState(1)\n",
    "    torch.manual_seed(rng.randint(np.iinfo(int).max))\n",
    "\n",
    "    # create a model directory to store the best model\n",
    "    model_dir = pathlib.Path(\"./models\").resolve()\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "    epoch_size=200\n",
    "    batch_size=128\n",
    "\n",
    "    best_val_acc = 0\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    n_epochs = 30\n",
    "    no_improvement_cnt = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"@ epoch {epoch}\", end=\"\")\n",
    "\n",
    "        # training loss\n",
    "        idxs = rng.choice(len(x_train), epoch_size * batch_size, replace=True)\n",
    "        train_dataloader = torch.utils.data.DataLoader([x_train[idx] for idx in idxs], batch_size=batch_size, num_workers=4)\n",
    "        train_loss, train_acc = process(model, train_dataloader, optimizer)\n",
    "\n",
    "        # validation loss\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = process(model, val_dataloader)\n",
    "\n",
    "        # save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_dir / \"best.ckpt\")\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "\n",
    "            # if there has been no improvement in validation loss, stop early\n",
    "            no_improvement_cnt += 1\n",
    "\n",
    "            if no_improvement_cnt % 10 == 0:\n",
    "                print(\"\\nEarly stopping!\")\n",
    "                break\n",
    "\n",
    "        # logging\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"\\ttrain_loss: {train_loss: .5f}, train_acc:{100*train_acc: 2.3f}%,  val_loss: {val_loss:.5f}, val_acc:{100*val_acc: 2.3f}%,\")\n",
    "\n",
    "    print(f\"best val acc: {100*best_val_acc:2.3f}%\")\n",
    "\n",
    "    # load the best model\n",
    "    model = model.__class__(*model.input_args)\n",
    "    model.load_state_dict(torch.load(model_dir / \"best.ckpt\"))\n",
    "    model = model.to(device) \n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    }\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowInceptionNet(c_in=input_shape[2])\n",
    "model, metrics = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=100)\n",
    "\n",
    "axs.plot(metrics['train_losses'], color=\"#BDD9BF\", marker=\"o\", label=\"Train loss\", linestyle=\":\", linewidth=2)\n",
    "axs.plot(metrics['val_losses'], color=\"#A997DF\", marker=\"o\", label=\"Val loss\", linestyle=\":\", linewidth=2)\n",
    "axs.set_ylabel(\"loss\", fontsize=20)\n",
    "\n",
    "acc_axs = axs.twinx()\n",
    "acc_axs.plot(list(map(lambda x: 100*x, metrics['train_accs'])), color=\"#BDD9BF\", marker=\"x\", label=\"train acc\", linewidth=2)\n",
    "acc_axs.plot(list(map(lambda x: 100*x, metrics['val_accs'])), color=\"#A997DF\", marker=\"x\", label=\"val acc\", linewidth=2)\n",
    "acc_axs.set_ylabel(\"accuracy\", fontsize=20)\n",
    "\n",
    "axs.set_xlabel(\"Epochs\", fontsize=20)\n",
    "\n",
    "# tick size\n",
    "for tick in axs.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(15)\n",
    "\n",
    "for tick in axs.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(15)\n",
    "\n",
    "for tick in acc_axs.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(15)\n",
    "    \n",
    "    \n",
    "# legend\n",
    "legend = []\n",
    "legend.append(Line2D([0,1], [1,0], color=\"#BDD9BF\", label=\"Train\", linewidth=5))\n",
    "legend.append(Line2D([0,1], [1,0], color=\"#A997DF\", label=\"Val\", linewidth=5))\n",
    "legend.append(Line2D([0,1], [1,0], color=\"black\", label=\"Accuracy\", linewidth=1))\n",
    "legend.append(Line2D([0,1], [1,0], color=\"black\", linestyle=\":\",label=\"loss\", linewidth=1))\n",
    "lgd = fig.legend(handles=legend, ncol=1, fontsize=15, loc=\"center right\", fancybox=True, bbox_to_anchor=(1.0, 0.5, 0.2, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set performance \n",
    "test_loss, test_acc = process(model, test_dataloader)\n",
    "\n",
    "print(f\"test dataset loss: {test_loss: 0.5f} \\t accuracy: {100*test_acc: 2.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkenv",
   "language": "python",
   "name": "dkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
