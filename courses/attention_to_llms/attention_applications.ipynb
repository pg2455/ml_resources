{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caa87d3",
   "metadata": {},
   "source": [
    "### Usage of attention mechanism\n",
    "\n",
    "- Attention mechanism has evolved over the years\n",
    "\n",
    "- It has been used in several applications\n",
    "\n",
    "- In this session, we will look at some of the ways in which attention mechanism has been used in various domains\n",
    "\n",
    "- Various survey exists which define their own taxonomy [1, 2, 3, 4] \n",
    "\n",
    "- This session follows the taxonomy from Brauwers et al.\n",
    "    * Simplified for the ease of explanation\n",
    "    * Applications are not covered\n",
    "\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/2203.14263) Brauwers et al. (2022) A General Survey on Attention Mechanisms in Deep Learning\n",
    "\n",
    "[[2]](https://link.springer.com/article/10.1007/s41095-022-0271-y) Guo et al. (2022) Attention mechanisms in computer vision: A survey\n",
    "\n",
    "[[3]](https://dl.acm.org/doi/abs/10.1145/3465055) Chaudhari et al. (2021) An Attentive Survey of Attention Models\n",
    "\n",
    "[[4]](https://arxiv.org/abs/1811.05544) Dichao Hu (2018) An Introductory Survey on Attention Mechanisms in NLP Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2bdc0",
   "metadata": {},
   "source": [
    "### Attention mechanism\n",
    "\n",
    "<img width=500, src=\"imgs/attention.png\">\n",
    "\n",
    "**Inputs:**\n",
    "- Features: $\\textbf{F} \\in \\mathcal{R}^{n_{F}\\times d}$\n",
    "- Query: $\\textbf{Q} \\in \\mathcal{R}^{n_{Q}\\times d}$\n",
    "    * Given a query, what features are important and how to combine them?\n",
    "    * optional: Self-attention mechanism learns the query itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31ac91",
   "metadata": {},
   "source": [
    "### Attention mechanism\n",
    "\n",
    "<img width=500, src=\"imgs/attention.png\">\n",
    "\n",
    "**Outputs:**\n",
    "- Context: $\\textbf{C} \\in \\mathcal{R}^{n_{c}\\times d}$\n",
    "    * Use it as **Output** for the downstream task\n",
    "    * Use it as **Query** for another attention mechansim\n",
    "    * Use it as **Features** for another attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605a17e",
   "metadata": {},
   "source": [
    "### Attention mechanism\n",
    "<img width=500, src=\"imgs/attention.png\">\n",
    "\n",
    "**Processing blocks:**\n",
    "- **Feature to Keys and Values** ($\\textbf{A}_1$): Mostly, simple MLPs to convert features into keys and values\n",
    "    * mostly, linear transformation\n",
    "    * low computational complexity\n",
    "\n",
    "- **Score**:\n",
    "    * combines queries with keys\n",
    "    * what are relevant keys to answer a query?\n",
    "    * high computational complexity\n",
    "\n",
    "- **Align**:\n",
    "    * given attention scores, how to combine values?\n",
    "    * low computational complexity, but potential for memory bandwidth optimization\n",
    " \n",
    "- **Combining values** ($\\textbf{A}_2$): \n",
    "    * mostly, elementwise multiplication\n",
    "    * low computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e4be0",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Feature Modality\n",
    "\n",
    "- **Single feature**: A single data modality, e.g., text, video, audio\n",
    "\n",
    "- **Multiple features**: Multiple data modalities, e.g., combinations of text, video, audio, tabular features\n",
    "\n",
    "<img src=\"imgs/multimodal.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32817b4b",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Feature Modality\n",
    "\n",
    "- **Multiple features**: How to combine features from various attention mechanisms?\n",
    "    * Alternating co-attention [1]\n",
    "\n",
    "    <img width=500 src=\"imgs/alternating_co-attn.png\">\n",
    "    \n",
    "[[1]](https://arxiv.org/pdf/1606.00061) Lu et al. (2016) Hierarchical Question-Image Co-Attention for Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dcba8",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Feature Modality\n",
    "\n",
    "- **Multiple features**: How to combine features from various attention mechanisms?\n",
    "    * Interactive co-attention [1]\n",
    "\n",
    "    <img width=500 src=\"imgs/interactive_co-attn.png\">\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/1709.00893) Ma et al. (2017) Interactive Attention Networks for Aspect-Level Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4509391",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Feature Modality\n",
    "\n",
    "- **Multiple features**: How to combine features from various attention mechanisms?\n",
    "    * Rotary co-attention [1]\n",
    "\n",
    "    <img width=500 src=\"imgs/rotary-attn.png\">    \n",
    "    \n",
    "[[1]](https://arxiv.org/abs/1802.00892) Zheng et al. (2018) Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4e302",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Feature Modality\n",
    "\n",
    "- **Multiple features**: How to combine features from various attention mechanisms?\n",
    "    * Hierarchical attention [1]\n",
    "\n",
    "    <img width=500 src=\"imgs/hierarchical_attn.png\">\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/1806.00723) Wu et al. (2018) A Hierarchical Attention Model for Social Contextual Image Recommendation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd1386",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Queries (Type)\n",
    "\n",
    "- **Standard Atttention**\n",
    "\n",
    "<img width=500 src=\"imgs/standard-attn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6766a96",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Queries (Type)\n",
    "\n",
    "- **Self-attention**: Learn the query so that the queries are specialized for the downstream task\n",
    "\n",
    "<img width=500 src=\"imgs/learn-query.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48814c",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Queries (Type)\n",
    "\n",
    "- **No query**: There is a constant query asking: which features are important?\n",
    "\n",
    "<img width=500 src=\"imgs/constant-query.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa792748",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Queries (Multiplicity)\n",
    "\n",
    "- Multi-head attention: Uses several queries from the same features. \n",
    "    * Attention mechanisms in parallel\n",
    "\n",
    "\n",
    "- Forming queries from the output of other attention mechanisms\n",
    "    * Alternating Co-attention\n",
    "    * Interactive Co-attention\n",
    "    * Rotary attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679b668",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Processing functions (Scoring - scalar)\n",
    "\n",
    "- Computes the score between query and keys, i.e., $score(\\mathbf{q}, \\mathbf{k}) \\in \\mathbb{R}$\n",
    "\n",
    "\n",
    "- Additive (Concatenate) [1] \n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}^T \\times activation(\\mathbf{W} [\\mathbf{q}, \\mathbf{k}] + \\mathbf{b}) $$\n",
    "\n",
    "- Multiplicative (Dot-product) [2]\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T \\times \\mathbf{k} $$\n",
    "\n",
    "- Scaled Multiplicative [3]\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\frac{\\mathbf{q}^T \\times \\mathbf{k}}{\\sqrt{d_k}} $$\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1409.0473) Bahdanau et al. (2014) Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1508.04025) Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation\n",
    "\n",
    "[[3]](https://arxiv.org/abs/1706.03762) Vaswani et al. (2017) Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8198",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Processing functions (Scoring - scalar)\n",
    "\n",
    "- Computes the score between query and keys, i.e., $score(\\mathbf{q}, \\mathbf{k}) \\in \\mathbb{R}$\n",
    "\n",
    "- General [1, 2, 3]\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\mathbf{k}^T \\times \\mathbf{W} \\times \\mathbf{q} $$\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\mathbf{k}^T \\times (\\mathbf{W} \\times \\mathbf{q}  + \\mathbf{b})$$\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = activation(\\mathbf{k}^T \\times \\mathbf{W} \\times \\mathbf{q}  + \\mathbf{b})$$\n",
    "\n",
    "- Miscellaneous [4]\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\lvert\\lvert \\mathbf{q}-\\mathbf{k} \\rvert\\rvert_2$$\n",
    "\n",
    "$$ score(\\mathbf{q}, \\mathbf{k}) = \\frac{\\mathbf{q}^{T}\\mathbf{k}}{\\|\\mathbf{q}\\|\\|\\mathbf{k}\\|}$$\n",
    "\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1508.04025) Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1606.02245) Sordoni et al. (2016) Iterative Alternating Neural Attention for Machine Reading\n",
    "\n",
    "[[3]](https://arxiv.org/abs/1709.00893) Ma et al. (2017) Interactive Attention Networks for Aspect-Level Sentiment Classification\n",
    "\n",
    "[[4]](https://arxiv.org/abs/1410.5401) Graves et al. (2014) Neural Turing Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e82b83",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Processing functions (Alignment)\n",
    "\n",
    "- How should the scores be combined to process values and produce a context vector?\n",
    "\n",
    "- Assuming 1 query and $n$ keys and values, $\\mathbf{e} = score(\\mathbf{q}, \\mathbf{K}) \\in \\mathbf{R}^n$, i.e., one score for each key $\\mathbf{k}$\n",
    "\n",
    "- $\\mathbf{a} = \\text{Alignment}(\\mathbf{e}) \\in \\mathbf{R}^n$ defines how to combine Values, $\\mathbf{V} \\in \\mathbf{R}^{n \\times d}$ to form the context vector\n",
    "\n",
    "$$\\mathbf{c} = \\mathbf{a}^T\\mathbf{V}$$\n",
    "\n",
    "- **Soft alignment/global alignment** [1]\n",
    "\n",
    "$$\\mathbf{a}_i = \\frac{\\exp(\\mathbf{e}_i)}{\\sum_i{\\exp(\\mathbf{e}_i)}}$$\n",
    "\n",
    "\n",
    "- **Hard alignment**: Select one Value [2]\n",
    "\n",
    "$$\\mathbf{a} = \\text{Multinomial}(\\frac{\\exp(\\mathbf{e}_i)}{\\sum_j{\\exp(\\mathbf{e}_j)}})$$\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1508.04025) Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1502.03044) Xu et al. (2015) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c14c0c",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Processing functions (Alignment)\n",
    "\n",
    "- How should the scores be combined to process values and produce a context vector?\n",
    "\n",
    "- **Local alignment**: Softmax distribution is calculated based only on the subset of the attention scores.\n",
    "    * Fix the midpoint, $p$, and compute the softmax scores based on the window around $p$, i.e, values too far from p will be ignored and only those closer to $p$ will be taken into account.\n",
    "    \n",
    "    $$\\mathbf{a}_i = \\frac{\\exp{\\mathbf{e}_i}}{\\sum_{j=p-D}^{p+D}\\exp{\\mathbf{e}_j}} $$\n",
    "\n",
    "    * Monotonic alingment: Fix $p$ to the position in the prediction token \n",
    " \n",
    "    * Predictive alignment: Learn $p$\n",
    "    \n",
    "[[1]](https://arxiv.org/abs/1502.03044) Xu et al. (2015) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd9da5",
   "metadata": {},
   "source": [
    "### Attention mechanisms by Processing functions (Score - multidimensional)\n",
    "\n",
    "- Attention scores are multidimensional, i.e., scores define how to combine each dimension in the values [1]\n",
    "\n",
    "- If keys, $\\mathbf{K} \\in \\mathbb{R}^{n \\times d}$ and values $\\mathbf{V} \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "$$score(\\mathbf{q}, \\mathbf{K}) \\in \\mathbb{R}^{n \\times d}$$\n",
    "\n",
    "$$\\mathbf{c} = \\mathbf{1}^T(score(\\mathbf{q}, \\mathbf{K}) \\cdot \\mathbf{V})$$ \n",
    "\n",
    "[[1]](https://arxiv.org/abs/1709.04696) Shen et al. (2017) DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
