{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Following module is intended to give a high-level overview of research landscape in neural network pruning. \n",
    "We also talk very brielfy about other neural network compression techniques. \n",
    "Thus, the module aims at  \n",
    "\n",
    "- understanding the **motivation** behind neural network compression\n",
    "- being aware of **different approaches** for neural network compression\n",
    "- learning **Whats and Hows of of neural network pruning**\n",
    "- getting comfortable with the **research and advances in  neural network pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger models: Performance\n",
    "\n",
    " - Bigger the model, more the number of parameters, **more expressive is the functional space**\n",
    " - As a result, SOTA language models are getting bigger and bigger\n",
    "\n",
    "<img src=\"img/language-models-scaling.png\" width=\"500\">\n",
    "\n",
    "[Image source: Robo-writers: the rise and risks of language-generating AI](https://www.nature.com/articles/d41586-021-00530-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger models: Storage & memory-bandwidth\n",
    "\n",
    "- Bigger models take up huge space to store\n",
    "\n",
    "- Difficult to store on low-space devices, e.g., smartphones, IoT devices\n",
    "\n",
    "- Difficult to distribute necessary for real-world application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger models: Computational efficiency\n",
    "\n",
    "- Requires expensive computational hardwares, limiting the progress in the hands of giants\n",
    "\n",
    "<img src=\"img/giants.png\" width=\"500\">\n",
    "\n",
    "- Runtimes are slow, thereby making them not suitable for time-critical applications\n",
    "\n",
    "- Unsuitable for embedded mobile applications. For example, 1 billion connection neural network that are not suitable for on-chip storage, takes 12.8W of energy just for DRAM access [Han et al. 2015]\n",
    "\n",
    "[Image source: DistilBERT, a distilled version of BERT: smaller,\n",
    "faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)\n",
    "\n",
    "[[Han et al. 2015] Learing both Weights and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression of Neural Networks\n",
    "\n",
    "- Aim is to reduce the size of the models a.k.a model compression\n",
    "\n",
    "- While **minimizing the loss in the quality** of the model\n",
    "    - Quality measure depends on the task \n",
    "    - For example, perplexity for language models, accuracy for visual recognition, etc.\n",
    "    \n",
    "- While **increasing the efficiency** of the models, where efficiency can relate to \n",
    "    - computational requirements, e.g, FLOPS, latency, etc.\n",
    "    \n",
    "    - storage requirements, e.g., compression ratio \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression of Neural Networks: Approaches\n",
    "\n",
    "- Constructive approaches\n",
    "\n",
    "    - **Hand-design** a smaller network: For example, replacing fully connected layers with global pooling average in GoogLenet [Szegedy et al. 2015], or Depthwise Separable Convolution in MobileNet [Howard et al. 2017]\n",
    "    \n",
    "    - **Auto-ML** Neural network architecture search (NAS) with a constraint on the number of parameters [Dong et el. 2019]\n",
    "    \n",
    "\n",
    "\n",
    "[[Szegedy et al. 2015] Going deeper with convolutions](https://openaccess.thecvf.com/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n",
    "\n",
    "[[Howard et al. 2017] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "[[Dong et el. 2019] Network Pruning via Transformable Architecture Search](https://arxiv.org/abs/1905.09717)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression of Neural Networks: Approaches\n",
    "\n",
    "- Destructive approaches\n",
    "\n",
    "    - **Network Pruning**: Removing redundant connections or weights\n",
    "    \n",
    "    - **Knowledge Distillation**: Transferring of knowledge from the larger model to a smaller one [Hinton et al. 2015]\n",
    "    \n",
    "    - **Quantization**: Reducing the precision of the weights and biases so that the model consumes less memmory, e.g., using 8-bit integers to represent 32-bit floats for network parameters reduces the size by a factor of 4. \n",
    "        - Post-training Quantization: Quantize the parameters after the training (leads to a higer loss in accuracy)\n",
    "        - Quantization aware training: Forward pass is with quantized parameters while the backward pass is assuming non-quantized parameters\n",
    "        \n",
    "        - Refer to Gholami et al. 2021 for an extensive survey of these methods \n",
    "    \n",
    "    - **Tensor Decomposition**: Low-rank approximation of fully connected layers in an over-parametrized neural network. [Read more here.](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)\n",
    "    \n",
    "    - **Mix of above**: These approaches can be combined together [Han et al. 2016, Wang et al. 2020]\n",
    "\n",
    "\n",
    "[[Hinton et al. 2015] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "\n",
    "[[Gholami et al. 2021] A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/pdf/2103.13630.pdf)\n",
    "\n",
    "[[Han et al. 2016] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)\n",
    "\n",
    "[[Wang et al. 2020] APQ: Joint Search for Network Architecture, Pruning and Quantization Policy](https://openaccess.thecvf.com/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: History \n",
    "\n",
    "- Le Cun et al. 1990 proposed Optimal Brain Damage (OBD) to prune neural networks by **removing redundant/less useful weights** that do not contribute significantly to the output \n",
    "\n",
    "- Hassibi et al. 1992 recognized that OBD often removed wrong weights, and proposed Optimal Brain Surgeon (OBS) to prune more weights while retaining the generalization error\n",
    "\n",
    "- Several works have followed up with different heuristics and methodologies to recognize redundancies in neural networks\n",
    "\n",
    "[[Le Cun et al. 1990] Optimal Brain Damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)\n",
    "\n",
    "[[Hassibi et al. 1992] Second order derivatives for network pruning: Optimal Brain Surgeon](https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Outline\n",
    "\n",
    "- **Pipeline**: At which stage of modelling should the pruning be done?\n",
    "\n",
    "- **Unstructured vs Structured**: From which parts of the model should the parameters be pruned?\n",
    "\n",
    "- **Criterion**: What is the quantitative metric to determining pruning?\n",
    "\n",
    "- **Prune Rate**: How much of network to prune in each iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pipeline\n",
    "\n",
    "- Pruning is done after the model is trained\n",
    "\n",
    "- Naturally, it leads to a higher loss of accuracy, thereby requiring **iterative training**\n",
    "\n",
    "- **Iterative Finetuning**: Pruned model is trained starting from the weights retained from the initial training phase with ***smaller learning rates*** [Han et al. 2015]\n",
    "\n",
    "- **Iterative Retraining**: Pruned model is trained starting from the weights retained from the initial training phase with the same learning rate schedule as was used in the training of the bigger model, a.k.a **Learning-rate Rewinding** [Renda et al. 2020]\n",
    "\n",
    "- **Iterative Rewinding**: Pruned model is trained with initial weights as that of the initialized model; weights and learning rates are all rewound to the initial values [Frankle et al. 2018]\n",
    "\n",
    "- **Prune before training**: The model is pruned before it is trained. This reduces the computational overhead related to iterative finetuning/retraining/rewinding [Lee et al. 2018]\n",
    "\n",
    "- **Pruning as an objective**: Some methods learn the sparse structure during training either by penalizing weights or by explicitly learning a pruning mask [Savarese et al. 2019]\n",
    "\n",
    "<img src=\"img/pipeline.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[[Han et al. 2015] Learing both Weights and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626.pdf)\n",
    "\n",
    "[[Renda et al. 2020] Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://arxiv.org/abs/2003.02389)\n",
    "\n",
    "[[Frankle et al. 2018] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)\n",
    "\n",
    "[[Lee et al. 2018] SNIP: Single-shot Network Pruning based on Connection Sensitivity](https://arxiv.org/abs/1810.02340)\n",
    "\n",
    "\n",
    "[[Savarese et al. 2019] Winning the Lottery with Continuous Sparsification](https://arxiv.org/abs/1912.04427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Unstructured vs Structured \n",
    "\n",
    "- **Unstructured Pruning**: Removes individual parameters, e.g, weights and biases\n",
    "    - Connections are the most fundamental units of a network - numerous enough to prune them in large quantities \n",
    "    - There are no constraints on which connections can be pruned\n",
    "    - Simple and intuitive\n",
    "    - Directly reduces FLOPs (floating-point operations per second) by removing individual connections or neurons [Han et al. 2015]\n",
    "    - ***Disadvantage*** - Most work shows the reduction in FLOPs, however, to actualize such gains, specialized hardware for sparse computation are required\n",
    "\n",
    "<img src=\"img/unstructured.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "[Image Source [Han et al. 2015] Learing both Weights and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Unstructured vs Structured \n",
    "    \n",
    "- **Structured Pruning**: Removes larger structures, e.g., convolution filters or kernels\n",
    "    - Applicable to specialized architectures, e.g., convolutional neural networks\n",
    "    - Final architectures do not require specialized hardwares \n",
    "    - Applications like object detection and segmentation needs intermediate representations. Thus, filter pruning techniques can be useful because the final models have low bandwidth for intermediate representations\n",
    "    - Differs from Network Architecture Search as it is a destructive approach \n",
    "    - [Anwar et al. 2015], [Li et al. 2016], [Wen et al. 2016], [Liu et al. 2017], [Hacene et al. 2019]\n",
    "\n",
    "[[Anwar et al. 2015] Structured Pruning of Deep Convolutional Neural Networks](https://arxiv.org/abs/1512.08571)\n",
    "\n",
    "[[Li et al. 2016] Pruning filters for efficient convnets.](https://arxiv.org/abs/1608.08710)\n",
    "\n",
    "[[Wen et al. 2016] Learning Structured Sparsity in Deep Neural Networks](https://arxiv.org/abs/1608.03665)\n",
    "\n",
    "[[Liu et al. 2017] Learning Efficient Convolution Networks through Network Slimming](https://arxiv.org/abs/1708.06519)\n",
    "\n",
    "[[Hacene et al. 2019] Attention Based Pruning for Shift Networks](https://arxiv.org/abs/1905.12300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Unstructured vs Structured \n",
    "    \n",
    "\n",
    "<img src=\"img/convolution-pruning.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[Image source: Neural Network Pruning 101](https://towardsdatascience.com/neural-network-pruning-101-af816aaea61)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "\n",
    "- We will consider the following notations\n",
    "\n",
    "    * $\\mathbf{x}$ is the input vector\n",
    "    * $\\mathbf{W}$ is the weight matrix \n",
    "    * $\\mathbf{b}$ is the bias vector\n",
    "    * $\\sigma$ is the activation function\n",
    "    * $f$ is the functional representation of a neural network\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = f(\\mathbf{x}) = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "- We use $\\mathbf{M}$ (of the same shape as $\\mathbf{W}$)  as a mask to zero-out weights. Thus, \n",
    "\n",
    "$$ \\mathbf{a} = f(\\mathbf{x}) = \\sigma(\\mathbf{M} \\odot \\mathbf{W}\\mathbf{x} + \\mathbf{b}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- **Weight magnitude**\n",
    "    * Widely used criterion that is simple and works well in practice\n",
    "    * Can be applied to individual weights\n",
    "    \n",
    "    $$ M_{i,j} = |\\mathbf{W}_{i,j}| \\leq \\lambda $$\n",
    "\n",
    "    * Can be applied to a group of weights, e.g., convolution kernels\n",
    "\n",
    "    $$ \\mathbf{M}_{l2} = \\frac{||\\mathbf{W}||_2}{||\\mathbf{W}||_0} \\leq \\lambda $$\n",
    "\n",
    "    $$ \\mathbf{M}_{l1} = \\frac{||\\mathbf{W}||_1}{||\\mathbf{W}||_0} \\leq \\lambda $$\n",
    "\n",
    "    * Above can also be done in conjunction with using $l1$ or $l2$ regularization\n",
    "    \n",
    "    * Use a learnable gate factor to completely switch off the connections (e.g, convolutional channels) [Liu et al. 2017]\n",
    "\n",
    "[[Liu et al. 2017] Learning Efficient Convolutional Networks through Network Slimming](https://arxiv.org/abs/1708.06519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- ** Activation value **\n",
    "    * **if the feature map is not useful, remove the weights that produce and use it**\n",
    "    * Example 1 (MLP): \n",
    "        * if a neuron in MLP is deactivated all the time, remove the associated weights \n",
    "        * For $B$ batches with $N_b$ samples in each batch, we compute the saliency score for the $k^{th}$ neuron of $l^{th}$ layer as  \n",
    "        \n",
    "        $$S_{avg}(\\mathbf{a}^l_{i,k}) = \\frac{1}{B}\\sum_{b}^{B} \\frac{1}{N_b}\\sum_{i}^{N_b} |\\mathbf{a}^l_{i,k}| $$\n",
    "        \n",
    "        * Thus, if $S_{avg} \\leq \\lambda$, remove $\\mathbf{W}^{l-1}_{k, :}$, i.e., the weights producing $\\mathbf{a}_l^k$ and $\\mathbf{W}^{l}_{:, k}$ weights using $\\mathbf{a}_l^k$.\n",
    "       \n",
    "        * Similarly, one can define a saliency score $S_{std}$ using standard deviation of $\\mathbf{a}^l_{i,k}$ across the batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- ** Activation value **\n",
    "    * **if the feature map is not useful, remove the weights that produce and use it**\n",
    "    \n",
    "    * Example 2 (CNN): \n",
    "        * Recall, a convolution layer has $C_l$ filters each with $C_{l-1}$ kernels containing $p \\times p$ parameters producing, thereby producing $C_{l}$ feature maps\n",
    "        \n",
    "        * Represent the $k^{th}$ feature map of the layer $l$ by $\\mathbf{z}_l^k$\n",
    "\n",
    "        * if a feature map is not active, remove the corresponding kernel that produces it and the kernels in the subsequent filters that uses it\n",
    "        \n",
    "        * We estimate saliency score of the $k^{th}$ filter in the $l^{th}$ layer, $\\mathbf{z}_k^l$ as \n",
    "    \n",
    "    $$ S_{avg}(\\mathbf{z}^l_{k}) =  \\frac{1}{B}\\sum_{b}^{B} \\frac{1}{N_b}\\sum_{i}^{N_b}\\Big|  \\frac{||\\mathbf{z}_{k,i}^l||_1}{||\\mathbf{z}_{k,i}^l||_0}   \\Big| $$\n",
    "    \n",
    "        * Thus, if $S_{avg} \\leq \\lambda$, remove the $k^{th}$ filter producing the $k^{th}$ feature map, and kernels in $l+1$ layer using the $k^{th}$ feature map\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- ** Gradient magnitude - Activation ** [Molchanov et al. 2016]\n",
    "    * Saliency of a feature map is computed as a product of gradient (w.r.t the feature map) and the feature map\n",
    "    * Example 1 (MLP):\n",
    "        * Compute saliency score for the activation $\\mathbf{a}^l_{i,k}$ as \n",
    "    \n",
    "    $$ S_{avg}(\\mathbf{a}^l_{i,k}) = \\frac{1}{B}\\sum_{b}^{B} \\frac{1}{N_b}\\sum_{i}^{N_b} \\Big| \\frac{\\delta \\mathcal{L}}{\\delta \\mathbf{a}^l_{i,k}} \\times \\mathbf{a}^l_{i,k} \\Big| $$\n",
    "    \n",
    "        * Thus, if $S_{avg} \\leq \\lambda$, remove $\\mathbf{W}^{l-1}_{k, :}$, i.e., the weights producing $\\mathbf{a}_l^k$ and $\\mathbf{W}^{l}_{:, k}$ weights using $\\mathbf{a}_l^k$\n",
    "       \n",
    "    \n",
    "[[Molchanov et al. 2016] Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- ** Gradient magnitudde - Activation ** [Molchanov et al. 2016]\n",
    "    * Saliency of a feature map is computed as a product of gradient (w.r.t the feature map) and the feature map   \n",
    "    * Example 2 (CNN):\n",
    "        * Compute saliency score for the feature map $\\mathbf{z}^l_{i,k}$ as \n",
    "    \n",
    "       $$ S_{avg}(\\mathbf{z}^l_{k}) = \\frac{1}{B}\\sum_{b}^{B} \\frac{1}{N_b}\\sum_{i}^{N_b} \\Big| \\frac{\\delta \\mathcal{L}}{\\delta \\mathbf{z}^l_{i,k}} \\odot \\mathbf{z}^l_{i,k} \\Big| $$\n",
    "       \n",
    "        * Thus, remove the $k^{th}$ filter producing the $k^{th}$ feature map, and kernels in $l+1$ layer using the $k^{th}$ feature map\n",
    "    \n",
    "    \n",
    "[[Molchanov et al. 2016] Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Criterion\n",
    "\n",
    "- **Average Percentage of Zeros (APoZ)** [Le Cun et al. 1990, Hu et al. 2016]\n",
    "    - ReLU activation imposes sparsity, therefore APoZ in $k^{th}$ feature map of the $l^{th}$ layer is used as a saliency score\n",
    "    \n",
    "\n",
    "- **FLOPs Regularization** [Molchanov et al. 2016]\n",
    "\n",
    "    - Minmizing FLOPs is an objective\n",
    "    \n",
    "    - Higher the FLOPs in producing $\\mathbf{z}^l_k$ more useful to prune the filter \n",
    "    \n",
    "    - Thus, if $S_l^{flops}$ is the FLOPs associated with $\\mathbf{z}^l_{i,k}$, we compute the FLOPs-regularized saliency for $\\mathbf{z}^l_{i,k}$ the as \n",
    "    \n",
    "    $$S_{FLOPs}(\\mathbf{z}^l_{k}) = S_{avg}(\\mathbf{z}^l_{k}) - \\lambda \\cdot S_l^{flops}$$ \n",
    "\n",
    "\n",
    "[[Le Cun et al. 1990] Optimal Brain Damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)\n",
    "\n",
    "[[Hu et al. 2016] Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures](https://arxiv.org/abs/1607.03250)\n",
    "\n",
    "[[Molchanov et al. 2016] Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning rate\n",
    "\n",
    "- How many parameters to prune? \n",
    "    * Could be a **number of parameters** each iteration\n",
    "    * Could be a **percentage of parameters** to prune each iteration\n",
    "\n",
    "- Should the prune rate be applied \n",
    "    * **locally**, i.e., the prune rate is applied to each layer, or\n",
    "    * **globally**, i.e., the prune rate is applied to all the parameters in the network\n",
    "\n",
    "[Image source: Neural Network Pruning 101](https://towardsdatascience.com/neural-network-pruning-101-af816aaea61)\n",
    "\n",
    "\n",
    "[[Tanaka et al. 2020] Pruning neural networks without any data by iteratively conserving synaptic flow](https://arxiv.org/abs/2006.05467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning rate\n",
    "\n",
    "- **Global Pruning** (right): might lead to better results, but it might lead to layer collapse (the entire layer is pruned, thereby preventing the backpropagation of errors) [Tanaka et al. 2020]\n",
    "\n",
    "\n",
    "<img src=\"img/prune_rate.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[Image source: Neural Network Pruning 101](https://towardsdatascience.com/neural-network-pruning-101-af816aaea61)\n",
    "\n",
    "\n",
    "[[Tanaka et al. 2020] Pruning neural networks without any data by iteratively conserving synaptic flow](https://arxiv.org/abs/2006.05467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Sparse Training\n",
    "\n",
    "- Step 1: Initialize a network with a random mask to prune certain connections of the network\n",
    "- Step 2: Train for one epoch\n",
    "- Step 3: Remove the weights of lower magnitude \n",
    "- Step 4: Regrow the same amount of weights in those layers\n",
    "\n",
    "[[Mocanu et al. 2018] Scalable training of artificial neural networks with adaptive sparse conneectivity inspired by network science](https://www.nature.com/articles/s41467-018-04316-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Sparse Training\n",
    "\n",
    "<img src=\"img/sparse-training.webp\" width=\"1000\">\n",
    "An illustration of the SET procedure. For each sparse connected layer, $SC^k$ (a), of an ANN at the end of a training epoch a fraction of the weights, the ones closest to zero, are removed (b). Then, new weighs are added randomly in the same amount as the ones previously removed (c). Further on, a new training epoch is performed (d), and the procedure to remove and add weights is repeated. The process continues for a finite number of training epochs, as usual in the ANNs training\n",
    "\n",
    "\n",
    "\n",
    "[[Mocanu et al. 2018] Scalable training of artificial neural networks with adaptive sparse conneectivity inspired by network science](https://www.nature.com/articles/s41467-018-04316-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Sparse Training\n",
    "\n",
    "\n",
    "- Mocanu et al. (2018) used local pruning, i.e., maintaining pruning rate per layer\n",
    "- Mostafa et al. (2019) used global pruning\n",
    "- Dettmers et al. (2019) and Evci et al. (2020) proposed novel paramater regrowing techniques\n",
    "\n",
    "\n",
    "[[Mocanu et al. 2018] Scalable training of artificial neural networks with adaptive sparse conneectivity inspired by network science](https://www.nature.com/articles/s41467-018-04316-3)\n",
    "\n",
    "[[Mostafa et al. 2019] Parameter Efficient Training of Deep Convolution Neural Networks by Dynamic Sparse Reparameterization](https://arxiv.org/abs/1902.05967)\n",
    "\n",
    "[[Dettmers et al. 2019] Sparse Networks from Scratch: Faster Training without Losing Performance](https://arxiv.org/abs/1907.04840)\n",
    "\n",
    "[[Evci et al. 2020] Rigging the Lottery: Making All Tickets Winners](https://proceedings.mlr.press/v119/evci20a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning while Training\n",
    "\n",
    "- Learn a pruning mask during training via a separate network:\n",
    "    - Huang et al. (2018) and He et al. (2018) trained the reinforcement learning agents to prune filters in CNNs\n",
    "    - Yamamoto et al. (2019) proposed using attention network before the layers of pre-trained CNN to identify filters that can be pruned\n",
    "\n",
    "[[Huang et al. 2018] Learning to Prune Filters in Convolutional Neural Networks](https://arxiv.org/pdf/1801.07365.pdf)\n",
    "\n",
    "[[He et al. 2018] AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/abs/1802.03494)\n",
    "\n",
    "[[Yamamoto et al. 2019] PCAS: Pruning Channels with Attention Statistics for Deep Network Compression](https://arxiv.org/pdf/1806.05382.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning as an objective\n",
    "\n",
    "- **Penalty-based methods** or **Mask learning through auxiliary parameters**\n",
    "    - Ideally, $L_0$ regularization should do the job, but it is non-differentiable.\n",
    "    - Use differentiable penalty schemes to reduce weights to 0, e.g., L1-regularization\n",
    "    - **Modified $L_{1/2}$ regularization proposed** by Chang et al. (2018) lends differentiability to $L_{1/2}$ regularizer\n",
    "  \n",
    "\n",
    "[[Chang et al. 2018] Prune deep neural networks with the modified L1/2 penalty](https://ieeexplore.ieee.org/iel7/6287639/6514899/08579132.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning as an objective\n",
    "\n",
    "- **Penalty-based methods**\n",
    "     - **Group Lasso** [Meier et al. 2008]: Applies LASSO regression to the group of parameters. \n",
    "        - Assuming $\\theta_G$ as  $m$ independent set of parameters , $\\theta_G = \\{\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(m)}\\}$\n",
    "        - With $p_l$ as the number of parameters in $\\theta^{(l)}$, the loss function becomes\n",
    "        $$ \\mathcal{L}(\\mathbf{X}, \\mathbf{y}) + \\lambda \\sum_{l=1}^m \\sqrt{p_l} \\cdot \\big|\\big|\\theta^{(l)}\\big|\\big|_2$$\n",
    "        \n",
    "        - $m=1$ is equivalent to Ridge regression\n",
    "        - $m=n$ is equaivalent to Lasso regression\n",
    "        - Intermediate values of m is commonly known as Group Lasso\n",
    "\n",
    "\n",
    "[[Meier et al. 2008] The group lasso for logistic regression](http://people.ee.duke.edu/~lcarin/lukas-sara-peter.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning as an objective\n",
    "\n",
    "- **Penalty-based methods**      \n",
    "    - Various ways to **target the above penalties selectively**  \n",
    "        - Carreira-Perpin et al. (2018) explore the subset of weights to prune in \"compression\" step \n",
    "        - Tessier et al. (2021) proposed Selective Weight Decay (SWD) to strongly penalize the weights below a certain threshold \n",
    "\n",
    "[[Tessier et al. 2022] Rethinking Weight Decay for Efficient Neural Network Pruning](https://arxiv.org/pdf/2011.10520.pdf)\n",
    "\n",
    "[[Carreira-Perpin et al. 2018] \"Learning Compression\" Algorithms for Neural Net Pruning](https://faculty.ucmerced.edu/mcarreira-perpinan/papers/cvpr18.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning as an objective\n",
    "\n",
    "- **Penalty-based methods**      \n",
    "    - Various ways to **target the above penalties selectively**  \n",
    "        - Tessier et al. (2021) proposed Selective Weight Decay (SWD) to stongly penalize the weights below a certain threshold \n",
    "    \n",
    "\n",
    "<img src=\"img/swd.png\" width=\"1000\">\n",
    "\n",
    "[[Tessier et al. 2022] Rethinking Weight Decay for Efficient Neural Network Pruning](https://arxiv.org/pdf/2011.10520.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Pruning as an objective\n",
    "\n",
    "- **Penalty-based methods**      \n",
    "    - **Bayesian methods **\n",
    "        - Molchanov et al. (2017) uses variational dropout to learn individial dropout rates such that high dropout rates effectively prunes the weight \n",
    "        - Louizos et al. (2017) uses sparsity inducing hierarchical priors to prune nodes (set of weights)\n",
    "        - Neklyudov et al.(2017) applies dropout-based regularization to structured elements, e.g, neurons, convolutional layers. \n",
    "  \n",
    "   \n",
    "[[Molchanov et al. 2017] Variational Droupout Sparsifies Deep Neural Networks](https://arxiv.org/pdf/1701.05369.pdf)\n",
    "\n",
    "[[Louizos et al. 2017] Bayesian Compression for Deep Learning](https://arxiv.org/abs/1705.07283)\n",
    "\n",
    "[[Neklyudov et al. 2017] Structured Bayesian Pruning via Log-Normal Multiplicative Noise](https://arxiv.org/abs/1705.07283)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pruning: Lottery Ticket Hypothesis\n",
    "\n",
    "- Frankle et al. (2018) empirically investigated whether there exists a smaller subnetwork within a dense neural network that performs just as well as the original network (within the same time budget)\n",
    "\n",
    "\n",
    "- The **experiments** involved the following steps:\n",
    "    1. Intialize a sufficiently deep neural network with random weights and train it for some iterations\n",
    "    2. Prune the weights with the least magnitude (threshold is a hyperparameter)\n",
    "    3. Re-initialize the remaining weights randomly\n",
    "    4. Train the model for some number of iterations again.\n",
    "    5. Repeat 2, 3, and 4 until the desired level of sparsity (70-80% in their experiments)\n",
    "\n",
    "\n",
    "- **Findings**: With the correct choice of hyperparameters (number of iterations, choice of threshold, choice of sparsity structurre), the above procedure finds a much smaller subnetwork (lottery ticket) that performs just as well as the larger unpruned network\n",
    "\n",
    "\n",
    "\n",
    "[[Frankle et al. 2018] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation\n",
    "\n",
    "- Hinton et al. (2015) proposed Knowledge Distillation (KD) as a technique to \"distill\" the knowledge of a bigger neural network to smaller and simpler neural networks\n",
    "\n",
    "<img src=\"img/kd.png\" width=\"1000\">\n",
    "\n",
    "[[Hinton et al. 2015] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation\n",
    "\n",
    "- KD requires modifying the loss function for the simpler network\n",
    "\n",
    "- For example, for the classification task, if $y$ is the one-hot encoded target vector and $z_l$ is the vector of logits output by the larger pre-trained network, the loss function for the smaller network will be\n",
    "\n",
    "$$\n",
    "\\alpha \\times CE(z_{s}, y) + (1-\\alpha) \\times KL(z_{s}, Softmax(z_l/T)),\n",
    "$$\n",
    "here, T is the temperature parameter, and $\\alpha \\in [0, 1]$ is the mixing parameter\n",
    "\n",
    "- Various ways of distilation have been studied since 2015. Refer to this [blog post](https://neptune.ai/blog/knowledge-distillation) for a complete overview. \n",
    "\n",
    "\n",
    "[[Hinton et al. 2015] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "- **Weight Hashing**\n",
    "    - Group weights into buckets to reduce model storage\n",
    "    - Thus, final model is represented by the weight values and indices\n",
    "    - However, at the inference time, these weights need to be restored for computation, so it might not lead to savings in inference time \n",
    "\n",
    "- **Weight Quantization**:\n",
    "    - Quantize the weights into integers, i.e., binary, ternary, etc. \n",
    "    - Requires special formulations for training neural networks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "- Chen et al. (2015) proposed **HashedNets**\n",
    "    - Before training, network weights are hashed into different groups \n",
    "    - These groups have the same value for the parameters\n",
    "    - Thus, the storage is consumed only by the shared weights and their corresponding indices \n",
    "    - However, during the inference, these weights will be restored to their original indices, there is less impact on the the run-time memory and the inference time \n",
    "    \n",
    "- Courbariaux et al. (2016) restrict the weights to binary, i.e., $\\{-1, +1\\}$, or and Rastegari et al. (2016) restricted it to teriatiary weights , i.e., $\\{-1, 0, 1\\}$\n",
    "    - Large model size savings \n",
    "    - Significant speedups\n",
    "    - Moderate accuracy loss \n",
    "\n",
    "[[Chen et al. 2015] Compressing Neural Networks with the Hashing Trick](https://arxiv.org/abs/1504.04788)\n",
    "\n",
    "[[Courbariaux et al. 2016] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activtations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)\n",
    "\n",
    "[[Rastegari et al. 2016] XNOR-Net: ImageNet Classifcation Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now open the following workbook `pruning-practical.ipynb` to learn how to build an InceptionNet\n",
    "\n",
    "<img src=\"img/jupyter.png\" width=\"250px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
