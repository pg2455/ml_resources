{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore autoencoders to perform the following three tasks\n",
    "\n",
    "- Data compression\n",
    "- Data generation\n",
    "- Data interpolation\n",
    "\n",
    "We will be using [MNIST digit dataset](https://en.wikipedia.org/wiki/MNIST_database), which is easily available through [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.MNIST). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will need the following python packages \n",
    "\n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. torch\n",
    "4. torchvision\n",
    "\n",
    "Please follow the instructions [here](https://pytorch.org/) to install the last two libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# fix seed for reproducibility \n",
    "rng = np.random.RandomState(1)\n",
    "torch.manual_seed(rng.randint(np.iinfo(int).max))\n",
    "\n",
    "# it is a good practice to define `device` globally\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU -> using CPU:\", device)\n",
    "\n",
    "# to store images\n",
    "img_dir = pathlib.path(\"./images\")\n",
    "if not img_dir.exists():\n",
    "    img_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "PyTorch has simple-to-use functions that downloads and loads the datasets. We will use these functions to streamline our deep learning pipeline.\n",
    "\n",
    "Checkout other image datasets at [torch.datasets](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data in data folder. It will create this folder if it doesn't exist\n",
    "torchvision.datasets.MNIST(root=\"./data/\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "\n",
    "We will be carrying out simple investigations as we did in the practical of denoising autoencoders. \n",
    "Specifically, we are interested in finding out the following :\n",
    "\n",
    "How does the data look like?\n",
    "\n",
    "- What is the `type` of data?\n",
    "- What does each element of data represent?\n",
    "- What are the constituent parts of each element?\n",
    "- How is the image represented?\n",
    "- What do we use to plot an image?\n",
    "- How do we use the image in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = torchvision.datasets.MNIST(root=\"./data/\", train=True) # only load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Type of data is the class\\n\", type(data))\n",
    "# print(\"\\nEach element of the data is\\n\", type(data[0]))\n",
    "# print(\"\\nA single element is\\n\", data[0],\"\\n\\nfirst element is the image and the second element is the category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data\n",
    "\n",
    "We know that the image data is present in `PIL.Image.Image` format. We need to convert it to array of pixel values to operate on it. We will also be displaying these images. `matplotlib.pyplot` has a simple function `plt.imshow()` to display matrix as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = transforms.ToTensor()(data[10][0])\n",
    "# print(f\"Raw image to tensor shape: {img.shape}\")\n",
    "\n",
    "# # To plot this, we need a 2D array, so we use squeeze_(0) to remove first dimension\n",
    "# img = img.squeeze_(0)\n",
    "# print(f\"2D image tensor shape: {img.shape}\")\n",
    "\n",
    "# # how to display the image \n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "n_samples = len(data)\n",
    "n_vis = 20\n",
    "\n",
    "nrows = 4\n",
    "ncols = math.ceil(n_vis/nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,6), dpi=100)\n",
    "\n",
    "idxs = np.random.randint(low=0, high=n_samples, size=n_vis)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    ax = axs[i%nrows, i//nrows]\n",
    "    img = transforms.ToTensor()(data[idx][0]).squeeze_(0)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "_ = fig.suptitle(f\"Original MNIST dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Compression: Simple setup & Vanilla Deep Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding a compact representation for digits in our dataset. \n",
    "We will choose our latent dimension to be a 2D plane so that it's easy to visualize. \n",
    "However, depending on the usecase (e.g, classification, semi-supervised classification, etc.), one might choose a higher dimensional latent space.\n",
    "\n",
    "In this section, will be setting up our code to test it on various autoencoders discussed in the lessons. \n",
    "We will be using a vailla Autoencoder to build our necessary functions, and then we will reuse these functions with different autoencoders.\n",
    "\n",
    "To do this, we will build an autoencoder that comprises of an encoder and a decoder. \n",
    "\n",
    "In the following block of code, we will be implementing a simple **deep autoencoder**.\n",
    "We will be using linear layers to process our images, although you are free to chose the architecture you like.\n",
    "It consists of two blocks -\n",
    "- Encoder\n",
    "- Decoder\n",
    "\n",
    "We will be following the same pattern to implement other autoencoders in the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.in_dim = input_shape[0] * input_shape[1]\n",
    "        \n",
    "        # encoder \n",
    "        self.encoder = nn.Sequential(\n",
    "            ## YOUR DESIGN OF ARCHITECTURE\n",
    "        )\n",
    "        \n",
    "        # decoder \n",
    "        self.decoder = nn.Sequential(\n",
    "            ## YOUR DESIGN OF ARCHITECTURE\n",
    "            ## MAKE SURE LAST LAYER's OUTPUT IS IN THE CORRECT RANGE\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        # ANY PREPROCESSING ON x\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        # ANY POST PROCESSING ON x_hat\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return z, x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write a function to fetch the memory statistics of our model.\n",
    "While building different autoencoder models, it will be important to ensure they have all similar number of learnable parameters. This will ensure fair comparison of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_size(model):\n",
    "    \"\"\"\n",
    "    Get model size in GB (as str: \"N GB\")\n",
    "    \"\"\"\n",
    "    mem_params = sum(\n",
    "        [param.nelement() * param.element_size() for param in model.parameters()]\n",
    "    )\n",
    "    mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs\n",
    "    return f\"{mem / 1e9:.4f} GB\"\n",
    "\n",
    "def num_params(model):\n",
    "    \"\"\"\n",
    "    Print number of parameters in model's named children\n",
    "    and total\n",
    "    \"\"\"\n",
    "    s = \"Number of parameters:\\n\"\n",
    "    n_params = 0\n",
    "    for name, child in model.named_children():\n",
    "        n = sum(p.numel() for p in child.parameters())\n",
    "        s += f\"  • {name:<15}: {n}\\n\"\n",
    "        n_params += n\n",
    "    s += f\"{'total':<19}: {n_params}\"\n",
    "\n",
    "    return s\n",
    "\n",
    "def pp_model_summary(model):\n",
    "    print(num_params(model))\n",
    "    print(f\"{'Total memory':<18} : {mem_size(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fair comparison of different models, we need to make sure that they have the same parameters. The above two functions will come in handy to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK HOW MANY PARAMETERS DOES THE MODEL HAVE\n",
    "# model_ae = DeepAutoencoder(input_shape, 2)\n",
    "# pp_model_summary(model_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "PyTorch uses `torch.utils.data.Dataset` class to load data in parallel on multiple CPUs. It enables faster loading of the batch of data. It then uses `torch.utils.data.DataLoader` to combine these loaded data points together into a batch. These batches are then used as an input to the models.\n",
    "\n",
    "For this reason, we need to customize `__len__` and `__getitem__` functions in the class. This will help us do any necessary preprocessing on the images before using them as an input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = transforms.ToTensor()(self.data[index][0]).squeeze_(0)\n",
    "        label = self.data[index][1] # to be used for visualization\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss \n",
    "\n",
    "We will create a function that will take following inputs:\n",
    "\n",
    "1. An autoencoder instance\n",
    "2. `torch.utils.data.DataLoader` instance \n",
    "3. loss function to compute loss based on the inputs \n",
    "4. optimizer (optional)\n",
    "\n",
    "This function returns the mean loss on this data. We will be redesigning parts of this model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, data, loss_fn, optimizer=None):\n",
    "    \"\"\"\n",
    "    Trains the `model` on `data`\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): model to process the data\n",
    "        data (torchvision.datasets): MNIST dataset\n",
    "        loss_fn (fn): how you want to compute the loss\n",
    "        optimizer (torch.optim): Optimizer of choice \n",
    "    \n",
    "    Returns:\n",
    "        (float): mean loss \n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = 0\n",
    "    running_loss = 0\n",
    "    for batch_img, _ in data:\n",
    "        # transfer to GPU if avaiable\n",
    "        x = batch_img.to(device)\n",
    "\n",
    "        n_samples += x.shape[0]\n",
    "        \n",
    "        # compute loss \n",
    "        loss = loss_fn(model, x)\n",
    "        \n",
    "        # backward pass \n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / n_samples\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Finally, we will be running the training for `n_epochs` number of epochs. \n",
    "Each epoch consists of doing a backward pass on the subset of training data and evaluating the model on the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If one is concerned with good representation of the training data only (i.e., generalization outside training data is not of concern), we need not split our dataset into train and validation. \n",
    "However, in this tutorial, we will use the validation dataset to pick a model with the best generalization error. \n",
    "\n",
    "We will create a function `train` to conveniently call it with various models and loss functions that we can experiment with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, epochs=30, append_title=\"\"):\n",
    "    \"\"\"\n",
    "    Trains the `model` using `process`\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): model to process the data\n",
    "        data (torchvision.datasets): MNIST dataset\n",
    "        optimizer (torch.optim): Optimizer of choice \n",
    "        process (fn): function defining how each epoch is handled\n",
    "    \n",
    "    Returns:\n",
    "        best_model (nn.Module): returns the model with the best training loss \n",
    "        loss_plot (matplotlib.subplots): a plot for how training loss varied over the epochs\n",
    "    \"\"\"\n",
    "    # create a model directory to store the best model\n",
    "    model_dir = pathlib.Path(\"./models\").resolve()\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "    n_epochs = epochs\n",
    "    epoch_size=200\n",
    "    batch_size=128\n",
    "\n",
    "    best_train_loss = np.inf\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"@ epoch {epoch}\", end=\"\")\n",
    "\n",
    "        # training loss\n",
    "        idxs = rng.choice(len(data), epoch_size * batch_size, replace=True)\n",
    "        train_data = torch.utils.data.DataLoader(Dataset([data[idx] for idx in idxs]), batch_size=batch_size, num_workers=4)\n",
    "        train_loss = process(model, train_data, loss_fn, optimizer)\n",
    "        \n",
    "        # save\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss \n",
    "            torch.save(model.state_dict(), model_dir / f\"best_{model.__class__.__name__}_L_{model.latent_dim}{append_title}.ckpt\")\n",
    "    \n",
    "        # log\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"\\ttrain_loss: {train_loss: .7f}\")\n",
    "\n",
    "    # load the best model\n",
    "    model = model.__class__(model.input_shape, model.latent_dim)\n",
    "    model.load_state_dict(torch.load(model_dir / f\"best_{model.__class__.__name__}_L_{model.latent_dim}{append_title}.ckpt\"))\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model, train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create a string out of model; to be used as a title for visualizing\n",
    "def get_model_title(model):\n",
    "    \"\"\"returns the string title for model\"\"\"\n",
    "    return f\"{model.__class__.__name__} (latent_dim={model.latent_dim})\"\n",
    "\n",
    "\n",
    "def visualize_losses(train_losses, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize the loss trajectory \n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): elements are float\n",
    "        title (str): title to appear at the top of the figure\n",
    "    \"\"\"\n",
    "    # plot losses\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=100)\n",
    "\n",
    "    axs.plot(train_losses, color=\"#BDD9BF\", marker=\"x\", label=\"Train loss\")\n",
    "    axs.set_xlabel(\"Epochs\", fontsize=20)\n",
    "    axs.legend(prop={\"size\":15})\n",
    "\n",
    "    # tick size\n",
    "    for tick in axs.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(15)\n",
    "\n",
    "    for tick in axs.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(15)\n",
    "        \n",
    "    fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reconstructed images\n",
    "\n",
    "Here we will write a function that can help us visualize the quality of reconstruction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_reconstructed_images(model, append_title=\"\"):\n",
    "    \"\"\"\n",
    "    Randomly samples images and compare them with reconstructed imates usiing `model`\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): model to use to reconstruct the images\n",
    "    \"\"\"\n",
    "    model = model.cpu()\n",
    "    n_samples = len(data)\n",
    "    n_vis = 20\n",
    "\n",
    "    nrows = 4\n",
    "    ncols = math.ceil(n_vis/nrows)\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,8), dpi=100)\n",
    "\n",
    "    idxs = np.random.randint(low=0, high=n_samples, size=n_vis)\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        ax = axs[i%nrows, i//nrows]\n",
    "\n",
    "        x = transforms.ToTensor()(data[idx][0]).squeeze_(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z, x_hat = model(x)\n",
    "            x_hat = x_hat.squeeze_(0)\n",
    "\n",
    "        filler =  np.ones((img.shape[0], 10))\n",
    "        concat_img = np.concatenate((x, filler, x_hat), axis=1)\n",
    "\n",
    "        ax.imshow(concat_img)\n",
    "        ax.axis('off')\n",
    "\n",
    "    title = f\"{get_model_title(model)}{append_title}\"\n",
    "    _ = fig.suptitle(f\"Original MNIST & recostructed images by {title} \")\n",
    "    \n",
    "    fig.savefig(f\"images/recons_imgs_{title}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "Here we will write a function that can help us visualize 2D embeddings of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_representations(model, n=1000, seed=1):\n",
    "    \"\"\"\n",
    "    Randomly samples data from x_train, and returns their encoded representation using `model`\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): model to call `encode` on\n",
    "        n (int): number of samples to use \n",
    "    \n",
    "    Returns:\n",
    "        points (list): encoded representation of random samples \n",
    "        labels (list): corresponding labels\n",
    "    \"\"\"\n",
    "    model = model.cpu()\n",
    "    n_samples = len(data)\n",
    "     \n",
    "    np.random.seed(seed)\n",
    "    idxs = np.random.randint(low=0, high=n_samples, size=n)\n",
    "\n",
    "    # encode\n",
    "    points = []\n",
    "    labels = []\n",
    "    for idx in idxs:\n",
    "        \n",
    "        img, y = data[idx]\n",
    "        img =  transforms.ToTensor()(img).squeeze_(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = model.encode(img).squeeze_(0).numpy().tolist()\n",
    "\n",
    "        points.append(z)\n",
    "        labels.append(y)\n",
    "    \n",
    "    return points, labels\n",
    "\n",
    "\n",
    "# use t-SNE for projecting to 2d: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "def project_points_on_2D(points):\n",
    "    \"\"\"\n",
    "    Returns the 2D projection of `points`\n",
    "    \n",
    "    Args:\n",
    "        poitns (list): some high dimensional points \n",
    "    \n",
    "    Returns:\n",
    "        (list): 2D representation of the points \n",
    "    \"\"\"\n",
    "    if len(points[0]) <=2 :\n",
    "        return list(points)\n",
    "\n",
    "    from sklearn.manifold import TSNE  \n",
    "    points = TSNE(n_components=2).fit_transform(np.array(points))\n",
    "    return points.tolist()\n",
    "    \n",
    "\n",
    "def vis2D(model, n=6000, seed=1, append_title=\"\"):\n",
    "    \"\"\"\n",
    "    visualizes 2D representation of the points encoded using `model`.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to call `encode` on\n",
    "        n (int): number of samples to use     \n",
    "    \"\"\"\n",
    "    points, labels = get_latent_representations(model, n, seed)\n",
    "    \n",
    "    if len(points[0]) > 2:\n",
    "        points = project_points_on_2D(points)\n",
    "    \n",
    "    # visualize\n",
    "    COLORS = ['#208ea3', '#62bb35', '#8d9f9b', '#e8384f', '#4178bc', '#fd817d', '#fdae33', '#a4c61a', '#37a862', '#7471f6', \"#ea4e9d\"]\n",
    "    colormap = {i:c for i,c in enumerate(COLORS)}\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,8), dpi=100)\n",
    "\n",
    "    x, y = zip(*points)\n",
    "    colors = [colormap[l] for l in labels]\n",
    "    ax.scatter(x, y, s=20, label=labels, c=colors)\n",
    "    \n",
    "    # legend\n",
    "    legend = []\n",
    "    for i, c in colormap.items():\n",
    "        legend.append(Line2D([0, 1], [0, 0], color=c, label=f\"{i}\", linewidth=5))\n",
    "    \n",
    "    ax.legend(handles=legend, ncol=1, loc=\"upper right\", fontsize=15, fancybox=True)\n",
    "                          \n",
    "     # title\n",
    "    title = f\"{get_model_title(model)}{append_title}\"\n",
    "    _ = fig.suptitle(f\"Latent representation of digits by {title} \")\n",
    "    fig.savefig(f\"images/vis2d_{title}.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full setup\n",
    "\n",
    "Finally, we only need to use the above defined functions in the following way to check the performance of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our final setup (vanilla AE)\n",
    "def reconstruction_loss(model, x):\n",
    "    z, x_hat = model(x)\n",
    "    return ## YOUR LOSS FUNCTION FOR RECONSTRUCTION GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepae, losses = train(DeepAutoencoder(input_shape, 2), reconstruction_loss, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "vis_reconstructed_images(model_deepae)\n",
    "vis2D(model_deepae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- What do we observe about the spatial arrangement of clusters?\n",
    "- Are they will separated?\n",
    "- What will it mean to pick a random point in the 2D space and use it as an input to the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation: Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(DeepAutoencoder):\n",
    "    \n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super().__init__(input_shape, latent_dim)\n",
    "        \n",
    "        # we will overwrite the encoder \n",
    "        self.encoder = nn.Sequential(\n",
    "            ## YOUR ENCODER ARCHITECTURE \n",
    "            ## TRY TO KEEP IT SIMILAR TO DEEPAUTOENCODER\n",
    "        )\n",
    "        \n",
    "        self.mu_head = nn.Sequential(\n",
    "             ## YOUT ARCHITECTURE TO COMPUTE MU\n",
    "        )\n",
    "        \n",
    "        self.log_var_head = nn.Sequential(\n",
    "             ## YOUT ARCHITECTURE TO COMPUTE log_var\n",
    "        )\n",
    "        \n",
    "    def encode_dist(self, x):\n",
    "        ## YOUR CODE \n",
    "        return mu, log_var\n",
    "    \n",
    "    def sample(self, mu, log_var):\n",
    "        # we sample from a normal distribution and use mu and log_var for appropriate transformation\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        return mu + torch.exp(0.5*log_var) * epsilon\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.sample(*self.encode_dist(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationalAutoencoder(input_shape, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our model has the same number of parameters as `model_deepae`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(model, x, beta=1):\n",
    "    \"\"\"\n",
    "    Computes vae loss\n",
    "    \"\"\"\n",
    "    mu, log_var = model.encode_dist(x)\n",
    "    x_hat = model.decode(model.sample(mu, log_var))\n",
    "    \n",
    "    ## CHECK THE SLIDES TO COMPUTE KL DIVERGENCE BETWEEN standard Normal and Normal with mu and var\n",
    "    variational_loss = ## YOUR CODE GOES HERE\n",
    "    \n",
    "    return nn.BCELoss(reduction=\"sum\")(x_hat, x) + beta * variational_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VariationalAutoencoder(input_shape, 2)\n",
    "\n",
    "\n",
    "# some sources suggest warming up without KL before introducing KL term ... \n",
    "# print(\"Warming up without KL:\\n\")\n",
    "# vae_loss_fn = lambda model, x: vae_loss(model, x, 0)\n",
    "# model_vae, losses = train(model_vae, vae_loss_fn, epochs=10)\n",
    "\n",
    "print(\"\\ntraining with KL:\\n\")\n",
    "beta = 1\n",
    "vae_loss_fn = lambda model, x: vae_loss(model, x, beta)\n",
    "model_vae, losses = train(model_vae, vae_loss_fn, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_title=f\"kl_{beta:.2f}\"\n",
    "vis_reconstructed_images(model_vae, append_title=append_title)\n",
    "vis2D(model_vae, append_title=append_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate random points in the latent space to see how the decoder can be used to generate data points\n",
    "we will use both of the above models to generate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model, append_title=\"\", n_gen=20):\n",
    "    n_gen = n_gen\n",
    "    z = torch.randn((n_gen, model.latent_dim))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_hat = model.decode(z)\n",
    "\n",
    "    n_rows = 4\n",
    "    n_cols = math.ceil(n_gen/n_rows)\n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12,8), dpi=100)\n",
    "    for i in range(n_gen):\n",
    "        ax = axs[i%n_rows, i//n_rows]\n",
    "        ax.imshow(x_hat[i, :])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    title = f\"{get_model_title(model)}{append_title}\"\n",
    "    _ = fig.suptitle(f\"Decoded representations of random points in the latent space {title}\")\n",
    "    fig.savefig(f\"images/rand_data_{title}.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data(model_vae, append_title=f\"vae_{beta:0.2f}\")\n",
    "generate_data(model_deepae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Which model seems to be good at generating better data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Interpolation using VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to see how the interpolation results look like for different models. \n",
    "For this we will write a function to be called on different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick some pairs of datapoints from the dataset\n",
    "n_interpolate = 5\n",
    "idxs = rng.randint(len(data), size=(n_interpolate, 2))\n",
    "dataset = Dataset(data)\n",
    "in1 = torch.cat([dataset[idx][0].unsqueeze_(0) for idx in idxs[:, 0]], dim=0)\n",
    "in2 = torch.cat([dataset[idx][0].unsqueeze_(0) for idx in idxs[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(in1, in2, model, append_title):\n",
    "    \"\"\"\n",
    "    Interpolates latent representations on `in1` and `in2`\n",
    "    \n",
    "    \"\"\"\n",
    "    alphas = np.arange(0, 1.01, 0.2)\n",
    "    x_hats = []\n",
    "    with torch.no_grad():\n",
    "        z1 = model.encode(in1)\n",
    "        z2 = model.encode(in2)\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            z = (1- alpha) * z1 + alpha * z2\n",
    "            x_hat = model.decode(z)\n",
    "            x_hats.append(x_hat)\n",
    "    \n",
    "    # visualize\n",
    "    fig, axs = plt.subplots(nrows=in1.shape[0], ncols=len(alphas) + 2, figsize=(12, 8), dpi=100)\n",
    "    \n",
    "    # plot originals \n",
    "    for j in range(in1.shape[0]):\n",
    "        axs[j, 0].imshow(in1[j, :])\n",
    "        axs[j, -1].imshow(in2[j, :])\n",
    "        \n",
    "        axs[j, 0].axis('off')\n",
    "        axs[j, -1].axis('off')\n",
    "    \n",
    "    # plot decoded images\n",
    "    for j in range(in1.shape[0]):\n",
    "        for i, _ in enumerate(alphas):\n",
    "            axs[j, i+1].imshow(x_hats[i][j, :])\n",
    "            axs[j, i+1].axis('off')\n",
    "    \n",
    "    ## \n",
    "    title = f\"{get_model_title(model)}{append_title}\"\n",
    "    _ = fig.suptitle(f\"Data interpolation for {title}\")\n",
    "    fig.savefig(f\"images/interpolated_{title}.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(in1, in2, model_vae, append_title=f\"vae_{beta:0.2f}\")\n",
    "interpolate(in1, in2, model_deepae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Can we say anything about the quality of data interpoalation for the two models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkenv",
   "language": "python",
   "name": "dkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
