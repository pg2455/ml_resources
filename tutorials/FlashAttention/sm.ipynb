{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwards and Backprop through Online Attention\n",
    "\n",
    "At a high level, **attention ([Vaswani et al.](https://arxiv.org/abs/1706.03762)) is a mechanism that allows a neural network to focus on the most relevant pieces of information when making a decision.** Instead of treating all input tokens (words, pixels, etc.) equally, it learns to weigh them differently depending on the current context.\n",
    "\n",
    "A useful way to think of it:\n",
    "\n",
    "* The network first maps the input into a high-dimensional space — a kind of *library of representations*.\n",
    "* Then, when the model needs to produce an output (like predicting the next word), attention acts like a **search engine inside the model**. It asks: *“Given the current context, which entries in this library are most useful right now?”*\n",
    "* The result is a weighted combination of those entries, so the model doesn’t just recall one thing, but **blends together the most relevant information** for the task at hand.\n",
    "\n",
    "Attention has been the engine behind much of the progress in LLMs. But the mechanism comes with a cost: computing attention in the straightforward way requires **quadratic time and memory in the sequence length**. As models stretch into contexts of tens or even hundreds of thousands of tokens, this quadratic blow-up quickly becomes the bottleneck.\n",
    "\n",
    "Early research explored many clever approximations—low-rank projections, kernel tricks, sparsity patterns—but most came with a trade-off: efficiency at the cost of exactness.\n",
    "\n",
    "What’s remarkable is that the most effective solution turned out to be almost trivial: compute attention online, piece by piece, without ever materializing the full attention matrix.\n",
    "\n",
    "In this post, we’ll unpack what that means, why it works, and how both the forward and backward passes of attention can be carried out in a streaming fashion. \n",
    "To avoid the memory overhead of storing large intermediate structures (like the full score matrix), we’ll also show how to derive gradients through attention online: processing only small chunks of keys, values, and queries at a time.\n",
    "\n",
    "I’ll use code examples throughout to make these ideas concrete.\n",
    "\n",
    "Much of this discussion is inspired by the appendices of [FlashAttention](https://github.com/Dao-AILab/flash-attention), which achieves single-GPU efficiency by fusing operations and carefully managing memory, and by [RingAttention](https://arxiv.org/abs/2310.01889), which extends the same principle across GPUs by overlapping partial computations with communication of keys and values.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "We start with three matrices:\n",
    "\n",
    "* **Queries** $Q \\in \\mathbb{R}^{N \\times d}$\n",
    "* **Keys** $K \\in \\mathbb{R}^{M \\times d}$\n",
    "* **Values** $V \\in \\mathbb{R}^{M \\times d}$\n",
    "\n",
    "Here, $d$ is the hidden dimension, $N$ is the number of query tokens, and $M$ is the number of key/value tokens.\n",
    "\n",
    "The attention output is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attn}(Q, K, V) = \\text{Softmax}(\\tau QK^\\top) V\n",
    "$$\n",
    "\n",
    "where $\\tau = \\frac{1}{\\sqrt{d}}$ is a scaling factor to keep dot products numerically stable.\n",
    "\n",
    "Breaking it down step by step:\n",
    "\n",
    "1. **Similarity scores:**\n",
    "\n",
    "   $$\n",
    "   S = QK^\\top\n",
    "   $$\n",
    "\n",
    "   Each entry $S_{ij}$ measures how much query $i$ attends to key $j$.\n",
    "\n",
    "2. **Attention weights:**\n",
    "\n",
    "   $$\n",
    "   P = \\text{Softmax}(\\tau S)\n",
    "   $$\n",
    "\n",
    "   The softmax turns raw scores into probabilities, so each query distributes its attention across all keys.\n",
    "\n",
    "3. **Weighted aggregation:**\n",
    "\n",
    "   $$\n",
    "   O = PV\n",
    "   $$\n",
    "\n",
    "   Finally, each query’s output is a weighted sum of the values, with weights given by $P$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we’ll manually implement the attention output as `O_manual` and verify that it matches PyTorch’s built-in result `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O == O_manual:  True\n"
     ]
    }
   ],
   "source": [
    "B = 10\n",
    "L = 20\n",
    "D = 16\n",
    "Q = torch.randn(B, L, D, requires_grad=True)\n",
    "K = torch.randn(B, L, D, requires_grad=True)\n",
    "V = torch.randn(B, L, D, requires_grad=True)\n",
    "\n",
    "tau = 1. / math.sqrt(D)\n",
    "\n",
    "# Using PyTorch Module\n",
    "# API expects [B, heads, L, D]; here head dim = 1\n",
    "O = F.scaled_dot_product_attention(Q.unsqueeze(1), K.unsqueeze(1), V.unsqueeze(1))\n",
    "O = O.squeeze(1)\n",
    "\n",
    "# Handcoded batch attention\n",
    "scores = tau * torch.matmul(Q, K.transpose(-2, -1))\n",
    "sm = torch.softmax(scores, dim=-1)\n",
    "O_manual = torch.matmul(sm, V)\n",
    "\n",
    "print(\"O == O_manual: \", torch.allclose(O, O_manual, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass in Online Fashion\n",
    "\n",
    "So far, we assumed the entire $Q, K, V$ matrices fit in memory at once. But in practice, this isn’t always possible:\n",
    "\n",
    "* On a single GPU, **memory constraints** motivate algorithms like *FlashAttention*, which compute attention block by block.\n",
    "* In distributed settings, **hardware memory limits** across devices motivate approaches like *RingAttention*, which stream keys and values across GPUs while overlapping computation with communication.\n",
    "\n",
    "The attention computation can be written elementwise as:\n",
    "\n",
    "$$\n",
    "S_{ij} = Q_i^\\top K_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{ij} = \\frac{\\exp(\\tau S_{ij})}{\\sum_{j}\\exp(\\tau S_{ij})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\sum_{j} P_{ij} \\, V_{j,:}\n",
    "$$\n",
    "\n",
    "Combining the steps, each output row can be expressed directly as:\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\frac{\\sum_{j}\\exp\\!\\big(\\tau Q_i^\\top K_j\\big)\\, V_{j,:}}{\\sum_{j}\\exp\\!\\big(\\tau Q_i^\\top K_j\\big)}\n",
    "$$\n",
    "\n",
    "### Numerical stability\n",
    "\n",
    "Directly exponentiating large dot products can cause overflow. To prevent this, we apply the classic log-sum-exp trick, subtracting the maximum score before exponentiation:\n",
    "\n",
    "$$\n",
    "m_i = \\max_j \\, Q_i^\\top K_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\frac{\\sum_{j}\\exp\\!\\big(\\tau Q_i^\\top K_j - m_i\\big)\\, V_{j,:}}{\\sum_{j}\\exp\\!\\big(\\tau Q_i^\\top K_j - m_i\\big)}\n",
    "$$\n",
    "\n",
    "This normalization step keeps the computation stable and memory-efficient.\n",
    "\n",
    "In the code block below, we’ll manually implement the normalized attention output as `O_m_with_max` and verify that it matches PyTorch’s built-in result `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O == O_m_with_max:  True\n"
     ]
    }
   ],
   "source": [
    "tau = 1. / math.sqrt(Q.shape[-1])\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "max_per_query = scores.max(dim=-1, keepdims=True).values\n",
    "A = torch.exp(tau * (scores - max_per_query))\n",
    "A = A / torch.sum(A, axis=-1, keepdims=True)\n",
    "O_m_with_max = torch.matmul(A, V)\n",
    "\n",
    "print(\"O == O_m_with_max: \", torch.allclose(O, O_m_with_max, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Computation with Blocks\n",
    "\n",
    "Instead of holding all of $Q, K, V$ in memory, we want to compute attention **block by block**.\n",
    "\n",
    "Let’s denote:\n",
    "\n",
    "* $Q_{B_q} \\in \\mathbb{R}^{B_q \\times d}$: a block of queries\n",
    "* $K_{B_k}, V_{B_k} \\in \\mathbb{R}^{B_k \\times d}$: a block of keys and values\n",
    "\n",
    "where $B_q < N$ and $B_k < M$.\n",
    "\n",
    "To see the idea more clearly, imagine the simplest case where $B_q = B_k = 1$:\n",
    "\n",
    "* We process **one query** against **one key–value pair** at a time.\n",
    "* As we stream through the keys/values, we maintain three running quantities:\n",
    "\n",
    "  * $m_i^t$: running maximum of scores (for stability)\n",
    "  * $n_i^t$: running numerator (vector)\n",
    "  * $d_i^t$: running denominator (scalar)\n",
    "\n",
    "At step $t$, when query $Q_i$ sees the key–value pair $(K_t, V_t)$, we update as follows:\n",
    "\n",
    "$$\n",
    "m_i^t = \\max \\big(m_i^{t-1}, \\, Q_i^\\top K_t\\big) \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "$$\n",
    "n_i^t = n_i^{t-1} \\cdot \\exp(m_i^{t-1} - m_i^t) \\;+\\; \\exp(\\tau Q_i^\\top K_t - m_i^t) \\, V_t \\quad \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_i^t = d_i^{t-1} \\cdot \\exp(m_i^{t-1} - m_i^t) \\;+\\; \\exp(\\tau Q_i^\\top K_t - m_i^t) \\quad \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "The correction factor $\\exp(m_i^{t-1} - m_i^t)$ ensures consistency when the running maximum changes.\n",
    "\n",
    "After processing all $M$ keys, the final attention output for query $i$ is simply:\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\frac{n_i^M}{d_i^M}\n",
    "$$\n",
    "\n",
    "\n",
    "### Key insight\n",
    "\n",
    "With this formulation, we only need to keep **a block of queries, keys, and values in memory at a time**, plus the running triplet $(m_i, n_i, d_i)$. This makes it possible to compute attention streaming-style while staying numerically stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O == O_online:  True\n"
     ]
    }
   ],
   "source": [
    "block_size = 2\n",
    "tau = 1. / math.sqrt(Q.shape[-1])\n",
    "\n",
    "max_per_query = torch.full((B, L), float('-inf'), dtype=torch.float32) # one max per query\n",
    "num = torch.zeros(B, L, D)\n",
    "den = torch.zeros(B, L)\n",
    "\n",
    "for start_idx in range(0, L, block_size):\n",
    "    outer_range = range(start_idx, min(start_idx + block_size, L))\n",
    "    K_b, V_b = K[:, outer_range], V[:, outer_range] # [B, BLOCK, D]\n",
    "\n",
    "    for start_jdx in range(0, L, block_size):\n",
    "        inner_range = range(start_jdx, min(start_jdx + block_size, L))\n",
    "        Q_b = Q[:, inner_range]\n",
    "\n",
    "        scores_b = tau * torch.matmul(Q_b, K_b.transpose(-2, -1))\n",
    "\n",
    "        ## Bookkeeping\n",
    "        m_old = max_per_query[:, inner_range]\n",
    "\n",
    "        # compute new maximums\n",
    "        m_new = max_per_query[:, inner_range] = torch.max(m_old, scores_b.max(axis=-1).values)\n",
    "        \n",
    "        # rescaling factors\n",
    "        exp_delta_m = torch.exp(m_old - m_new).unsqueeze(-1)\n",
    "        new_score_exp = torch.exp(scores_b  - m_new.unsqueeze(-1)) # Broadcast max to the last dimension\n",
    "\n",
    "        # numerator\n",
    "        num[:, inner_range] = num[:, inner_range] * exp_delta_m + torch.matmul(new_score_exp, V_b)\n",
    "\n",
    "        # denominator \n",
    "        den[:, inner_range] = den[:, inner_range] * exp_delta_m.squeeze(-1) + new_score_exp.sum(dim=-1)\n",
    "\n",
    "        \n",
    "O_online = num/den.unsqueeze(-1)\n",
    "\n",
    "print(\"O == O_online: \", torch.allclose(O, O_online, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass in Batch\n",
    "\n",
    "At the end of the forward computation, we have access to:\n",
    "\n",
    "* the denominator $d_i = d_i^M$,\n",
    "* the stability term $m_i = \\max_j Q_i^\\top K_j$.\n",
    "\n",
    "During the backward pass, our goal is to compute the gradients\n",
    "\n",
    "$$\n",
    "\\partial Q = \\frac{\\partial L}{\\partial Q}, \\quad \n",
    "\\partial K = \\frac{\\partial L}{\\partial K}, \\quad \n",
    "\\partial V = \\frac{\\partial L}{\\partial V},\n",
    "$$\n",
    "\n",
    "given $\\partial O = \\frac{\\partial L}{\\partial O}$, where $L \\in \\mathbb{R}$ is the loss.\n",
    "\n",
    "For clarity, let’s drop the $\\partial L/\\partial$ notation and just write $\\partial O, \\partial Q, \\partial K, \\partial V$.\n",
    "\n",
    "The shapes are as follows:\n",
    "$\n",
    "\\partial O \\in \\mathbb{R}^{N \\times d}, \\quad\n",
    "\\partial Q \\in \\mathbb{R}^{N \\times d}, \\quad\n",
    "\\partial K \\in \\mathbb{R}^{M \\times d}, \\quad\n",
    "\\partial V \\in \\mathbb{R}^{M \\times d}\n",
    "$\n",
    "\n",
    "**Step 1: From output to values**\n",
    "\n",
    "Recall the forward equations:\n",
    "\n",
    "$$\n",
    "O = P V\n",
    "$$\n",
    "\n",
    "Taking derivatives gives:\n",
    "\n",
    "$$\n",
    "\\partial V = P^\\top \\partial O \\in \\mathbb{R}^{M \\times d}, \\quad \n",
    "\\partial P = \\partial O V^\\top \\in \\mathbb{R}^{N \\times M}\n",
    "$$\n",
    "\n",
    "**Step 2: Softmax Jacobian**\n",
    "\n",
    "Each row of $P$ comes from a softmax over the corresponding row of the score matrix $S = QK^\\top$. Since all entries in a row are coupled, we must handle them together.\n",
    "\n",
    "For a vector softmax $\\mathbf{y} = \\text{Softmax}(\\mathbf{x})$:\n",
    "\n",
    "$$\n",
    "\\partial \\mathbf{x} = \\big(\\text{diag}(\\mathbf{y}) - \\mathbf{y}\\mathbf{y}^\\top\\big)\\, \\partial \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Equivalently, using elementwise notation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j} =\n",
    "\\begin{cases}\n",
    "y_i(1 - y_i) & \\text{if } i=j \\\\\n",
    "- y_i y_j & \\text{if } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Step 3: Apply to attention**\n",
    "\n",
    "For the $i$-th row of $P$:\n",
    "\n",
    "$$\n",
    "\\partial S_{i,:} = \\big(\\text{diag}(P_{i,:}) - P_{i,:} P_{i,:}^\\top\\big) \\, \\partial P_{i,:}\n",
    "$$\n",
    "\n",
    "This can also be written compactly as:\n",
    "\n",
    "$$\n",
    "\\partial S_{i,:} = P_{i,:} \\odot \\big(\\partial P_{i,:} - (\\partial P_{i,:}^\\top P_{i,:}) \\mathbf{1}\\big)\n",
    "$$\n",
    "\n",
    "where $P_{i,:} \\in \\mathbb{R}^M$ is treated as a column vector.\n",
    "\n",
    "**Step 4: From scores back to queries and keys**\n",
    "\n",
    "Finally, since $S = QK^\\top$:\n",
    "\n",
    "$$\n",
    "\\partial Q = \\partial S K \\in \\mathbb{R}^{N \\times d}, \\quad \n",
    "\\partial K^\\top = Q^\\top \\partial S \\in \\mathbb{R}^{d \\times M}\n",
    "$$\n",
    "\n",
    "\n",
    "**Note:** If you’re fluent with matrix calculus, these steps are straightforward. If not, I’ll include an appendix at the end of the post showing the derivations in more detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O == O_manual:  True\n",
      "dV == dV_manual:  True\n",
      "dQ == dQ_manual:  True\n",
      "dK == dK_manual:  True\n"
     ]
    }
   ],
   "source": [
    "# simulating gradient wrt the loss\n",
    "dO = torch.randn(B, L, D)\n",
    "O.backward(dO) # perform backprop\n",
    "\n",
    "# read off the gradients \n",
    "dQ = Q.grad \n",
    "dK = K.grad\n",
    "dV = V.grad \n",
    "\n",
    "# # Handcoded batch attention\n",
    "scores = tau * torch.matmul(Q, K.transpose(-2, -1))\n",
    "P = torch.softmax(scores, dim=-1)\n",
    "O_manual = torch.matmul(P, V)\n",
    "\n",
    "print(\"O == O_manual: \", torch.allclose(O, O_manual, atol=1e-6))\n",
    "\n",
    "# backward pass in batch updates\n",
    "dV_manual = torch.matmul(sm.transpose(-2, -1), dO)\n",
    "\n",
    "dP = torch.matmul(dO, V.transpose(-2, -1))\n",
    "dScores = P * (dP -  (dP * P).sum(dim=-1).unsqueeze(-1))\n",
    "\n",
    "dQ_manual = tau * torch.matmul(dScores, K) # multiply the scaling factor \n",
    "dK_manual = tau * torch.matmul(dScores.transpose(-2, -1), Q) # multiply the scaling factor \n",
    "\n",
    "print(\"dV == dV_manual: \", torch.allclose(dV, dV_manual, atol=1e-6))\n",
    "print(\"dQ == dQ_manual: \", torch.allclose(dQ, dQ_manual, atol=1e-6))\n",
    "print(\"dK == dK_manual: \", torch.allclose(dK, dK_manual, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass in Online Fashion\n",
    "\n",
    "Computing gradients in the naïve way would require storing the **entire softmax matrix**. For long sequences, this is completely impractical.\n",
    "\n",
    "For example, with a sequence length of 128K, the attention matrix has size $128K \\times 128K$. At 32-bit precision, that’s **\\~65 GB just for one example**. Clearly infeasible.\n",
    "\n",
    "Instead, we follow the approach used in *FlashAttention*.\n",
    "\n",
    "**Step 1: Recompute scores in blocks**\n",
    "\n",
    "As in the forward pass, we stream over blocks of queries, keys, and values. The scores and softmax probabilities are recomputed on the fly. The key difference from the forward pass is that:\n",
    "\n",
    "* We already have the stored row-wise maxima ($m_i$), so no need to maintain running maxima.\n",
    "* We also have the stored denominators ($d_i$), so we can reuse them directly.\n",
    "\n",
    "**Step 2: The softmax Jacobian bottleneck**\n",
    "\n",
    "In the gradient formulas, a problematic term appears:\n",
    "\n",
    "$$\n",
    "\\partial P_{i,:}^\\top P_{i,:}\n",
    "$$\n",
    "\n",
    "This is a scalar dot product that gets broadcast across all components of $P_{i,:}$. If computed naively, it prevents simple parallelization row-by-row.\n",
    "\n",
    "The FlashAttention paper resolves this by rewriting the scalar as:\n",
    "\n",
    "$$\n",
    "D_i = \\partial P_{i,:}^\\top P_{i,:} \n",
    "= \\sum_{j} P_{ij}\\,\\partial P_{ij} \n",
    "= \\sum_{j} P_{ij} \\, (\\partial O_i V_j)\n",
    "$$\n",
    "\n",
    "Notice the last step:\n",
    "\n",
    "$$\n",
    "D_i = \\partial O_i \\Big(\\sum_j P_{ij} V_j\\Big) = \\partial O_i^\\top O_i\n",
    "$$\n",
    "\n",
    "This turns the awkward scalar-product term into something much cleaner: the dot product between $\\partial O_i$ and the forward output $O_i$.\n",
    "\n",
    "This trick is critical because it allows the backward pass to be parallelized efficiently, in the same streaming/blockwise fashion as the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dV == dV_online:  True\n",
      "dQ == dQ_online:  True\n",
      "dK == dK_online:  True\n"
     ]
    }
   ],
   "source": [
    "block_size = 2\n",
    "tau = 1. / math.sqrt(Q.shape[-1])\n",
    "\n",
    "# given:\n",
    "# max_per_query: max over the query-key inner product\n",
    "# den: denominator sum exp (query, key inner product) per query\n",
    "\n",
    "\n",
    "dQ_online = torch.zeros(B, L ,D)\n",
    "dK_online = torch.zeros(B, L ,D)\n",
    "dV_online = torch.zeros(B, L ,D)\n",
    "\n",
    "for start_idx in range(0, L, block_size):\n",
    "    outer_range = range(start_idx, min(start_idx + block_size, L))\n",
    "    K_b, V_b = K[:, outer_range], V[:, outer_range] # [B, BLOCK, D]\n",
    "\n",
    "    for start_jdx in range(0, L, block_size):\n",
    "        inner_range = range(start_jdx, min(start_jdx + block_size, L))\n",
    "        Q_b = Q[:, inner_range]\n",
    "        dO_b = dO[:, inner_range]\n",
    "        O_b = O[:, inner_range]\n",
    "\n",
    "        # recompute local probs P_b exactly using global m, d for the query rows\n",
    "        scores_b = tau * torch.matmul(Q_b, K_b.transpose(-2, -1)) \n",
    "        m_b = max_per_query[:, inner_range, None]\n",
    "        d_b = den[:, inner_range, None] \n",
    "        P_b = torch.exp(scores_b - m_b) / d_b # these are exact attn weights \n",
    "        \n",
    "        # computing the derivatives wrt V and P\n",
    "        dV_b = torch.matmul(P_b.transpose(-2, -1), dO_b)\n",
    "        dP_b = torch.matmul(dO_b, V_b.transpose(-2, -1))\n",
    "\n",
    "        # computing the derivatives wrt scores\n",
    "        D_i = (dO_b * O_b).sum(dim=-1)\n",
    "        dScores_b = P_b * (dP_b -  D_i.unsqueeze(-1))\n",
    "\n",
    "        # computing the derivatives wrt Q and K\n",
    "        dQ_b = tau * torch.matmul(dScores_b, K_b) # multiply the scaling factor \n",
    "        dK_b = tau * torch.matmul(dScores_b.transpose(-2, -1), Q_b) # multiply the scaling factor \n",
    "\n",
    "        # accumulating these gradients\n",
    "        dQ_online[:, inner_range] += dQ_b\n",
    "        dK_online[:, outer_range] += dK_b\n",
    "        dV_online[:, outer_range] += dV_b\n",
    "        \n",
    "print(\"dV == dV_online: \", torch.allclose(dV, dV_online, atol=1e-6))\n",
    "print(\"dQ == dQ_online: \", torch.allclose(dQ, dQ_online, atol=1e-6))\n",
    "print(\"dK == dK_online: \", torch.allclose(dK, dK_online, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Computing derivatives of P and V\n",
    "\n",
    "Here we will build the intuition to perform the following,\n",
    "\n",
    "$$\n",
    "O = PV  \\in \\mathcal{R}^{N \\times d}\\\\\n",
    "\n",
    "\\partial V = P^T \\partial O \\in \\mathcal{R}^{M \\times d}, \\qquad \\partial P = \\partial O V^T \\in \\mathcal{R}^{N \\times M} \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "## Matrix Calculus View\n",
    "\n",
    "Let’s start with some definitions. In calculus, the **gradient** is just the derivative, and **differentials** are small changes in variables, also called **variations**.\n",
    "\n",
    "Our goal is to understand how to get the gradients of the loss with respect to $P$ and $V$, i.e. $\\nabla_P L$ and $\\nabla_V L$, when we already know the gradient with respect to $O$, denoted $\\nabla_O L$. Here the relation is\n",
    "\n",
    "$$\n",
    "O = P V, \\quad O, P, V \\in \\mathbb{R}^{N \\times D}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 1. Variations of $O$\n",
    "\n",
    "The first step is to ask: if $P$ and $V$ change a little bit, how does $O$ change? Denote these small changes by $\\delta P$, $\\delta V$, and $\\delta O$.\n",
    "\n",
    "For the function $f(P,V) = P V$, the change in output is\n",
    "\n",
    "$$\n",
    "\\delta O = P \\,\\delta V + \\delta P \\,V.\n",
    "$$\n",
    "\n",
    "This is just the usual product rule, written in matrix form: wiggle $P$, you get $\\delta P V$; wiggle $V$, you get $P \\delta V$.\n",
    "\n",
    "\n",
    "#### Step 2. Variation of the loss\n",
    "\n",
    "Now we want to see how the loss $L$ changes when $O$ changes. By definition, the variation in $L$ is given by the inner product between the gradient and the perturbation:\n",
    "\n",
    "$$\n",
    "\\delta L = \\mathrm{tr} \\!\\left( (\\nabla_O L)^\\top \\delta O \\right).\n",
    "$$\n",
    "\n",
    "Substitute $\\delta O = P \\delta V + \\delta P V$:\n",
    "\n",
    "$$\n",
    "\\delta L\n",
    "= \\mathrm{tr}\\!\\big((\\nabla_O L)^\\top P \\,\\delta V\\big)\n",
    "+ \\mathrm{tr}\\!\\big((\\nabla_O L)^\\top \\delta P V\\big).\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 3. Rearranging with the trace identity\n",
    "\n",
    "Using the identity $\\mathrm{tr}(ABC) = \\mathrm{tr}(CBA)$, we can move terms around so that $\\delta V$ and $\\delta P$ appear at the end:\n",
    "\n",
    "$$\n",
    "\\delta L\n",
    "= \\mathrm{tr}\\!\\big((P^\\top \\nabla_O L)^\\top \\delta V\\big)\n",
    "+ \\mathrm{tr}\\!\\big((\\nabla_O L V^\\top)^\\top \\delta P\\big).\n",
    "$$\n",
    "\n",
    "Now it’s clear which matrices multiply with $\\delta V$ and $\\delta P$.\n",
    "\n",
    "\n",
    "#### Step 4. Read off the gradients\n",
    "\n",
    "By definition of the trace/Frobenius inner product, the coefficients of $\\delta V$ and $\\delta P$ are the gradients we seek:\n",
    "\n",
    "$$\n",
    "\\nabla_V L = P^\\top \\nabla_O L,\n",
    "\\qquad\n",
    "\\nabla_P L = \\nabla_O L V^\\top.\n",
    "$$\n",
    "\n",
    "\n",
    "## Row-wise perturbation view \n",
    "\n",
    "### Computing $\\nabla_V L$\n",
    "\n",
    "Let’s focus on the $j$-th row of $V \\in \\mathbb{R}^{M \\times d}$.\n",
    "\n",
    "From the forward computation (row form of matrix multiplication), we have\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\sum_{j=1}^M P_{ij}\\, V_{j,:}.\n",
    "$$\n",
    "\n",
    "So each row $V_{j,:}$ contributes to the output row $O_{i,:}$ with weight $P_{ij}$.\n",
    "\n",
    "\n",
    "#### Step 1. Effect of perturbing $V_{j,:}$\n",
    "\n",
    "If we perturb $V_{j,:}$ by $\\delta V_{j,:}$, then the corresponding change in $O_{i,:}$ is\n",
    "\n",
    "$$\n",
    "\\delta O_{i,:} = P_{ij}\\,\\delta V_{j,:}, \\qquad \\forall i \\in \\{1,\\dots,N\\}.\n",
    "$$\n",
    "\n",
    "#### Step 2. Effect on the loss\n",
    "\n",
    "How do we connect the change in $O$ to the change in the loss $L$?\n",
    "By definition, the gradient of $L$ with respect to $O_{i,:}$ tells us how sensitive the loss is to small changes in that row. The precise rule is:\n",
    "\n",
    "$$\n",
    "\\delta L = \\sum_{i=1}^N \\Big\\langle \\nabla_{O_{i,:}} L,\\, \\delta O_{i,:} \\Big\\rangle.\n",
    "$$\n",
    "\n",
    "Here $\\langle \\cdot , \\cdot \\rangle$ is just the standard inner product on row vectors.\n",
    "This says: *the change in the loss is the dot product between the gradient and the perturbation, summed over all rows.*\n",
    "\n",
    "Now plug in the perturbation $\\delta O_{i,:} = P_{ij}\\,\\delta V_{j,:}$:\n",
    "\n",
    "$$\n",
    "\\delta L = \\sum_{i=1}^N \\Big\\langle \\nabla_{O_{i,:}} L,\\, P_{ij}\\,\\delta V_{j,:} \\Big\\rangle.\n",
    "$$\n",
    "\n",
    "Factor out the common $\\delta V_{j,:}$:\n",
    "\n",
    "$$\n",
    "\\delta L = \\Big\\langle \\sum_{i=1}^N P_{ij}\\,\\nabla_{O_{i,:}} L,\\, \\delta V_{j,:} \\Big\\rangle.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 3. Read off the gradient\n",
    "\n",
    "From the definition of the inner product, this gives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial V_{j,:}} = \\sum_{i=1}^N P_{ij}\\,\\frac{\\partial L}{\\partial O_{i,:}}.\n",
    "$$\n",
    "\n",
    "Stacking over all rows $j$, we get the compact matrix form:\n",
    "\n",
    "$$\n",
    "\\nabla_V L = P^\\top \\nabla_O L.\n",
    "$$\n",
    "\n",
    "### Computing $\\nabla_{P_{ij}} L$\n",
    "\n",
    "Let’s now switch to the **column view** of matrix multiplication. Each entry of $P$ picks out a column of $V$ and scales it:\n",
    "\n",
    "$$\n",
    "O_{i,:} = \\sum_{j=1}^M P_{ij}\\, V_{j,:}.\n",
    "$$\n",
    "\n",
    "So, if we perturb just a single entry $P_{ij}$ by a small amount $\\delta P_{ij}$, the corresponding perturbation in the output row is\n",
    "\n",
    "$$\n",
    "\\delta O_{i,:} = \\delta P_{ij}\\, V_{j,:}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 1. Effect on the loss\n",
    "\n",
    "By definition, the change in the loss is given by the inner product between the gradient and the perturbation:\n",
    "\n",
    "$$\n",
    "\\delta L = \\big\\langle \\nabla_{O_{i,:}} L,\\; \\delta O_{i,:} \\big\\rangle.\n",
    "$$\n",
    "\n",
    "Substitute $\\delta O_{i,:} = \\delta P_{ij}\\,V_{j,:}$:\n",
    "\n",
    "$$\n",
    "\\delta L = \\big\\langle \\nabla_{O_{i,:}} L,\\; \\delta P_{ij}\\, V_{j,:} \\big\\rangle.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 2. Factor out the scalar $\\delta P_{ij}$\n",
    "\n",
    "Since $\\delta P_{ij}$ is just a number, we can pull it out:\n",
    "\n",
    "$$\n",
    "\\delta L = \\big(\\nabla_{O_{i,:}} L^\\top V_{j,:}\\big)\\,\\delta P_{ij}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 3. Read off the gradient\n",
    "\n",
    "From the definition of the inner product, the coefficient multiplying $\\delta P_{ij}$ is exactly the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_{P_{ij}} L = \\nabla_{O_{i,:}} L^\\top V_{j,:}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 4. Collect into matrix form\n",
    "\n",
    "Stacking all entries together gives the compact expression:\n",
    "\n",
    "$$\n",
    "\\nabla_P L = \\nabla_O L\\, V^\\top.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Full Attention with Dropout and Mask\n",
    "\n",
    "Now that we’ve seen how to compute attention in an online fashion for both the forward and backward passes, let’s extend the formulation to include **dropout** and **masking**, which are standard in practice.\n",
    "\n",
    "The overall structure remains the same, with just a few modifications:\n",
    "\n",
    "* **Masking:** applied directly to the scores before softmax (e.g., causal mask or padding mask).\n",
    "* **Dropout:** applied to the softmax probabilities $P$ before they are multiplied by the values.\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "* Compute scores block by block.\n",
    "* Apply mask (setting masked positions to $-\\infty$ or a large negative number).\n",
    "* Apply softmax with stability trick.\n",
    "* Optionally apply dropout to the probabilities $P$.\n",
    "* Multiply by values $V$ to get outputs.\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The backward pass follows the same blockwise structure as before, but with two additional considerations:\n",
    "\n",
    "1. **Masking:** The same mask used in the forward pass must be reapplied to the scores in the backward pass to zero out contributions from masked positions.\n",
    "2. **Dropout:** To remain consistent, the *exact same dropout pattern* used in the forward pass must be applied again during backpropagation. This requires careful handling of the random number generator (RNG) to ensure reproducibility.\n",
    "\n",
    "\n",
    "⚠️ **Note:** Correct RNG management is subtle. In practice, frameworks like PyTorch handle dropout reproducibility automatically by saving RNG state between forward and backward. Implementing this manually in a custom kernel (e.g., for FlashAttention) requires special care, and we won’t go into the details here.\n",
    "\n",
    "This way, the online attention mechanism is extended to the “real” transformer case: with masks for structure and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O == O_manual (True if dropout_p = 0.0, current dropout: 0.0):  True\n",
      "dV == dV_manual:  True\n",
      "dQ == dQ_manual:  True\n",
      "dK == dK_manual:  True\n"
     ]
    }
   ],
   "source": [
    "B = 10\n",
    "L = 20\n",
    "D = 16\n",
    "dropout_p = 0.\n",
    "Q = torch.randn(B, L, D, requires_grad=True)\n",
    "K = torch.randn(B, L, D, requires_grad=True)\n",
    "V = torch.randn(B, L, D, requires_grad=True)\n",
    "MASK = torch.ones(L, L, dtype=bool).tril(diagonal=0) # 0 offset from the diagonal\n",
    "\n",
    "tau = 1. / math.sqrt(D)\n",
    "\n",
    "# Using PyTorch Module\n",
    "# API expects [B, heads, L, D]; here head dim = 1\n",
    "O = F.scaled_dot_product_attention(Q.unsqueeze(1), K.unsqueeze(1), V.unsqueeze(1), attn_mask=MASK, dropout_p=dropout_p)\n",
    "O = O.squeeze(1)\n",
    "\n",
    "# Handcoded batch attention\n",
    "scores = tau * torch.matmul(Q, K.transpose(-2, -1))\n",
    "scores.masked_fill_(~MASK, float('-inf')) # apply mask\n",
    "P = torch.softmax(scores, dim=-1)\n",
    "\n",
    "# dropout on some attention weights \n",
    "keep_prob = 1 - dropout_p\n",
    "dropout_mask = (torch.rand_like(P) < keep_prob).to(sm.dtype)\n",
    "P = P * dropout_mask / keep_prob # We divide by keep_prob so that we don't have to multiply this factor at the test time\n",
    "\n",
    "O_manual = torch.matmul(P, V)\n",
    "print(f\"O == O_manual (True if dropout_p = 0.0, current dropout: {dropout_p}): \", torch.allclose(O, O_manual, atol=1e-6))\n",
    "\n",
    "\n",
    "## Computing backwards\n",
    "\n",
    "# simulate gradients of loss wrt O\n",
    "dO = torch.rand_like(O)\n",
    "O.backward(dO)\n",
    "dQ = Q.grad \n",
    "dK = K.grad \n",
    "dV = V.grad\n",
    "\n",
    "dV_manual = torch.matmul(P.transpose(-2, -1), dO)\n",
    "dP = torch.matmul(dO, V.transpose(-2, -1))\n",
    "dP = dP * dropout_mask / keep_prob\n",
    "dScores = P * (dP - (dP * P).sum(dim=-1).unsqueeze(-1))\n",
    "dQ_manual = tau * torch.matmul(dScores, K) # multiply the scaling factor \n",
    "dK_manual = tau * torch.matmul(dScores.transpose(-2, -1), Q) # multiply the scaling factor \n",
    "\n",
    "print(\"dV == dV_manual: \", torch.allclose(dV, dV_manual, atol=1e-6))\n",
    "print(\"dQ == dQ_manual: \", torch.allclose(dQ, dQ_manual, atol=1e-6))\n",
    "print(\"dK == dK_manual: \", torch.allclose(dK, dK_manual, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
