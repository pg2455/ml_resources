{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f11f58d-c4f2-43c6-a2ef-f7b4deed3046",
   "metadata": {},
   "source": [
    "# Toy Simulacra 1.4\n",
    "\n",
    "So far, we have built agents that have daily schedules. They can carry out these activities without having to interact with anyone or observe anything about their environment.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, we aim to add perception to the agents, allowing them to react to perceived events with volition. Specifically, we want to enable agents to perceive their surroundings, add any events therein to their memory, and react if necessary.\n",
    "\n",
    "Thus, we need to write functions that do the following:\n",
    "\n",
    "- perceive: Observes the events and adds them to respective memories.\n",
    "- retrieve: Retrieves the relevant information from the memory to be used for downstream inferences.\n",
    "- react / chat: Decides the reaction mode for the agents and how to respond to the focused event.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9d695-48c8-4da6-ae2c-17a803996070",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Note: Additional print functions have been implemented for clean logging of various aspects of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff310bd-f03c-4f37-973c-edb42163e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template folder /Users/gupta/Workspace/tutorials/simulacra/prompt_templates\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from maze import Maze\n",
    "\n",
    "CLIENT = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "AZURE_MODEL_MAP = {\n",
    "  'gpt-3.5-turbo-instruct': 'gpt-35-turbo-instruct',\n",
    "  'gpt-3.5-turbo': 'gpt-35-turbo'\n",
    "}\n",
    "\n",
    "GPT_PARAMS = {\n",
    "    \"engine\": \"gpt-3.5-turbo-instruct\", \n",
    "    \"max_tokens\": 500, \n",
    "    \"temperature\": 1.0, \n",
    "    \"top_p\": 1, \n",
    "    \"stream\": False,\n",
    "    \"frequency_penalty\": 0, \n",
    "    \"presence_penalty\": 0, \n",
    "    \"stop\": None\n",
    "}\n",
    "\n",
    "HOUR_STR = [\"00:00 AM\", \"01:00 AM\", \"02:00 AM\", \"03:00 AM\", \"04:00 AM\", \n",
    "              \"05:00 AM\", \"06:00 AM\", \"07:00 AM\", \"08:00 AM\", \"09:00 AM\", \n",
    "              \"10:00 AM\", \"11:00 AM\", \"12:00 PM\", \"01:00 PM\", \"02:00 PM\", \n",
    "              \"03:00 PM\", \"04:00 PM\", \"05:00 PM\", \"06:00 PM\", \"07:00 PM\",\n",
    "              \"08:00 PM\", \"09:00 PM\", \"10:00 PM\", \"11:00 PM\"]\n",
    "\n",
    "TIME_SLEEP_BETWEEN_REQUESTS = 0.1 # seconds\n",
    "\n",
    "TEMPLATE_FOLDER = Path('./prompt_templates').resolve()\n",
    "print(\"template folder\", TEMPLATE_FOLDER)\n",
    "\n",
    "BASE_SIM_FOLDER = Path(\"./generative_agents/environment/frontend_server/storage/base_the_ville_isabella_maria_klaus/\").resolve()\n",
    "PERSONAS_FOLDER = BASE_SIM_FOLDER / \"personas\"\n",
    "\n",
    "PROMPT_LOGFILE = \"./prompts_log.txt\"\n",
    "SIM_LOGFILE = \"./sim_logs.txt\"\n",
    "CONVERSATION_LOGFILE = \"./convo_logs.txt\"\n",
    "FAILSAFE_LOGFILE = \"./failsafe_logs.txt\"\n",
    "SCHEDULES_LOGFILE = \"./schedules_logfile.txt\"\n",
    "PRINT_SCHEDULE = True\n",
    "PRINT_PROMPTS = True\n",
    "PRINT_CONVO = True\n",
    "PRINT_FAILSAFE = True\n",
    "\n",
    "CALL_LOGS = {\n",
    "    \"api_calls\": 0,\n",
    "    \"fail_safe_counts\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56ecd05-04ca-4714-9b73-2022f6bb569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prompt(fn_name, persona, prompt, response, params, do_not_print=False):\n",
    "    if not PRINT_PROMPTS:\n",
    "        return\n",
    "    \n",
    "    if do_not_print:\n",
    "        return\n",
    "        \n",
    "    curr_time = persona.scratch.curr_time.strftime('%A %B %d %H:%M:%S')\n",
    "    with open(PROMPT_LOGFILE, mode=\"a\") as f:\n",
    "        string = \"\\n\\n\" + \">\"*50 + \"<\"*50 + \"\\n\\n\" + str(params) + \"\\n\\n\"\n",
    "        print(f\"{string}{curr_time}\\n{fn_name} --- {persona.name}\\n\\n --- PROMPT: ---\\n{prompt}\\n\\n--- RESPONSE: ---\\n{response}\", file=f)\n",
    "\n",
    "def _normalize(seq):\n",
    "    _min = min(seq)\n",
    "    _max = max(seq)\n",
    "    return [(i - _min) / (1e-6 + _max-_min) for i in seq]\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def print_to_file(string, logfile):\n",
    "    with open(logfile, 'a') as f:\n",
    "        print(string, file=f)\n",
    "\n",
    "def print_convo(convo, convo_duration_min, convo_summary, curr_time, persona):\n",
    "    if not PRINT_CONVO:\n",
    "        return\n",
    "    string = f\"{curr_time.strftime('%A %B %d %H:%M')} -- Initiator: {persona.name}\\n\\n\"\n",
    "    string += f\"Summary: {convo_summary}\\n\"\n",
    "    string += f\"Time taken (minutes): {str(convo_duration_min)}\\n\"\n",
    "    string += \"\".join([\": \".join(i) + \"\\n\" for i in convo])\n",
    "    with open(CONVERSATION_LOGFILE, 'a') as f:\n",
    "        print(string, file=f)\n",
    "\n",
    "def print_failsafe(fn_name, string):\n",
    "    if not PRINT_FAILSAFE:\n",
    "        return \n",
    "\n",
    "    if fn_name in CALL_LOGS['fail_safe_counts']:\n",
    "        CALL_LOGS['fail_safe_counts'][fn_name] += 1\n",
    "    else:\n",
    "        CALL_LOGS['fail_safe_counts'][fn_name] = 0\n",
    "\n",
    "    string = f\"Fn: {string}\"\n",
    "    with open(FAILSAFE_LOGFILE, 'a') as f:\n",
    "        print(string, file=f)\n",
    "    \n",
    "def print_schedule(string, schedule, curr_time):\n",
    "    if not PRINT_SCHEDULE:\n",
    "        return\n",
    "\n",
    "    string = f\"{string}\\n\"\n",
    "    t = midnight = datetime(curr_time.year, curr_time.month, curr_time.day)\n",
    "    for activity, duration in schedule:\n",
    "        t += timedelta(minutes=duration)\n",
    "        string += f\"{t.strftime('%H:%M')} -- {activity}\\n\"\n",
    "\n",
    "    with open(SCHEDULES_LOGFILE, \"a\") as f:\n",
    "        print(string, file=f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a424d23c-cce3-40e9-b9f3-39f1de89a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic prompting helper functions\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    if not text:\n",
    "        text = \"blank\"\n",
    "\n",
    "    response = CLIENT.embeddings.create(\n",
    "        input = [text],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def generate_prompt(prompt_inputs, template_file):\n",
    "    with open(template_file) as f:\n",
    "        template = f.read()\n",
    "\n",
    "    prompt = template.split(\"<prompt_start>###</prompt_start>\")[1]\n",
    "    for count, input in enumerate(prompt_inputs):\n",
    "        replace_str = input if input is not None else \"\"\n",
    "        prompt = prompt.replace(f\"!<INPUT {count}>!\", replace_str)\n",
    "\n",
    "    return prompt.strip()\n",
    "\n",
    "def prompt_gpt(prompt, parameters):\n",
    "    try:\n",
    "        response = CLIENT.completions.create(\n",
    "            model=AZURE_MODEL_MAP[parameters[\"engine\"]],\n",
    "            prompt=prompt,\n",
    "            temperature=parameters[\"temperature\"],\n",
    "            max_tokens=parameters[\"max_tokens\"],\n",
    "            top_p=parameters[\"top_p\"],\n",
    "            frequency_penalty=parameters[\"frequency_penalty\"],\n",
    "            presence_penalty=parameters[\"presence_penalty\"],\n",
    "            stream=parameters[\"stream\"],\n",
    "            stop=parameters[\"stop\"]\n",
    "        )\n",
    "        return response.choices[0].text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1\n",
    "\n",
    "\n",
    "def prompt_gpt4(prompt, parameters):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant that helps people complete the text either by continuing where they leave or by following the instructions. Don't write unnecessary text. Only the asked tasks.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        response = CLIENT.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            temperature=parameters[\"temperature\"],\n",
    "            max_tokens=parameters[\"max_tokens\"],\n",
    "            top_p=parameters[\"top_p\"],\n",
    "            frequency_penalty=parameters[\"frequency_penalty\"],\n",
    "            presence_penalty=parameters[\"presence_penalty\"],\n",
    "            stop=parameters[\"stop\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1\n",
    "\n",
    "def safe_prompting(prompt, parameters, func_clean_up, func_validate=None, repeat=5):\n",
    "    if func_validate is None:\n",
    "        def func_validate(response):\n",
    "            try: func_clean_up(response)\n",
    "            except: return False\n",
    "            return True\n",
    "\n",
    "    for i in range(repeat):\n",
    "        curr_response = PROMPT_FN(prompt, parameters)\n",
    "        CALL_LOGS[\"api_calls\"] += 1\n",
    "        if func_validate(curr_response):\n",
    "            return func_clean_up(curr_response)\n",
    "        else:\n",
    "            time.sleep(TIME_SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(f\"{prompt} failed after {repeat} attempt. Returning None.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "#### DEFINE YOUR PROMPT U=FN\n",
    "PROMPT_FN = prompt_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724aa4ab-01f4-46ba-b131-fbd3484e2a2e",
   "metadata": {},
   "source": [
    "### Spatial Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2dcb1f-6c5f-4590-91d0-667a2d116d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialMemoryTree: \n",
    "    def __init__(self, f_saved): \n",
    "        self.tree = {}\n",
    "        self.tree = json.load(open(f_saved))\n",
    "    \n",
    "    def print_tree(self): \n",
    "        def _print_tree(tree, depth):\n",
    "            dash = \" >\" * depth\n",
    "            if type(tree) == type(list()): \n",
    "                if tree:\n",
    "                  print (dash, tree)\n",
    "                return \n",
    "            \n",
    "            for key, val in tree.items(): \n",
    "                if key: \n",
    "                  print (dash, key)\n",
    "                _print_tree(val, depth+1)\n",
    "        _print_tree(self.tree, 0)\n",
    "    \n",
    "    def get_str_accessible_sectors(self, curr_world):\n",
    "        return \", \".join(list(self.tree[curr_world].keys()))\n",
    "    \n",
    "    def get_str_accessible_sector_arenas(self, sector):\n",
    "        curr_world, curr_sector = sector.split(\":\")\n",
    "        if not curr_sector:\n",
    "            return \"\"\n",
    "        return \", \".join(list(self.tree[curr_world][curr_sector].keys()))\n",
    "\n",
    "    def get_str_accessible_arena_game_objects(self, arena):\n",
    "        curr_world, curr_sector, curr_arena = arena.split(\":\")\n",
    "        if not curr_arena:\n",
    "            return \"\"\n",
    "\n",
    "        try: \n",
    "            x = \", \".join(self.tree[curr_world][curr_sector][curr_arena])\n",
    "        except:\n",
    "            x = \", \".join(self.tree[curr_world][curr_sector][curr_arena.lower()])\n",
    "\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b690d5-7336-47b0-aff5-9303563b84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the Ville\n",
      " > Hobbs Cafe\n",
      " > > cafe\n",
      " > > > ['refrigerator', 'cafe customer seating', 'cooking area', 'kitchen sink', 'behind the cafe counter', 'piano']\n",
      " > Isabella Rodriguez's apartment\n",
      " > > main room\n",
      " > > > ['bed', 'desk', 'refrigerator', 'closet', 'shelf']\n",
      " > The Rose and Crown Pub\n",
      " > > pub\n",
      " > > > ['shelf', 'refrigerator', 'bar customer seating', 'behind the bar counter', 'kitchen sink', 'cooking area', 'microphone']\n",
      " > Harvey Oak Supply Store\n",
      " > > supply store\n",
      " > > > ['supply store product shelf', 'behind the supply store counter', 'supply store counter']\n",
      " > The Willows Market and Pharmacy\n",
      " > > store\n",
      " > > > ['behind the pharmacy counter', 'pharmacy store shelf', 'pharmacy store counter', 'grocery store shelf', 'behind the grocery counter', 'grocery store counter']\n",
      " > Dorm for Oak Hill College\n",
      " > > garden\n",
      " > > > ['dorm garden']\n",
      " > > common room\n",
      " > > > ['common room sofa', 'pool table', 'common room table']\n",
      " > Johnson Park\n",
      " > > park\n",
      " > > > ['park garden']\n"
     ]
    }
   ],
   "source": [
    "filename = str(PERSONAS_FOLDER / \"Isabella Rodriguez/bootstrap_memory/spatial_memory.json\")\n",
    "s_mem = SpatialMemoryTree(filename)\n",
    "s_mem.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc232c7b-a532-4fbd-bab8-e316cbcb18da",
   "metadata": {},
   "source": [
    "### Associative memory (Long term memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42502c56-32a9-4a59-97b4-2ea36492e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptNode:\n",
    "    node_id: int # bookkeeping\n",
    "    node_count: int #bookkeeping\n",
    "    node_type: str # thought / event / chat\n",
    "    type_count: int # bookkeeping\n",
    "    depth: int # \n",
    "\n",
    "    created: int # \n",
    "    expiration: int #\n",
    "\n",
    "    subject: str # subject usually the agent itself\n",
    "    predicate: str # \n",
    "    object: str # object of this event\n",
    "\n",
    "    description: str # A full description of the event (usually obtained from LLM)\n",
    "    embedding_key: str # a key to reference while accessing the embeddings of the event\n",
    "    embedding: int  # a vector instead. \n",
    "    poignancy: int # ??\n",
    "    keywords: list # keywords to retrieve this node\n",
    "    filling: int # ??\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for key in self.__annotations__:\n",
    "            setattr(self, key, kwargs.get(key, None)) \n",
    "        self.last_accessed = self.created\n",
    "\n",
    "    def spo_summary(self):\n",
    "        return (self.subject, self.predicate, self.object)\n",
    "\n",
    "\n",
    "class AssociativeMemory:\n",
    "    def __init__(self, folder_name):\n",
    "        self.id_to_node = dict()\n",
    "\n",
    "        self.seq_event = []\n",
    "        self.seq_thought = []\n",
    "        self.seq_chat = []\n",
    "\n",
    "        self.kw_to_event = dict()\n",
    "        self.kw_to_thought = dict()\n",
    "        self.kw_to_chat = dict()\n",
    "\n",
    "        x = json.load(open(folder_name + '/kw_strength.json'))\n",
    "        self.kw_strength_event = x.get('kw_strength_event', dict())\n",
    "        self.kw_strength_thought = x.get('kw_strength_thought', dict())\n",
    "\n",
    "        self.embeddings = json.load(open(folder_name + \"/embeddings.json\"))\n",
    "        nodes = json.load(open(folder_name + \"/nodes.json\"))\n",
    "        for count in range(len(nodes)):\n",
    "            node_id = f\"node_{str(count+1)}\"\n",
    "            node_details = nodes[node_id]\n",
    "\n",
    "            node_count = node_details['node_count']\n",
    "            depth = node_details['depth']\n",
    "\n",
    "            created = datetime.datetime.strptime(node_details['created'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            expiration = None\n",
    "            if node_details['expiration']:\n",
    "                expiration = datetime.datetime.strptime(node_details['expiration'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            subject, predicate, object = node_details['subject'], node_details['predicate'], node_details['object']\n",
    "            description = node_detals['description']\n",
    "            embedding_pair = (node_details['embedding_key'], self.embeddings[node_details['embedding_key']])\n",
    "            poignancy = node_details['poignancy']\n",
    "            keywords = set(node_details['keywords'])\n",
    "            filling = node_details['filling']\n",
    "\n",
    "            self.add_node(node_type, created, expiration, subject, predicate,\n",
    "                 object, description, keywords, poignancy, embedding_pair, filling)\n",
    "        \n",
    "    def add_node(self, node_type, created, expiration, subject, predicate,\n",
    "                 object, description, keywords, poignancy, embedding_pair, filling):\n",
    "\n",
    "        node_count = len(self.id_to_node.keys()) + 1\n",
    "        node_id = f'node_{node_count}'\n",
    "\n",
    "            \n",
    "        if node_type == 'chat':\n",
    "            type_count = len(self.seq_chat) + 1\n",
    "            depth = 0\n",
    "        elif node_type == 'event':\n",
    "            type_count = len(self.seq_event) + 1\n",
    "            depth = 0  \n",
    "        elif node_type == 'thought':\n",
    "            type_count = len(self.seq_thought) + 1\n",
    "            depth = 1\n",
    "            if filling:\n",
    "                depth += max([self.id_to_node[i].depth for i in filling])\n",
    "                \n",
    "        node = ConceptNode(node_id=node_id, node_count=node_count, type_count=type_count, node_type=node_type, depth=depth,\n",
    "                           created=created, expiration=expiration, subject=subject, predicate=predicate,\n",
    "                           object=object, description=description, embedding_key=embedding_pair[0], poignancy=poignancy,\n",
    "                           keywords=keywords, filling=filling, embedding=embedding_pair[1])\n",
    "\n",
    "        if node_type == 'chat':\n",
    "            self.seq_chat[0:0] = [node]\n",
    "            cache, cache_strength = self.kw_to_chat, None\n",
    "        elif node_type == 'event':\n",
    "            self.seq_event[0:0] = [node]\n",
    "            cache, cache_strength = self.kw_to_event, self.kw_strength_event\n",
    "        elif node_type == 'thought':\n",
    "            self.seq_thought[0:0] = [node]\n",
    "            cache, cache_strength = self.kw_to_thought, self.kw_strength_thought\n",
    "\n",
    "        keywords = [i.lower() for i in keywords]\n",
    "        for kw in keywords:\n",
    "            if kw in cache:\n",
    "                cache[kw][0:0] = [node]\n",
    "            else:\n",
    "                cache[kw] = [node]\n",
    "\n",
    "            if cache_strength is not None and f\"{predicate} {object}\" != \"is idle\":\n",
    "                if kw in cache_strength:\n",
    "                    cache_strength[kw] += 1\n",
    "                else:\n",
    "                    cache_strength[kw] = 1\n",
    "\n",
    "        self.embeddings[embedding_pair[0]] = embedding_pair[1]\n",
    "\n",
    "        return node\n",
    "            \n",
    "        \n",
    "    def get_summarized_latest_events(self, retention):\n",
    "        ret_set = set()\n",
    "        for e_node in self.seq_event[:retention]:\n",
    "            ret_set.add(e_node.spo_summary())\n",
    "        return ret_set\n",
    "\n",
    "    def get_str_seq_events(self):\n",
    "        ret_str = \"\"\n",
    "        for count, event in enumerate(self.seq_event):\n",
    "            ret_str += f'{\"Event\", len(self.seq_event) - count, \": \", event.spo_summary(), \" -- \", event.description}\\n' # returns a string of tuple\n",
    "        return ret_str    \n",
    "\n",
    "    def get_str_seq_thoughts(self):\n",
    "        ret_str = \"\"\n",
    "        for count, event in enumerate(self.seq_event):\n",
    "            ret_str += f'{\"Thought\", len(self.seq_event) - count, \": \", event.spo_summary(), \" -- \", event.description}\\n' # returns a string of tuple\n",
    "        return ret_str  \n",
    "\n",
    "    def get_str_seq_chats(self):\n",
    "        ret_str = \"\"\n",
    "        for event in self.seq_chat:\n",
    "            ret_str += f\"with {event.object.content} ({event.description})\\n\"\n",
    "            ret_str += f\"{event.created.strftime('%B %d, %Y, %H:%M:%S')}\\n\"\n",
    "            for row in event.filling:\n",
    "                ret_str += f\"{row[0]}: {row[1]}\\n\"\n",
    "        return ret_str\n",
    "            \n",
    "\n",
    "    def retrieve_relevant_thoughts(self, s, p, o):\n",
    "        contents = [s, p, o]\n",
    "        ret = []\n",
    "        for i in contents:\n",
    "            if i in self.kw_to_thought:\n",
    "                ret += self.kw_to_thought[i.lower()]\n",
    "        return set(ret)\n",
    "\n",
    "    def retrieve_relevant_events(self, s, p, o):\n",
    "        contents = [s, p, o]\n",
    "        ret = []\n",
    "        for i in contents:\n",
    "            if i in self.kw_to_event:\n",
    "                ret += self.kw_to_event[i.lower()]\n",
    "        return set(ret)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f1f48-6c27-48a9-b613-b97e4f2c27a3",
   "metadata": {},
   "source": [
    "### Short term memory (scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c5014c-077e-42a5-8bd0-bdf7b0baa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch:\n",
    "    def __init__(self, filename):\n",
    "        scratch = json.load(open(filename))\n",
    "\n",
    "        # PERSONA HYPERPARAMETERS\n",
    "        self.vision_r = scratch.get('vision_r', 4) # Radius of visible boundaries\n",
    "        self.att_bandwidth = scratch.get('att_bandwidth', 3) # ??\n",
    "        self.retention = scratch.get('retention', 5) # ??\n",
    "\n",
    "        # CORE IDENTITY \n",
    "        self.name = scratch.get('name', None)\n",
    "        self.first_name = scratch.get('first_name', None)\n",
    "        self.last_name = scratch.get('last_name', None)\n",
    "        self.age = scratch.get('age', None)\n",
    "        self.innate = scratch.get('innate', None) # Nature of the person\n",
    "        self.learned = scratch.get('learned', None) # Learned traits of the person\n",
    "        self.currently = scratch.get('currently', None) # Any current plans?\n",
    "        self.lifestyle = scratch.get('lifestyle', None) # Lifestyle of the person\n",
    "        self.living_area = scratch.get('living_area', None) # General living area; area where they spent the time most outside of work\n",
    "\n",
    "        # RETRIEVAL RELATED VARIABLES\n",
    "        self.recency_w = scratch.get('recency_w', 1)\n",
    "        self.relevance_w = scratch.get('relevance_w', 1)\n",
    "        self.importance_w = scratch.get('importance_w', 1)\n",
    "        self.recency_decay = scratch.get('recency_decay', 0.99)\n",
    "\n",
    "        # WORLD INFORMATION\n",
    "        self.curr_time = scratch.get('curr_time', datetime.now()) # What's the current time?\n",
    "        self.curr_tile = scratch.get('curr_tile', None) # Where is the person now?\n",
    "        self.daily_plan_req = scratch.get('daily_plan_req', '') # What's a typical daily plan?\n",
    "        tile_filename = '/'.join(filename.split('/')[:-4]) + '/environment/0.json'\n",
    "        curr_tile = json.load(open(tile_filename))[self.name]\n",
    "        self.curr_tile = (curr_tile['x'], curr_tile['y'])\n",
    "        \n",
    "        # PLANNING VARIABLES\n",
    "        self.daily_req = scratch.get('daily_req', [])\n",
    "        self.f_daily_schedule = scratch.get('f_daily_schedule', []) # This is changed every time the hourly activity is decomposed\n",
    "        self.f_daily_schedule_hourly_org = scratch.get('f_daily_schedule_hourly_org', []) # This remains same as f_daily_schedule\n",
    "\n",
    "        # ACTIONS OF THE PERSON\n",
    "        self.act_address = scratch.get('act_address', None)\n",
    "        self.act_start_time = scratch.get('act_start_time', None)\n",
    "        self.act_duration = scratch.get('act_duration', None)\n",
    "        self.act_description = scratch.get('act_description', None)\n",
    "        self.act_event = scratch.get('act_event', (self.name, None, None)) # See the section on events.\n",
    "\n",
    "        # CONVERSATION VARIABLES\n",
    "        self.chatting_with = scratch.get('chatting_with', None)\n",
    "        self.chat = scratch.get('chat', None)\n",
    "        self.chatting_with_buffer = scratch.get('chatting_with_buffer', dict())\n",
    "        self.chatting_end_time = scratch.get('chatting_end_time', None)\n",
    "\n",
    "    def get_f_daily_schedule_index(self, advance=0, main=True):\n",
    "        \"\"\"\n",
    "        Returns the index of action that is taking place now (advance=0) or sometime in future for a non-zero advance minutes. \n",
    "        \"\"\" \n",
    "        total_time_elapsed = self.curr_time.hour * 60 \n",
    "        total_time_elapsed += self.curr_time.minute + advance\n",
    "\n",
    "        ref_list = self.f_daily_schedule if main else self.f_daily_schedule_hourly_org\n",
    "        elapsed, curr_index = 0, 0\n",
    "        for task, duration in ref_list:\n",
    "            elapsed += duration\n",
    "            if elapsed > total_time_elapsed:\n",
    "                return curr_index\n",
    "            curr_index += 1\n",
    "        return curr_index\n",
    "\n",
    "    def get_str_iss(self):\n",
    "        # ISS stands for Identity Stable Set - a bare minimum description of the persona that is used in prompts that need to call on the persona.\n",
    "        commonset = \"\"\n",
    "        commonset += f\"Name: {self.name}\\n\"\n",
    "        commonset += f\"Age: {self.age}\\n\"\n",
    "        commonset += f\"Innate traits: {self.innate}\\n\"\n",
    "        commonset += f\"Learned traits: {self.learned}\\n\"\n",
    "        commonset += f\"Currently: {self.currently}\\n\"\n",
    "        commonset += f\"Lifestyle: {self.lifestyle}\\n\"\n",
    "        commonset += f\"Daily plan requirement: {self.daily_plan_req}\\n\"\n",
    "        if self.curr_time:\n",
    "            commonset += f\"Current Date: {self.curr_time.strftime('%A %B %d')}\\n\"\n",
    "        return commonset\n",
    "\n",
    "    def add_new_action(self, \n",
    "                       action_address,\n",
    "                       action_duration,\n",
    "                       action_description,\n",
    "                       action_event,\n",
    "                       chatting_with, \n",
    "                       chat, \n",
    "                       chatting_with_buffer,\n",
    "                       chatting_end_time,\n",
    "                       act_obj_description,\n",
    "                       act_obj_event,\n",
    "                       act_start_time=None):\n",
    "        self.act_address = action_address\n",
    "        self.act_duration = action_duration\n",
    "        self.act_description = action_description\n",
    "        self.act_event = action_event\n",
    "\n",
    "        self.chatting_with = chatting_with\n",
    "        self.chat = chat\n",
    "        if chatting_with_buffer:\n",
    "            self.chatting_with_buffer.update(chatting_with_buffer)\n",
    "\n",
    "        self.chatting_end_time = chatting_end_time \n",
    "        self.act_start_time = self.curr_time # This is the start time of the action\n",
    "        self.act_path_set = False\n",
    "\n",
    "    def act_check_finished(self):\n",
    "        # Returns True if the action has finished\n",
    "        if not self.act_address:\n",
    "            return True\n",
    "\n",
    "        # Compute the end time for the chat\n",
    "        if self.chatting_with:\n",
    "            end_time = self.chatting_end_time\n",
    "        else:\n",
    "            x = self.act_start_time\n",
    "            if x.second != 0:\n",
    "                x = x.replace(second=0)\n",
    "                x = (x + timedelta(minutes=1))\n",
    "            end_time = (x + timedelta(minutes=self.act_duration))\n",
    "\n",
    "        if end_time < self.curr_time:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def act_summarize_str(self):\n",
    "        start_datetime_str = self.act_start_time.strftime('%A %B %d -- %H:%M %p')\n",
    "        x = f\"[{start_datetime_str}]\\n\"\n",
    "        x += f\"Activity: {self.name} is {self.act_description}\\n\"\n",
    "        x += f\"Address: {self.act_address}\\n\"\n",
    "        x += f\"Duration in minutes (e.g., x min): {str(self.act_duration)} min\\n\"\n",
    "        return ret\n",
    "\n",
    "    def get_curr_event_and_desc(self): \n",
    "        if not self.act_address: \n",
    "          return (self.name, None, None, None)\n",
    "        else: \n",
    "          return (self.act_event[0], \n",
    "                  self.act_event[1], \n",
    "                  self.act_event[2],\n",
    "                  self.act_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1fd36b-bba8-40ed-b6c1-4d9ad6678246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Isabella Rodriguez\n",
      "Age: 34\n",
      "Innate traits: friendly, outgoing, hospitable\n",
      "Learned traits: Isabella Rodriguez is a cafe owner of Hobbs Cafe who loves to make people feel welcome. She is always looking for ways to make the cafe a place where people can come to relax and enjoy themselves.\n",
      "Currently: Isabella Rodriguez is planning on having a Valentine's Day party at Hobbs Cafe with her customers on February 14th, 2023 at 5pm. She is gathering party material, and is telling everyone to join the party at Hobbs Cafe on February 14th, 2023, from 5pm to 7pm.\n",
      "Lifestyle: Isabella Rodriguez goes to bed around 11pm, awakes up around 6am.\n",
      "Daily plan requirement: Isabella Rodriguez opens Hobbs Cafe at 8am everyday, and works at the counter until 8pm, at which point she closes the cafe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = str(PERSONAS_FOLDER / \"Isabella Rodriguez/bootstrap_memory/scratch.json\")\n",
    "stm_mem = Scratch(filename)\n",
    "print(stm_mem.get_str_iss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ae7ae-d9e0-431d-9233-d6dc12e34bd8",
   "metadata": {},
   "source": [
    "## Integrating Memory into Agents\n",
    "\n",
    "**Finally, we define a cognitive function in the agent `perceive_and_retrieve_and_focus` and outline functions related to its downstream effects, such as `open_conversation`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba300629-411b-420f-ad0a-f527e3c23d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persona:\n",
    "    def __init__(self, name, folder_mem, curr_time, initiate_plan=True):\n",
    "        self.name = name\n",
    "        self.s_mem = SpatialMemoryTree(f\"{folder_mem}/bootstrap_memory/spatial_memory.json\")\n",
    "        self.a_mem = AssociativeMemory(f\"{folder_mem}/bootstrap_memory/associative_memory\")\n",
    "        self.scratch = Scratch(f\"{folder_mem}/bootstrap_memory/scratch.json\")\n",
    "        self.scratch.curr_time = curr_time\n",
    "        \n",
    "        if initiate_plan:\n",
    "            self.generate_day_plan(first_day=True)\n",
    "\n",
    "    def generate_day_plan(self, first_day):\n",
    "        # To ensure continuity in plans, we update currently that is akin to reflection on the day and broad goals for the next day\n",
    "        if not first_day:\n",
    "            self.scratch.currently = get_new_currently(self)\n",
    "            \n",
    "        # Generating persona's daily plan in the short term memory\n",
    "        self.scratch.daily_req = generate_day_plan(self)\n",
    "        self.scratch.f_daily_schedule = generate_hourly_schedule(self) # Breaks down the plan into sub units with coherence across the time\n",
    "        self.scratch.f_daily_schedule_hourly_org[:] = (self.scratch.f_daily_schedule)\n",
    "\n",
    "        # Adding the broad plan to the long term memory (associative memory)\n",
    "        curr_date = self.scratch.curr_time.strftime(\"%A %B %d\")\n",
    "        thought = f\"This is {self.scratch.first_name}'s plan for {curr_date}\"\n",
    "        for i in self.scratch.daily_req:\n",
    "            thought += f\" {i},\"\n",
    "        thought = thought[:-1] + \".\"\n",
    "        created = self.scratch.curr_time\n",
    "        expiration =  self.scratch.curr_time + timedelta(days=7) ## EXPIRY = 7 days\n",
    "        s, p, o = (self.scratch.name, \"plan\", curr_date)\n",
    "        keywords = set([\"plan\"])\n",
    "        poignancy = 5\n",
    "        thought_embedding_pair = (thought, get_embedding(thought))\n",
    "        self.a_mem.add_node(\"thought\", created, expiration, s, p, o,\n",
    "                               thought, keywords, poignancy, thought_embedding_pair, None)\n",
    "\n",
    "    def perceive_and_retrieve_and_focus(self, maze):\n",
    "        if self.scratch.act_description is not None and \"sleeping\" in self.scratch.act_description:\n",
    "            return\n",
    "        perceived = perceive(self, maze) # Adds new information to the memory.\n",
    "        retrieved = retrieve(self, perceived)\n",
    "        # Retrieve relevant information from the memory and choose to focus on it, if relevant.\n",
    "        if not retrieved.keys():\n",
    "            return\n",
    "        focus_event = choose_retrieved(persona, retrieved)\n",
    "        return focus_event\n",
    "\n",
    "    def advance_one_step(self, maze, personas, curr_time):\n",
    "        # Obeserve the surroundings, adjust the memory, retrieve relevant information from the memory, chose the event to react to\n",
    "        focus_event = self.perceive_and_retrieve_and_focus(maze)\n",
    "\n",
    "        if focus_event:\n",
    "            react_mode, other = should_react(self, focus_event, personas)\n",
    "            if react_mode == 2 and self.scratch.act_event[1] != \"chat with\": \n",
    "                # If not chatting already, Open a conversation and adjust the schedule\n",
    "                self.open_conversation(maze, other)\n",
    "                \n",
    "        if self.scratch.curr_time.strftime('%A %B %d') != curr_time.strftime('%A %B %d'):\n",
    "            self.generate_day_plan(first_day=False)\n",
    "            \n",
    "        self.scratch.curr_time = curr_time\n",
    "        if self.scratch.act_check_finished():\n",
    "            new_action = determine_action(self, maze) # NOTE: this function call makes changes to f_daily_schedule\n",
    "            self.scratch.add_new_action(**new_action)\n",
    "\n",
    "            # determine it's location [don't change anything yet. Changing it here will lead to other personas not observing this event if reuqired]\n",
    "            action_address = new_action['action_address']\n",
    "            target_tiles = maze.address_tiles[action_address]\n",
    "            new_tile = random.sample(list(target_tiles), 1)[0]\n",
    "            return new_tile\n",
    "        return None\n",
    "\n",
    "    def open_conversation(self, maze, other):\n",
    "        convo, convo_duration_min = generate_convo(maze, self, other)\n",
    "        convo_summary = generate_convo_summary(self, other, convo)\n",
    "        print_convo(convo, convo_duration_min, convo_summary, self.scratch.curr_time, self)\n",
    "\n",
    "        for person, other_person in [(self, other), (other, self)]:\n",
    "            # new actions for each of self and other\n",
    "    \n",
    "            act_address = f\"<persona> {other_person.name}\"\n",
    "            act_event = (person.name, \"chat with\", other_person.name)\n",
    "            chatting_with = other.name\n",
    "            chatting_with_buffer = {other.name: 800}\n",
    "            chatting_end_time = self.scratch.curr_time + timedelta(minutes=convo_duration_min)\n",
    "            chatting_end_time += timedelta(seconds=60 - chatting_end_time.second)\n",
    "            \n",
    "            new_action = {\n",
    "                \"action_address\": act_address,\n",
    "                \"action_duration\": int(convo_duration_min),\n",
    "                \"action_description\": convo_summary,\n",
    "                \"action_event\": act_event,\n",
    "                \"chatting_with\": chatting_with,\n",
    "                \"chat\": convo,\n",
    "                \"chatting_with_buffer\": chatting_with_buffer,\n",
    "                \"chatting_end_time\":chatting_end_time,\n",
    "                \"act_obj_description\": None, \n",
    "                \"act_obj_event\": None\n",
    "            }\n",
    "            person.scratch.add_new_action(**new_action)\n",
    "            # NOTE: In this tutorial, we are not letting conversation affect the new schedules.\n",
    "            # curr_index = person.scratch.get_f_daily_schedule_index(main=True)\n",
    "            # new_schedule, start_index, end_index = generate_updated_schedule(person, convo_summary, convo_duration_min)\n",
    "            # person.scratch.f_daily_schedule[start_index: end_index] = new_schedule\n",
    "\n",
    "    def get_curr_tile(self):\n",
    "        return self.scratch.curr_tile\n",
    "\n",
    "    def move(self, new_tile):\n",
    "        self.scratch.curr_tile = new_tile\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c169cc-3eed-46e6-9cb5-7313df72f2dc",
   "metadata": {},
   "source": [
    "### Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b9ada8-666d-480e-adc7-dfdc1f36a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_day_plan(persona):\n",
    "    \"\"\"Produces broad agenda for the day. \"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"day_planning.txt\")\n",
    "    prompt_input = [\n",
    "        persona.scratch.get_str_iss(),\n",
    "        persona.scratch.lifestyle,\n",
    "        persona.scratch.curr_time.strftime(\"%A %B %d\"),\n",
    "        persona.scratch.first_name\n",
    "    ]\n",
    "    \n",
    "    prompt = generate_prompt(prompt_input, prompt_template_file)\n",
    "    schedule = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "\n",
    "    print_prompt(\"generate_daily_plan\", persona, prompt, schedule, GPT_PARAMS)\n",
    "\n",
    "    schedule = prompt + schedule\n",
    "    schedule = schedule[schedule.find(\"1)\") + 2:]\n",
    "    schedule = re.split(r'\\d+\\)', schedule)\n",
    "\n",
    "    string = f\"day high-level schedule -- {persona.name} -- {persona.scratch.curr_time.strftime('%A %B %d %H:%M')}\\n\"\n",
    "    print_schedule(string, persona.scratch.f_daily_schedule, persona.scratch.curr_time)\n",
    "\n",
    "    return [i.strip() for i in schedule]\n",
    "\n",
    "def generate_hourly_schedule(persona):\n",
    "    \"\"\"Uses broad agenda for the day to plan hourly schedule.\"\"\"\n",
    "    curr_date = persona.scratch.curr_time.strftime(\"%A %B %d\")\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"hourly_planning.txt\")\n",
    "    # Example of a schedule\n",
    "    schedule_format = \"\"\n",
    "    for hour in HOUR_STR:\n",
    "        schedule_format += f\"[{curr_date} -- {hour}]\"\n",
    "        schedule_format += f\" Activity: [Fill in]\\n\"\n",
    "    schedule_format = schedule_format[:-1]\n",
    "\n",
    "    # Broad plan of the persona\n",
    "    plan_str = f\"Here is the orginally intended today's schedule of {persona.scratch.first_name}: \"\n",
    "    for count, activity in enumerate(persona.scratch.daily_req):\n",
    "        plan_str += f\"({str(count+1)}) {activity}, \"\n",
    "    plan_str = plan_str[:-2]\n",
    "    plan_str += f\"\\nIf {persona.scratch.first_name} is sleeping, use 'sleeping' as the activity\"\n",
    "\n",
    "    prompt_inputs = [schedule_format, persona.scratch.get_str_iss(), plan_str, None, None]\n",
    "\n",
    "    # today's prior schedule (needed for coherence)\n",
    "    activities_list = []\n",
    "    prior_schedule = \"\\n\"\n",
    "    for count, hour in enumerate(HOUR_STR):\n",
    "        # prepare the string for prior schedule\n",
    "        if count > 0:\n",
    "            prior_schedule += f\"{curr_date} -- {HOUR_STR[count-1]} Acitvity:\"\n",
    "            prior_schedule += f\" {persona.scratch.first_name} is {activities_list[count - 1]}\\n\"\n",
    "            prompt_inputs[-2] = prior_schedule\n",
    "\n",
    "        # final prompt to be completed\n",
    "        final_prompt = f\" [{curr_date} -- {hour}] Activity: {persona.scratch.first_name} is\"\n",
    "        prompt_inputs[-1] = final_prompt\n",
    "\n",
    "        # modify the parameters because we don't need to generate a lot of tokens\n",
    "        prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "        params = GPT_PARAMS.copy()\n",
    "        params['stop'] = ['\\n']\n",
    "        params['temperature'] = 0.5\n",
    "        params['max_tokens'] = 50\n",
    "        next_hour_activity = safe_prompting(prompt, params, lambda x:x)\n",
    "\n",
    "        print_prompt(\"generate_hourly_schedule\", persona, prompt, next_hour_activity, params)\n",
    "        \n",
    "        activities_list.append(next_hour_activity.strip())\n",
    "\n",
    "    # post-processing the output\n",
    "    compressed_list = [('###', 0)]\n",
    "    for activity in activities_list:\n",
    "        if compressed_list[-1][0] == activity:\n",
    "            compressed_list[-1][1] += 1\n",
    "        else:\n",
    "            compressed_list.append([activity, 1])\n",
    "    compressed_list.pop(0)\n",
    "\n",
    "    string = f\"hourly schedule -- {persona.name} -- {persona.scratch.curr_time.strftime('%A %B %d %H:%M')}\\n\"\n",
    "    print_schedule(string, persona.scratch.f_daily_schedule, persona.scratch.curr_time)\n",
    "    \n",
    "    return [(x, y*60) for x,y in compressed_list]\n",
    "\n",
    "def generate_task_decompose(persona):\n",
    "    \"\"\"Generates 5 min increments of the current action for duration of that action.\"\"\"\n",
    "    curr_date = persona.scratch.curr_time.strftime(\"%A %B %d\")\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"decompose_task.txt\")\n",
    "    \n",
    "    curr_f_org_index = persona.scratch.get_f_daily_schedule_index(main=False) # gets from f_daily_schedule_hourly_org\n",
    "    # print(curr_f_org_index, persona.scratch.f_daily_schedule_hourly_org, len(persona.scratch.f_daily_schedule_hourly_org))\n",
    "    # Prepare a summary string to capture an hour before and an hour after the current action event\n",
    "    summary_str = f\"Today is {curr_date}. From \"\n",
    "    for index in [curr_f_org_index- 1, curr_f_org_index, curr_f_org_index+1]:\n",
    "        if index >= len(persona.scratch.f_daily_schedule_hourly_org) or index < 0:\n",
    "            continue\n",
    "\n",
    "        start_min = sum(i[1] for i in persona.scratch.f_daily_schedule_hourly_org[:index])\n",
    "        action, time_elapsed = persona.scratch.f_daily_schedule_hourly_org[index]\n",
    "        start_time = datetime.strptime(\"00:00:00\", \"%H:%M:%S\") + timedelta(minutes=start_min)\n",
    "        end_time = start_time + timedelta(minutes=time_elapsed)\n",
    "\n",
    "        start_time_str, end_time_str = start_time.strftime('%H:%M%p'), end_time.strftime('%H:%M%p')\n",
    "        summary_str += f\"{start_time_str} ~ {end_time_str}, {persona.name} is planning {action}, \"\n",
    "\n",
    "        if index == curr_f_org_index: # We are interested in decomposing the activity at curr_f_org_index\n",
    "            curr_time_range, curr_time_duration, curr_action_desc = f\"{start_time_str} ~ {end_time_str}\", str(time_elapsed), action\n",
    "            total_time_range = time_elapsed\n",
    "\n",
    "    summary_str = summary_str[:-2] + \". \"\n",
    "\n",
    "    prompt_inputs = [\n",
    "        persona.scratch.get_str_iss(),\n",
    "        summary_str,\n",
    "        persona.scratch.first_name,\n",
    "        curr_action_desc,\n",
    "        curr_time_range,\n",
    "        curr_time_duration\n",
    "    ]\n",
    "\n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['temperature'] = 0.8 ## Empirically, so that it doesn't deviate from the output format.\n",
    "    prompt  = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, params, lambda x:x)\n",
    "    \n",
    "    print_prompt(\"generate_task_decompose\", persona, prompt, response, params)\n",
    "\n",
    "    full_str = prompt + response\n",
    "    rem_str = full_str.split(\"---\")[3]\n",
    "    schedule = re.split(r'\\d+\\>', rem_str)\n",
    "    schedule = [i.strip() for i in schedule if i.strip()]\n",
    "\n",
    "    # post-process this schedule to 5 min increments\n",
    "    activities = [[\"dummy\", -1]]\n",
    "    for activity in schedule:\n",
    "        try:\n",
    "            task, rest = activity.split(\"(duration in minutes:\")\n",
    "        except:\n",
    "            task, rest = activity.split(\"(duration in minutes\") # Failure prevention\n",
    "            \n",
    "        if \",\" not in rest: # FAIL PREVENTION: Sometimes prompt might not give \", minutes left: xx)\" in the end as prompted.\n",
    "            duration = int(rest[:-1])\n",
    "        else:\n",
    "            duration = int(rest.split(\",\")[0])\n",
    "\n",
    "        activities.append([task.strip(), duration])\n",
    "    activities = activities[1:]\n",
    "\n",
    "    # Making sure that the activities fall in the time range.    \n",
    "    lagging_sum, duration_sum, idx = 0, 0, 0\n",
    "    output = []\n",
    "    for task, duration in activities:\n",
    "        duration_sum += duration\n",
    "        if duration_sum <= total_time_range:\n",
    "            output.append([task, duration])\n",
    "        else:\n",
    "            output.append([task, total_time_range - lagging_sum])\n",
    "        idx += 1\n",
    "        lagging_sum += duration\n",
    "\n",
    "    return output\n",
    "\n",
    "def determine_action(persona, maze):\n",
    "    \"\"\"Updates the agent's activity as well as decompose if necessary.\"\"\"\n",
    "    def determine_decompose(act_desc, act_dura):\n",
    "        if \"sleeping\" in act_desc:\n",
    "            return False\n",
    "        if act_dura < 60:\n",
    "            return False \n",
    "        return True\n",
    "    \n",
    "    curr_action_index = persona.scratch.get_f_daily_schedule_index()\n",
    "\n",
    "    act_desc, act_dura =  persona.scratch.f_daily_schedule[curr_action_index]\n",
    "    if determine_decompose(act_desc, act_dura):\n",
    "        persona.scratch.f_daily_schedule[curr_action_index: curr_action_index+1] = (\n",
    "            generate_task_decompose(persona) # GPT\n",
    "        )\n",
    "        string = f\"decomposed -- {persona.name} -- {persona.scratch.curr_time.strftime('%A %B %d %H:%M')}\\n\"\n",
    "        string += f\"Current action: {act_desc}\\nDuration: {act_dura}\"\n",
    "        print_schedule(string, persona.scratch.f_daily_schedule, persona.scratch.curr_time)\n",
    "        \n",
    "        # to add up minutes\n",
    "        total_time_accounted = sum(i[1] for i in persona.scratch.f_daily_schedule)\n",
    "        if total_time_accounted < 1440:\n",
    "            persona.scratch.f_daily_schedule += [[\"sleeping\", 1440 - total_time_accounted]]\n",
    "\n",
    "    act_desc, act_dura = persona.scratch.f_daily_schedule[curr_action_index]\n",
    "\n",
    "    # Now we determine this action's location to execute\n",
    "    act_world = maze.access_tile(persona.scratch.curr_tile)['world']\n",
    "    act_sector = generate_action_sector(act_desc, persona, maze, \n",
    "                                         curr_determined_address=act_world) # GPT\n",
    "    act_arena = generate_action_sector_arena(act_desc, persona, maze, \n",
    "                                        curr_determined_address=f\"{act_world}:{act_sector}\") # GPT\n",
    "    act_object = generate_action_sector_arena_object(act_desc, persona, maze, \n",
    "                                                     curr_determined_address=f\"{act_world}:{act_sector}:{act_arena}\") # GPT\n",
    "    new_address = f\"{act_world}:{act_sector}:{act_arena}\" \n",
    "    new_address += f\":{act_object}\" if act_object else \"\"\n",
    "    act_event = generate_action_event_triple(act_desc, persona)\n",
    "    act_obj_desc = None\n",
    "    act_obj_event = None\n",
    "\n",
    "    return {\n",
    "        \"action_address\": new_address,\n",
    "        \"action_duration\": int(act_dura),\n",
    "        \"action_description\": act_desc,\n",
    "        \"action_event\": act_event,\n",
    "        \"chatting_with\": None, \"chat\": None, \"chatting_with_buffer\": None, \"chatting_end_time\": None,\n",
    "        \"act_obj_description\": act_obj_desc,\n",
    "        \"act_obj_event\": act_obj_event,\n",
    "    }\n",
    "                                   \n",
    "def generate_action_sector(action, persona, maze, curr_determined_address=None):\n",
    "    \"\"\"Returns the appropriate sector to carry out that action.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"determine_action_sector.txt\")\n",
    "    curr_tile = persona.scratch.curr_tile\n",
    "    tile_info = maze.access_tile(curr_tile)\n",
    "    world, curr_sector = tile_info['world'], tile_info['sector']\n",
    "    all_sectors = persona.s_mem.get_str_accessible_sectors(f\"{world}\")\n",
    "    curr_sector_arenas = persona.s_mem.get_str_accessible_sector_arenas(f\"{world}:{curr_sector}\")\n",
    "    \n",
    "    living_area = persona.scratch.living_area\n",
    "    living_area_sector = living_area.split(\":\")[1]\n",
    "    living_area_arenas = persona.s_mem.get_str_accessible_sector_arenas(f\"{world}:{living_area_sector}\")\n",
    "    prompt_inputs = [\n",
    "        persona.scratch.name,\n",
    "        living_area_sector,\n",
    "        living_area_arenas,\n",
    "        curr_sector,\n",
    "        curr_sector_arenas,\n",
    "        all_sectors,\n",
    "        action\n",
    "    ]\n",
    "    \n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['max_tokens'] = 20\n",
    "    params['temperature'] = 0\n",
    "    params['top_p'] = 1\n",
    "    prompt  = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"generate_action_sector\", persona, prompt, response, params)\n",
    "\n",
    "    return response.split(\"}\")[0]\n",
    "\n",
    "def generate_action_sector_arena(action, persona, maze, curr_determined_address):\n",
    "    \"\"\"Returns the appropriate arena within the sector to carry out that action.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"determine_action_arena.txt\")\n",
    "    new_sector = curr_determined_address.split(\":\")[1]\n",
    "    new_possible_arenas = persona.s_mem.get_str_accessible_sector_arenas(curr_determined_address)\n",
    "    prompt_inputs = [\n",
    "        persona.scratch.name,\n",
    "        new_sector,\n",
    "        new_possible_arenas,\n",
    "        action     \n",
    "    ]\n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['max_tokens'] = 20\n",
    "    params['temperature'] = 0\n",
    "    params['top_p'] = 1\n",
    "    prompt  = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, params, lambda x:x)\n",
    "    \n",
    "    print_prompt(\"generate_action_sector\", persona, prompt, response, params)\n",
    "\n",
    "    if response.split(\"}\")[0] not in [x.strip() for x in new_possible_arenas.split(\",\")]:\n",
    "        object = random.sample(new_possible_arenas.split(\",\"), 1)[0].strip()\n",
    "        \n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"new_possible_arenas @ {persona.name} @ {curr_determined_address} @ {action} --> {object}\\n\"\n",
    "        string += f\"response: {response}\\n\"\n",
    "        string += f\"failed: response not in {new_possible_arenas.split(',')}\\n\"\n",
    "        string += \"default: random.sample \\n\\n\"\n",
    "        print_failsafe(\"generate_action_sector_arena\", string)\n",
    "    \n",
    "    return response.split(\"}\")[0]\n",
    "\n",
    "def generate_action_sector_arena_object(action, persona, maze, curr_determined_address):\n",
    "    \"\"\"Returns the appropriate object within the arena to carry out that action.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"determine_action_object.txt\")\n",
    "    possible_objects = persona.s_mem.get_str_accessible_arena_game_objects(curr_determined_address)\n",
    "    prompt_inputs = [\n",
    "        f\"{action}\",\n",
    "        possible_objects    \n",
    "    ]\n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['max_tokens'] = 20\n",
    "    params['temperature'] = 0.1\n",
    "    params['top_p'] = 1\n",
    "    params['stop'] = [\"\\n\"]\n",
    "    prompt  = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"generate_action_sector_arena_object\", persona, prompt, response, params)\n",
    "    \n",
    "    # Fail safe mechanism\n",
    "    if response.strip() not in [x.strip() for x in possible_objects.split(\",\")]:\n",
    "        object = random.sample(possible_objects.split(\",\"), 1)[0].strip()\n",
    "        \n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"generate_action_sector_arena_object @ {persona.name} @ {curr_determined_address} @ {action} --> {object}\\n\"\n",
    "        string += f\"response: {response}\\n\"\n",
    "        string += f\"failed: response not in {possible_objects.split(',')}\\n\"\n",
    "        string += \"default: random.sample \\n\\n\"\n",
    "        print_failsafe(\"generate_action_sector_arena_object\", string)\n",
    "        \n",
    "        return object\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "def generate_action_event_triple(action, persona):\n",
    "    \"\"\"Returns the (subject, predicate, object) decomposition corresponding to the action.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"generate_event_triplet.txt\")\n",
    "    prompt_inputs = [\n",
    "        persona.scratch.name,\n",
    "        action.lower()\n",
    "    ]  \n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['max_tokens'] = 50\n",
    "    params['temperature'] = 0\n",
    "    params['top_p'] = 1\n",
    "    prompt  = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"generate_action_event_triple\", persona, prompt, response, params)\n",
    "\n",
    "    full_str = prompt + response\n",
    "    output = full_str.split(\"---\")[-1].split(\"Output:\")[-1].strip()[1:]\n",
    "\n",
    "    output = [i.strip() for i in output.split(\")\")[0].split(\",\")]\n",
    "    if len(output) != 3:\n",
    "        output = [persona.scratch.name, 'is', output[-1]]\n",
    "\n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"generate_action_event_triple @ {persona.name} @ {action} --> {output}\\n\"\n",
    "        string += f\"response: {response}\\n\"\n",
    "        string += f\"failed: len(output) != 3\\n\"\n",
    "        string += \"default: output = [persona.scratch.name, 'is', output[-1]] \\n\\n\"\n",
    "        print_failsafe(\"generate_action_event_triple\", string)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_new_currently(persona):\n",
    "    \"\"\"Reflects on the day's activity and returns a new `currently` for persona to take on. \"\"\"\n",
    "    name = persona.scratch.name \n",
    "    curr_day = persona.scratch.curr_time.strftime(\"%A %B %d\")\n",
    "    queries = [\n",
    "        f\"{name}'s plan for {curr_day}\",\n",
    "        f\"Important recent events for {name}'s life.\"\n",
    "    ]\n",
    "    retrieved = extract_relevant_nodes(persona, queries, count=30)\n",
    "\n",
    "    # Add statements about the retrieved nodes\n",
    "    statements = \"[Statements]\\n\"\n",
    "    for query, nodes in retrieved.items():\n",
    "        for node in nodes:\n",
    "            statements += f\"{node.created.strftime('%A %B %d -- %H:%M %p')}: {node.embedding_key}\\n\"\n",
    "\n",
    "    # Create a broad agenda for the next day\n",
    "    planning_prompt = f\"\"\"{statements}\n",
    "    Given the statements above, is there anything that {name} should remember as they plan for *{curr_day}*?\n",
    "    If there is any scheduling information, be as specific as possible (including date, time, and location if stated in the statement).\\n\n",
    "    Write the response from {name}'s perspective.\n",
    "    \"\"\"\n",
    "    params = GPT_PARAMS.copy()\n",
    "    params['model'] = \"gpt-3.5-turbo\"\n",
    "    params['max_tokens'] = 1000\n",
    "    params['temperature'] = 0.8\n",
    "    plan_note = safe_prompting(planning_prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"get_new_currently --> plan_note\", persona, planning_prompt, plan_note, params)\n",
    "\n",
    "    thought_prompt = f\"\"\"{statements}\n",
    "    Given the statements above, how might we summarize {name}'s feelings about their days up to now?\\n\n",
    "    Write the response from {name}'s perspective.\n",
    "    \"\"\"\n",
    "    thought_note = safe_prompting(thought_prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"get_new_currently --> thought_note\", persona, thought_prompt, thought_note, params)\n",
    "\n",
    "    prev_currently = persona.scratch.currently\n",
    "    prev_day = persona.scratch.curr_time - timedelta(days=1)\n",
    "    prev_day = prev_day.strftime('%A %B %d')\n",
    "    update_currently_prompt = f\"\"\"\n",
    "    {name}'s status from {prev_day}: {prev_currently}\\n\\n\n",
    "    {name}'s thoughts at the end of {prev_day}: {plan_note} {thought_note}\\n\\n\n",
    "    It is now {curr_day}. Given the above, write {name}'s status for {curr_day} that reflects {name}'s thoughts at the end of {curr_day}.\n",
    "    Write this in third-person talking about {name}.\n",
    "    If there is any scheduling information, be as specific as possible (include date, time, and location if stated in the statement).\\n\\n\n",
    "    Follow this format below:\\nStatus: <new_status>\n",
    "    \"\"\"\n",
    "    new_currently = safe_prompting(update_currently_prompt, params, lambda x:x)\n",
    "\n",
    "    print_prompt(\"get_new_currently --> new_currently\", persona, update_currently_prompt, new_currently, params)\n",
    "\n",
    "    return new_currently\n",
    "\n",
    "def extract_relevant_nodes(persona, queries, count=30):\n",
    "    \"\"\"Retrieves nodes from agent's memory relevant to all `query` in `queries`.\"\"\"\n",
    "    nodes = []\n",
    "    for node in persona.a_mem.seq_thought + persona.a_mem.seq_event + persona.a_mem.seq_chat:\n",
    "        if \"idle\" not in node.embedding_key:\n",
    "            nodes.append([node.last_accessed, node])\n",
    "    nodes = sorted(nodes, key=lambda x:x[0])\n",
    "    nodes = [node for _, node in nodes]\n",
    "\n",
    "    persona_receny_w = persona.scratch.recency_decay\n",
    "    recency = _normalize([persona_receny_w**i for i in range(1, len(nodes) + 1)])\n",
    "    importance = _normalize([node.poignancy for node in nodes])\n",
    "    \n",
    "    retrieved = dict()\n",
    "    v1, v2, v3 = persona.scratch.recency_w, persona.scratch.relevance_w, persona.scratch.importance_w\n",
    "    w1, w2, w3 = 0.5, 3, 2 ## HARD CODED WEIGHTS \n",
    "    for query in queries:\n",
    "        query_embedding = get_embedding(query)\n",
    "        node_relevance = _normalize([cos_sim(node.embedding, query_embedding) for node in nodes])\n",
    "        node_relevance = [x*v1*w1 + y*v2*w2 + z*v3*w3 for x,y,z in zip(recency, importance, node_relevance)]\n",
    "        top_nodes = sorted([(val, idx) for idx, val in enumerate(node_relevance)], key=lambda x:x[0])[-count:]\n",
    "        for _, idx in top_nodes:\n",
    "            nodes[idx].last_accessed = persona.scratch.curr_time\n",
    "        retrieved[query] = [nodes[idx] for _, idx in top_nodes]\n",
    "\n",
    "    return retrieved\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1bdf20d-f3dd-413d-be0f-e124d9982408",
   "metadata": {},
   "source": [
    "## Cognitive Functions\n",
    "Enhancing agency in simulation agents.\n",
    "\n",
    "In this section, we outline the cognitive functions of an agent, focusing on how these functions record the surrounding environment as perceptions, document interactions (chats) with other agents, add them to appropriate memory structures, and retrieve relevant memories as needed.\n",
    "\n",
    "### Perceive\n",
    "Perception in agents is influenced by several design factors. For example, agents only observe tiles within their perception radius (`vision_r`). Moreover, among all perceived events, an agent can focus on only a limited number (`att_bandwidth`) of them. Once determined, these events are added to the associative memory, categorized either as chats or events.\n",
    "\n",
    "It's important to note that each event is assigned a score for its importance (`poignancy`). These scores are generated by LLMs using specific prompts, which are then utilized in the retrieval function defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a351b99-c122-4ebf-8419-1c71771ba461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceive(persona, maze):\n",
    "    curr_tile = persona.scratch.curr_tile\n",
    "    curr_arena_address = maze.get_tile_path(curr_tile, level='arena')\n",
    "    nearby_tiles = maze.get_nearby_tiles(persona.scratch.curr_tile, persona.scratch.vision_r)\n",
    "\n",
    "    percept_events_set = set()\n",
    "    percept_events_list = []\n",
    "\n",
    "    # We add new objects to the spatial memory of the agent\n",
    "    # We also take notice of new events and their distance from the agent\n",
    "    for i in nearby_tiles:\n",
    "        tile_info = maze.access_tile(i)\n",
    "\n",
    "        world, sector = tile_info.get('world', None), tile_info.get('sector', None)\n",
    "        arena, object = tile_info.get('arena', None), tile_info.get('object', None)\n",
    "        \n",
    "        # Initialize spatial memory of persona with this tile\n",
    "        if world and world not in persona.s_mem.tree: \n",
    "            persona.s_mem.tree[world] = {}\n",
    "\n",
    "        if sector and sector not in persona.s_mem.tree[world]: \n",
    "            persona.s_mem.tree[world][sector] = {}\n",
    "\n",
    "        if arena and arena not in persona.s_mem.tree[world][sector]: \n",
    "            persona.s_mem.tree[world][sector][arena] = []\n",
    "\n",
    "        if object and object not in persona.s_mem.tree[world][sector][arena]: \n",
    "            persona.s_mem.tree[world][sector][arena].append(object)\n",
    "\n",
    "        tile_arena_address = maze.get_tile_path(i, level=\"arena\")\n",
    "        if tile_info['events'] and tile_arena_address == curr_arena_address:\n",
    "            dist = math.dist(i, curr_tile)\n",
    "\n",
    "            # add events\n",
    "            for event in tile_info['events']:\n",
    "                if event not in percept_events_set:\n",
    "                    percept_events_list += [[dist, event]]\n",
    "                    percept_events_set.add(event)\n",
    "\n",
    "    # Agent only retains events based on their distance\n",
    "    percept_events_list = sorted(percept_events_list, key=itemgetter(0))\n",
    "    perceived_events = [event for dist, event in percept_events_list[:persona.scratch.att_bandwidth]]\n",
    "\n",
    "    # Add these events (if new) to appropriate memory structure of the agent (associative memory and scratch)\n",
    "    latest_events = persona.a_mem.get_summarized_latest_events(persona.scratch.retention)\n",
    "    ret_events = []\n",
    "    # print(f\"LATEST EVENTS: {latest_events}\\n\\nPERCEIVED EVENTS:{perceived_events}\")\n",
    "    for p_event in perceived_events:\n",
    "        subject, predicate, object, desc = p_event\n",
    "        if not predicate:\n",
    "            predicate, object, desc = \"is\", \"idle\", \"idle\"\n",
    "        desc = f\"{subject.split(':')[-1]} is {desc}\"\n",
    "        p_event = (subject, predicate, object)\n",
    "        \n",
    "        if p_event not in latest_events:\n",
    "            \n",
    "            sub = p_event[0] if \":\" not in p_event[0] else p_event[0].split(\":\")[-1]\n",
    "            obj = p_event[2] if \":\" not in p_event[2] else p_event[2].split(\":\")[-1]\n",
    "            keywords = set([sub, obj])\n",
    "\n",
    "            # embeddings to represent these events (think of neural encodings in brain, maybe)\n",
    "            desc_embedding_in = desc\n",
    "            if \"(\" in desc:\n",
    "                # \"(xyz)\" --> (\"xyz\")\n",
    "                desc_embedding_in = (desc_embedding_in.split(\"(\")[1].split(\")\")[0].strip())\n",
    "\n",
    "            if desc_embedding_in in persona.a_mem.embeddings:\n",
    "                event_embedding = persona.a_mem.embeddings[desc_embedding_in]\n",
    "            else:\n",
    "                event_embedding = get_embedding(desc_embedding_in)\n",
    "            event_embedding_pair = (desc_embedding_in, event_embedding)\n",
    "\n",
    "            # poignancy\n",
    "            event_poignancy = generate_poignancy_score(persona, \"event\", desc_embedding_in)\n",
    "\n",
    "            # Add chats to the memory\n",
    "            created, expiration = persona.scratch.curr_time, None\n",
    "            chat_node_ids = []\n",
    "            if p_event[0] == persona.name and p_event[1] == \"chat with\":\n",
    "                curr_event = persona.scratch.act_event\n",
    "                act_desc = persona.scratch.act_description\n",
    "                if act_desc in persona.a_mem.embeddings:\n",
    "                    chat_embedding = persona.a_mem.embeddings[act_desc]\n",
    "                else:\n",
    "                    chat_embedding = get_embedding(act_desc)\n",
    "                chat_embedding_pair = (act_desc, chat_embedding)\n",
    "                chat_poignancy = generate_poignancy_score(persona, \"chat\", act_desc)\n",
    "                chat_node = persona.a_mem.add_node(\"chat\", created, expiration, \n",
    "                                                   curr_event[0], curr_event[1], curr_event[2],\n",
    "                                                   act_desc, keywords, chat_poignancy, chat_embedding_pair,\n",
    "                                                   persona.scratch.chat)\n",
    "                chat_node_ids = [chat_node.node_id]            \n",
    "            \n",
    "            new_node_in_mem = persona.a_mem.add_node('event', created, expiration, subject, predicate,\n",
    "                 object, desc, keywords, event_poignancy, event_embedding_pair, chat_node_ids) \n",
    "            ret_events.append(new_node_in_mem)\n",
    "\n",
    "    return ret_events \n",
    "\n",
    "def generate_poignancy_score(persona, event_type, description):\n",
    "    \"\"\"Returns the importance or poignancy of the event based on its description and the agent characteristics. \"\"\"\n",
    "    if \"is idle\" in description:\n",
    "        return 1\n",
    "\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / f\"generate_{event_type}_poignancy_score.txt\")\n",
    "        \n",
    "    prompt_inputs = [\n",
    "        persona.name,\n",
    "        persona.scratch.get_str_iss(),\n",
    "        description,\n",
    "    ]\n",
    "\n",
    "    prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    score = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "    \n",
    "    print_prompt(f\"generate_poignancy_score -- {event_type}\", persona, prompt, score, GPT_PARAMS)\n",
    "\n",
    "    try: return int(score)\n",
    "    except:\n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"generate_poignancy_score -- {event_type} @ {persona.name} @ {description} --- Response: {score}\\n\"\n",
    "        string += f\"response: {score}\\n\"\n",
    "        string += f\"failed: int(score)\\n\"\n",
    "        string += \"default: 0\\n\\n\"\n",
    "        print_failsafe(\"generate_poignancy_score\", string)\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bde60-6457-4d47-860e-e89901b5e78e",
   "metadata": {},
   "source": [
    "### Retrieve\n",
    "\n",
    "Each perceived event is a `ConceptNode` in the agent's memory. These evetns invoke certain thoughts and events in the generative agent. \n",
    "The following function does the job of retrieving (`retrieve`) and determining what's worth focusing on (`choose_retrieved`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28964b2b-92a6-4af5-a453-f973b4073b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(persona, perceived):\n",
    "    \"\"\"Returns relevant events and thoughts in the agent memory corresponding to each event in `perceived`.\"\"\"\n",
    "    retrieved = dict()\n",
    "    for node in perceived: \n",
    "        retrieved[node.description] = dict()\n",
    "        retrieved[node.description][\"curr_event\"] = node\n",
    "        \n",
    "        relevant_events = persona.a_mem.retrieve_relevant_events(\n",
    "                            node.subject, node.predicate, node.object)\n",
    "        retrieved[node.description][\"events\"] = list(relevant_events)\n",
    "        \n",
    "        relevant_thoughts = persona.a_mem.retrieve_relevant_thoughts(\n",
    "                              node.subject, node.predicate, node.object)\n",
    "        retrieved[node.description][\"thoughts\"] = list(relevant_thoughts)\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "def choose_retrieved(persona, retrieved):\n",
    "    \"\"\"Chooses one of the retrieved events. Currently, it only retrieves if there are other agents around. \"\"\"\n",
    "    relevant = []\n",
    "    # conditions are as used in the simulacra code. See _choose_retrieved in plan.py\n",
    "    for event_desc, info in retrieved.items():\n",
    "        node = info['curr_event']\n",
    "        if node.subject == persona.name: \n",
    "            continue\n",
    "        if (\":\" not in node.subject and \"is idle\" not in event_desc):\n",
    "            relevant.append(info)\n",
    "\n",
    "    if relevant:\n",
    "        # print(f\"RELEVANT: {relevant}\")\n",
    "        return random.choice(relevant)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc4d06-1173-4951-ae34-9bbf420e3424",
   "metadata": {},
   "source": [
    "### React / Chat\n",
    "\n",
    "This section outlines the decision-making process regarding an agent's response to a focused event (`should_react`). The determination of whether and how an agent should react (`generate_reaction_type`) encompasses several outcomes: taking no action, continuing its current activity, or initiating a conversation with another agent.\n",
    "\n",
    "Should the agent opt to engage in a conversation, we have functions designed to facilitate this interaction between two agents (`generate_convo`). Following the completion of the conversation, a summarized version (`generate_convo_summary`) is incorporated into the agent’s short-term memory. Subsequently, the perception function archives this summary into the long-term memory along with the conversation's details.\n",
    "\n",
    "Note: Ideally, any such conversation would be reflected in the agent's schedule. However, for simplicity, we will not account for changes to the schedule resulting from these interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3054b8be-d9f2-4a24-82ad-405c4c3b2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_react(persona, focused_event, all_personas):\n",
    "    \"\"\"Determines iff there should be a reaction. If so, what type of reaction? 0=do nothing, 1=continue with their work (~0), 2=chat.\"\"\"\n",
    "    \n",
    "    if \"<waiting>\" in persona.scratch.act_address:\n",
    "        return 0, None\n",
    "    \n",
    "    event_node = focused_event['curr_event']\n",
    "\n",
    "    # In this notebook, we don't converse with other agents --- see the next notebook for that.\n",
    "    if \":\" in event_node.subject: # this is an object --- we don't consider any reaction to the objects in this simulation\n",
    "        return 0, None\n",
    "\n",
    "    other_persona = [p for p in all_personas if p.name == event_node.subject]\n",
    "    if not other_persona:\n",
    "        return 0, None\n",
    "\n",
    "    react_mode = generate_reaction_type(persona, focused_event, other_persona[0])\n",
    "    return react_mode, other_persona[0]\n",
    "\n",
    "def generate_reaction_type(persona, focused_event, other_persona):\n",
    "    \"\"\"Prompts LLMs to determine the reaction type in response to `focus_event`. \"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"generate_reaction_type.txt\")\n",
    "    curr_time_str = persona.scratch.curr_time.strftime(\"%B %d, %Y, %H:%M:%S %p\")\n",
    "    focused_event_description = focused_event['curr_event'].description\n",
    "\n",
    "    name = persona.name\n",
    "    # relevant events from persona's memory\n",
    "    context_str = f\"{name} just observed {focused_event_description}.\"\n",
    "    context_str += f\"These are the past relevant events in {name}'s experience:\\n\"\n",
    "    for node in focused_event['events']:\n",
    "        context_str += f\"{node.description}. \"\n",
    "    \n",
    "    # relevant thoughts from persona's memory\n",
    "    context_str += f\"\\nThese are the relevant thoughts in {name}'s mind:\"\n",
    "    for node in focused_event['thoughts']:\n",
    "        context_str += f\"{node.description}. \"\n",
    "\n",
    "    # what is persona doing right now\n",
    "    persona_action_desc = persona.scratch.act_description\n",
    "\n",
    "    # what is other_persona doing right now\n",
    "    other_persona_action_desc = other_persona.scratch.act_description\n",
    "    \n",
    "    prompt_inputs = [\n",
    "        context_str,\n",
    "        curr_time_str,\n",
    "        persona_action_desc,\n",
    "        other_persona_action_desc,\n",
    "        name,\n",
    "        other_persona.scratch.name,\n",
    "        persona.scratch.get_str_iss(),\n",
    "        other_persona.scratch.get_str_iss(),\n",
    "    ]\n",
    "\n",
    "    prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "\n",
    "    print_prompt(f\"generate_reaction_type\", persona, prompt, response, GPT_PARAMS)\n",
    "    \n",
    "    try:\n",
    "        return int(response.split(\":\")[0].split(\"Option \")[-1])\n",
    "    except:\n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"generate_reaction_type -- {persona.scratch.name} -- {other_persona.scratch.name} -- {curr_time_str}\\n\"\n",
    "        string += f\"response: {response}\\n\"\n",
    "        string += \"failed: int(response.split(':')[0].split('Option ')[-1])\\n\"\n",
    "        string += \"default: Option 2\\n\\n\"\n",
    "        print_failsafe(\"generate_reaction_type\", string)\n",
    "        return 2\n",
    "\n",
    "def generate_convo(maze, persona, other_persona):\n",
    "    \"\"\"Generates conversation.\"\"\"\n",
    "    convo = simulate_convo(maze, persona, other_persona)\n",
    "\n",
    "    all_utt = \"\"\n",
    "    for row in convo:\n",
    "        speaker = row[0]\n",
    "        utt = row[1]\n",
    "        all_utt += f\"{speaker}: {utt}\\n\"\n",
    "\n",
    "    # Heuristic: 30 words per minute where each word has 8 characters on average\n",
    "    # Note: usual statistics is different; 120 words per minute in normal conversation. \n",
    "    convo_duration_min = math.ceil(int(len(all_utt)/8 / 30))\n",
    "    return convo, convo_duration_min\n",
    "\n",
    "\n",
    "def simulate_convo(maze, persona, other_persona):\n",
    "    \"\"\"Simulates conversation between two agents.\"\"\"\n",
    "    curr_chat = []\n",
    "    for i in range(8):\n",
    "        for speaker, listener in [(persona, other_persona), (other_persona, persona)]:\n",
    "            focal_points = [f\"{listener.scratch.name}\"]\n",
    "            retrieved_nodes = extract_relevant_nodes(speaker, focal_points, count=50) # What does agent know about the other persona?\n",
    "            relationship = generate_summarize_relationship(speaker, listener, retrieved_nodes) # summarize relationship between them\n",
    "    \n",
    "            # Create new focal points for the new conversation\n",
    "            focal_points = [\n",
    "                f\"{relationship}\",\n",
    "                f\"{listener.scratch.name} is {listener.scratch.act_description}\"\n",
    "            ]\n",
    "            last_chat = [\": \".join(i) + \"\\n\" for i in curr_chat[-4:]]\n",
    "            if last_chat:\n",
    "                focal_points.append(\"\".join(last_chat))\n",
    "            retrieved_nodes = extract_relevant_nodes(speaker, focal_points, count=15)\n",
    "            utterance, end = generate_one_utterance(maze, speaker, listener, retrieved_nodes, curr_chat)\n",
    "            curr_chat += [[speaker.scratch.name, utterance.strip()]]\n",
    "\n",
    "            if end:\n",
    "                break\n",
    "    \n",
    "        if end: \n",
    "            break\n",
    "    \n",
    "    return curr_chat\n",
    "\n",
    "def generate_summarize_relationship(persona, other_persona, retrieved_nodes):\n",
    "    \"\"\"Summarizes relationship between the to agents.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"summarize_relationship.txt\")\n",
    "    all_embedding_keys = list()\n",
    "    all_embedding_keys = [f\"{i.embedding_key}\\n\" for key, val in retrieved_nodes.items() for i in val]\n",
    "    all_embedding_keys_str = \"\".join(all_embedding_keys)\n",
    "    prompt_inputs = [\n",
    "        all_embedding_keys_str,\n",
    "        persona.scratch.name,\n",
    "        other_persona.scratch.name\n",
    "    ]\n",
    "\n",
    "    prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "\n",
    "    print_prompt(f\"generate_summarize_relationship\", persona, prompt, response, GPT_PARAMS)\n",
    "    return response\n",
    "\n",
    "def generate_one_utterance(maze, speaker, listener, retrieved_nodes, curr_chat):\n",
    "    \"\"\"Generates the utterance of the `speaker` in response to `listener` and the current context. \"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"generate_one_utterance.txt\")\n",
    "    curr_context = f\"\"\"\n",
    "    {speaker.scratch.name} was {speaker.scratch.act_description} when {speaker.scratch.name} saw {listener.scratch.name}\n",
    "    in the middle of {listener.scratch.act_description}.\\n{speaker.scratch.name} is initiating a conversation with {listener.scratch.name}.\n",
    "    \"\"\"\n",
    "    retrieved_memory_str = [f\"- {v.description}\\n\" for key, vals in retrieved_nodes.items() for v in vals]\n",
    "    # Adding the last conversation between the two\n",
    "    prev_convo = \"\"\n",
    "    for i in speaker.a_mem.seq_chat:\n",
    "        if i.object == listener.scratch.name:\n",
    "            mins_ago = int((speaker.scratch.curr_time - i.created).total_seconds()/60)\n",
    "            prev_convo = f\"{str(mins_ago)} minutes ago, {speaker.scratch.name} and {listener.scratch.name} were already {i.description}. This context takes place after that conversation.\"\n",
    "            break\n",
    "\n",
    "    tile_info = maze.access_tile(speaker.scratch.curr_tile)\n",
    "    curr_location = f\"{tile_info['arena']} in {tile_info['sector']}\"\n",
    "    if len(curr_chat) == 0:\n",
    "        convo_str = \"[The conversation has not started yet -- start it!]\"\n",
    "    else:\n",
    "        convo_str = [\": \".join(i) + \"\\n\" for i in curr_chat]\n",
    "        \n",
    "    prompt_inputs = [\n",
    "        speaker.scratch.get_str_iss(),\n",
    "        speaker.scratch.name,\n",
    "        \"\".join(retrieved_memory_str),\n",
    "        prev_convo,\n",
    "        curr_location,\n",
    "        curr_context,\n",
    "        listener.scratch.name,\n",
    "        \"\".join(convo_str),\n",
    "    ]\n",
    "    prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "    response = response.strip()\n",
    "    print_prompt(f\"generate_one_utterance -- speaker: {speaker.scratch.name} -- listener: {listener.name}\", speaker, prompt, response, GPT_PARAMS)\n",
    "\n",
    "    try:\n",
    "        x = response.strip().split(\"\\n\")\n",
    "        utt = x[0].split(f\"{speaker.scratch.name}:\")[-1].strip()\n",
    "        if len(x) == 2 and x[1] != \"\" :\n",
    "            end = False if \"False\" in x[1] else True\n",
    "        else:\n",
    "            end = True\n",
    "    except Exception as e:\n",
    "        string = \">\"*50 + \"<\"*50 + \"\\n\" + f\"generate_one_utterance -- speaker: {speaker.scratch.name} -- listener: {listener.name}. returning dict()\\n\"\n",
    "        string += f\"response: {response}\\n\"\n",
    "        string += f\"failed: response.split(f'{speaker.scratch.name}:')[-1] -- {e}\\n\"\n",
    "        string += \"default: utt:'' end:True \\n\\n\"\n",
    "        print_failsafe(\"generate_one_utterance\", string)\n",
    "        utt, end = \"\", True\n",
    "\n",
    "    return utt, end\n",
    "\n",
    "def generate_convo_summary(persona, other_persona, convo):\n",
    "    \"\"\"Summarizes the conversation between two agents.\"\"\"\n",
    "    prompt_template_file = str(TEMPLATE_FOLDER / \"summarize_convo.txt\")\n",
    "    convo_str  = [\": \".join(row) + \"\\n\" for row in convo]\n",
    "    prompt_inputs = [\n",
    "        \"\".join(convo_str),\n",
    "        persona.scratch.name,\n",
    "        other_persona.scratch.name,\n",
    "    ]\n",
    "\n",
    "    prompt = generate_prompt(prompt_inputs, prompt_template_file)\n",
    "    response = safe_prompting(prompt, GPT_PARAMS, lambda x:x)\n",
    "\n",
    "    print_prompt(f\"generate_convo_summary -- intiator: {persona.scratch.name} -- other: {other_persona.name}\", persona, prompt, response, GPT_PARAMS)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7fe64-17a1-4771-90fb-0f8baba643db",
   "metadata": {},
   "source": [
    "## Simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ab8eba-e7ad-4492-bc2b-7d4c41f46a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:10\n",
      "00:20\n",
      "00:30\n",
      "00:40\n",
      "00:50\n",
      "01:00\n",
      "01:10\n",
      "01:20\n",
      "01:30\n",
      "01:40\n",
      "01:50\n",
      "02:00\n",
      "02:10\n",
      "02:20\n",
      "02:30\n",
      "02:40\n",
      "02:50\n",
      "03:00\n",
      "03:10\n",
      "03:20\n",
      "03:30\n",
      "03:40\n",
      "03:50\n",
      "04:00\n",
      "04:10\n",
      "04:20\n",
      "04:30\n",
      "04:40\n",
      "04:50\n",
      "05:00\n",
      "05:10\n",
      "05:20\n",
      "05:30\n",
      "05:40\n",
      "05:50\n",
      "06:00\n",
      "06:10\n",
      "06:20\n",
      "06:30\n",
      "06:40\n",
      "06:50\n",
      "07:00\n",
      "07:10\n",
      "07:20\n",
      "07:30\n",
      "07:40\n",
      "07:50\n",
      "08:00\n",
      "08:10\n",
      "08:20\n",
      "08:30\n",
      "08:40\n",
      "08:50\n",
      "09:00\n",
      "09:10\n",
      "09:20\n",
      "09:30\n",
      "09:40\n",
      "09:50\n",
      "10:00\n",
      "10:10\n",
      "10:20\n",
      "10:30\n",
      "10:40\n",
      "10:50\n",
      "11:00\n",
      "11:10\n",
      "11:20\n",
      "11:30\n",
      "11:40\n",
      "11:50\n",
      "12:00\n",
      "12:10\n",
      "12:20\n",
      "12:30\n",
      "12:40\n",
      "12:50\n",
      "13:00\n",
      "13:10\n",
      "13:20\n",
      "13:30\n",
      "13:40\n",
      "13:50\n",
      "14:00\n",
      "14:10\n",
      "14:20\n",
      "14:30\n",
      "14:40\n",
      "14:50\n",
      "15:00\n",
      "15:10\n",
      "15:20\n",
      "15:30\n",
      "15:40\n",
      "15:50\n",
      "16:00\n",
      "16:10\n",
      "16:20\n",
      "16:30\n",
      "16:40\n",
      "16:50\n",
      "17:00\n",
      "17:10\n",
      "17:20\n",
      "17:30\n",
      "17:40\n",
      "17:50\n",
      "18:00\n",
      "18:10\n",
      "18:20\n",
      "18:30\n",
      "18:40\n",
      "18:50\n",
      "19:00\n",
      "19:10\n",
      "19:20\n",
      "19:30\n",
      "19:40\n",
      "19:50\n",
      "20:00\n",
      "20:10\n",
      "20:20\n",
      "20:30\n",
      "20:40\n",
      "20:50\n",
      "21:00\n",
      "21:10\n",
      "21:20\n",
      "21:30\n",
      "21:40\n",
      "21:50\n",
      "22:00\n",
      "22:10\n",
      "22:20\n",
      "22:30\n",
      "22:40\n",
      "22:50\n",
      "23:00\n",
      "23:10\n",
      "23:20\n",
      "23:30\n",
      "23:40\n",
      "23:50\n",
      "00:00\n",
      "00:10\n",
      "00:20\n",
      "00:30\n",
      "00:40\n",
      "00:50\n",
      "01:00\n",
      "01:10\n",
      "01:20\n",
      "01:30\n",
      "01:40\n",
      "01:50\n",
      "02:00\n",
      "02:10\n",
      "02:20\n",
      "02:30\n",
      "02:40\n",
      "02:50\n",
      "03:00\n",
      "03:10\n",
      "03:20\n",
      "03:30\n",
      "03:40\n",
      "03:50\n",
      "04:00\n",
      "04:10\n",
      "04:20\n",
      "04:30\n",
      "04:40\n",
      "04:50\n",
      "05:00\n",
      "05:10\n",
      "05:20\n",
      "05:30\n",
      "05:40\n",
      "05:50\n",
      "06:00\n"
     ]
    }
   ],
   "source": [
    "# Clear file contents\n",
    "open(SIM_LOGFILE, 'w').close()\n",
    "open(PROMPT_LOGFILE, 'w').close() \n",
    "open(FAILSAFE_LOGFILE, 'w').close()\n",
    "open(CONVERSATION_LOGFILE, 'w').close() \n",
    "open(SCHEDULES_LOGFILE, 'w').close() \n",
    "CALL_LOGS = {'api_calls': 0, 'fail_safe_counts': {}}\n",
    "\n",
    "maze = Maze(\"the Ville\")\n",
    "curr_time = sim_start_time = datetime(2024, 2, 13, 0, 0, 0) # Start at midnight\n",
    "seconds_per_step = 10 * 60 # 10 minutes\n",
    "n_steps = 180\n",
    "\n",
    "personas = []\n",
    "for persona_folder in PERSONAS_FOLDER.iterdir():\n",
    "    personas.append(Persona(persona_folder.name, persona_folder, curr_time, initiate_plan=True))\n",
    "\n",
    "step = 0\n",
    "personas = personas[:3]\n",
    "movements = {}\n",
    "while step < n_steps:\n",
    "    \n",
    "    # update and execute activities\n",
    "    for persona in personas:\n",
    "        curr_tile = persona.get_curr_tile()\n",
    "        new_tile = persona.advance_one_step(maze, personas, curr_time)\n",
    "        movements[persona.name] = new_tile\n",
    "\n",
    "    # update location\n",
    "    for persona in personas:\n",
    "        new_tile = movements[persona.name]\n",
    "        if new_tile:            \n",
    "            maze.remove_subject_events_from_tile(persona.name, curr_tile)\n",
    "            maze.add_event_from_tile(persona.scratch.get_curr_event_and_desc(), new_tile)\n",
    "            persona.move(new_tile)\n",
    "        \n",
    "        tile_path = maze.get_tile_path(persona.scratch.curr_tile, level='object')\n",
    "        string = f\"{curr_time.strftime('%H:%M')} {persona.name} {persona.scratch.curr_tile} {persona.scratch.act_description} {persona.scratch.act_address} {tile_path}\"\n",
    "        print_to_file(string, SIM_LOGFILE)\n",
    "    \n",
    "    step += 1\n",
    "    curr_time = sim_start_time + timedelta(seconds=seconds_per_step*step)\n",
    "    print(curr_time.strftime(\"%H:%M\"),)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6e7de3-cd65-419f-8100-3e2387e275c2",
   "metadata": {},
   "source": [
    "Head to the log files to observe the conversations between agents, their movements across the maze, the prompts sent to the LLMs, and how the LLMs are managing the scheduling.\n",
    "\n",
    "That concludes this tutorial series. While we've covered the basics to make the original Simulacra code more accessible, there's much more to explore. For those interested in deepening their understanding, I encourage you to delve into the original codebase and accompanying paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
