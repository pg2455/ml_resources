{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMTime - Zero-shot prompting LLMs for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the use of zero-shot prompting with Large Language Models (LLMs) for time series forecasting, following the approach by Gruver et al. at NeurIPS 2023.\n",
    "\n",
    "We will leverage advanced pre-trained models like GPT-3, which offer powerful probabilistic tools, including likelihood evaluation and sampling. Remarkably, LLMs can be applied directly to time series data without any fine-tuning, enabling zero-shot learning. Additionally, LLMs can generate explanations for their predictions, enhancing our understanding of their outputs.\n",
    "\n",
    "But why are LLMs effective for time series forecasting? Authors provide the answer in LLM's preference for simplicity (Occam's razor) - LLMs tend to favor simple or repetitive patterns. This aligns well with common time series features like seasonality.\n",
    "\n",
    "However, applying LLMs to time series data presents unique challenges compared to traditional language modeling:\n",
    "\n",
    "1. **Numerical Sequences**: Time series data consists of numerical values, not words.\n",
    "2. **Complex Probability Distributions**: Language models excel at representing discrete distributions, whereas time series require continuous distributions.\n",
    "3. **Tokenization Variability**: The representation of numbers can differ based on the tokenizer. For example, '4223560' might be tokenized as [422, 35, 630] using the GPT tokenizer. This variability can impact model performance. To address this, the LLaMA tokenizer, as highlighted by Touvron et al. in 2023, maps numbers to individual digits, significantly enhancing the model's mathematical capabilities.\n",
    "\n",
    "Through this tutorial, we aim to explore potential of using LLMs in time series forecasting. We will use weather forecasting to experiment with this method.\n",
    "\n",
    "Notable differences from a typical machine learning pipeline are as follows -\n",
    "1. We don't need any sophisticated ML libraries such as scikit or PyTorch. \n",
    "2. We don't need high performance computing infrastructure. CPUs will do just fine. \n",
    "3. Since we will be querying OpenAI's API, it is crucial to think about the parameters to reduce the API costs. \n",
    "4. Most of the work goes in forming the prompt and decoding the outputs as compared to thinking about modeling choices in ML. \n",
    "\n",
    "**Note 1:** Although Gruver et al. 2023 suggests a methodology for both deterministic and probabilistic time series forecasting, we will be only focused on building this pipeline for deterministic forecasting. We will also limit our models to GPT-3. Please read the [publication](!https://arxiv.org/abs/2310.07820) and look at the [Github repo](!https://github.com/ngruver/llmtime/tree/main) for details into other models. \n",
    "\n",
    "**Note 2:** Throughout the tutorial, there are comments marked as `### YOUR CODE HERE` where the students are asked to fill in the code. This is to ensure that the students actively engage with the material and apply their learning to practical examples, reinforcing their understanding and skills in a hands-on manner.\n",
    "\n",
    "---\n",
    "\n",
    "[[Gruver et al. 2023](!https://arxiv.org/abs/2310.07820)] Large Language Models are Zero Shot Time Series Forecasters (NeurIPS 2023)\n",
    "\n",
    "[[Touvron et al. 2023](!https://arxiv.org/abs/2302.13971)] Llama: Open and effecient foundation language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Objectives\n",
    "In this tutorial, we aim to:\n",
    "1. Acquaint you with the application of machine learning techniques using Large Language Models (LLMs).\n",
    "2. Enhance your understanding of LLMs and the parameters influencing their behavior.\n",
    "3. Guide you through the essentials for successful time series prediction with LLMs.\n",
    "4. Translate knowledge from transformers to the realm of LLMs.\n",
    "\n",
    "### Task: Time Series Analysis of Weather Data\n",
    "- **Objective**: Predict the average maximum temperature (T_max) for the upcoming weeks. Our goal is to forecast average T_max up to 6 months (24 weeks) ahead.\n",
    "- **Evaluation**: The predictions will be assessed using the Mean Absolute Error (MAE) metric.\n",
    "- **Validation and Tuning**: We'll utilize Negative Log-Likelihood per Dimension (NLL/D) for hyperparameter tuning and validation.\n",
    "\n",
    "#### Caveats: Mindful API Usage\n",
    "Each API call incurs a cost. It's crucial to be mindful of the frequency of calls and the parameters used, as they can quickly add to the overall expense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Install the following libraries:\n",
    "\n",
    "1. `tiktoken`: Library containing tokenizers for the majority of LLMs. This tutorial has been built using `tiktoken==0.5.1`\n",
    "2. `openai`: Library to call OpenAI's API. We will be using GPT-3 mostly. This tutorial has been built using `openai==1.2.2`\n",
    "3. `jax`: Library to compute gradient of a transformation. Only used once in the tutorial. This tutorial has been built using `jax==0.4.18`\n",
    "\n",
    "\n",
    "**Setup OpenAI API access:** To set up your OpenAI API access and begin using it for your projects, follow these  steps:\n",
    "\n",
    "1. **Create an OpenAI Account**: Visit the OpenAI Platform website at [platform.openai.com](https://platform.openai.com) and sign in or create a new account if you don't already have one.\n",
    "\n",
    "2. **Generate an API Key**: After logging in, click your profile icon located at the top-right corner of the page. Select \"View API Keys\" from the dropdown menu. Then, you'll find an option to \"Create New Secret Key\". Click this button to generate your new API key. Save your API key immediately after generation, as you won't be able to view it again once the window showing it closes.\n",
    "\n",
    "3. **Free Credits and Billing Information**: As a new user, you will receive $5 worth of free credit upon creating your API key. This credit expires after three months. After using up this credit or upon its expiration, you can enter your billing information to continue using the API. If you don't provide billing information, you will retain login access but won't be able to make further API requests. \n",
    "\n",
    "4. **Storing Your API Key**: Store your API key securely. It's a common practice to put the key in a `.env` file within your project. For this tutorial, we can either provide it manually or put the key in environment variable by using `export OPEN_API_KEY=YOUR_KEY`. Please consult your tutor for the right way to access the API. \n",
    "\n",
    "By following these steps, you can successfully set up and start using OpenAI's API for your projects. Remember to handle your API key with care, as it provides access to OpenAI's powerful AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from jax import vmap, grad # Only used for computing log-likelihood\n",
    "\n",
    "client = OpenAI(\n",
    "#     api_key = \"PUT YOUR KEY HERE\"\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# information regarding specific LLMs\n",
    "MODELS = {\n",
    "    'GPT-3': {\n",
    "        'model_name': 'text-davinci-003', \n",
    "        'context_length': 4097\n",
    "    }\n",
    "}\n",
    "\n",
    "# Other Parameters\n",
    "LLM_MODEL = 'GPT-3'\n",
    "MODEL_NAME = MODELS[LLM_MODEL]['model_name']\n",
    "CONTEXT_LENGTH = MODELS[LLM_MODEL]['context_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series data for weather forecasting\n",
    "\n",
    "We are interested in weather forecasting. Data is provided in `data/` folder. The following code block loads the data for time series to be used for training and testing.\n",
    "\n",
    "Please follow the steps to ensure you understand what's loaded in `y_train` and `y_test`. \n",
    "\n",
    "As a benchmark, we also have `predictions.csv` file with predictions from seasonal arima and ground truth time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training/Testing dataframes: \n",
      "\n",
      "   Unnamed: 0        Date  tmax  tmin  prcp\n",
      "0       36001  1992-07-26  88.0  63.0   0.0\n",
      "1       36002  1992-07-27  88.0  64.0   0.0\n",
      "2       36003  1992-07-28  88.0  64.0   0.0\n",
      "3       36004  1992-07-29  89.0  64.0   0.0\n",
      "4       36005  1992-07-30  88.0  65.0   0.0\n",
      "   Unnamed: 0        Date  tmax  tmin  prcp\n",
      "0       20001  1948-10-05  70.0  53.0   0.0\n",
      "1       20002  1948-10-06  83.0  52.0   0.0\n",
      "2       20003  1948-10-07  93.0  48.0   0.0\n",
      "3       20004  1948-10-08  97.0  50.0   0.0\n",
      "4       20005  1948-10-09  78.0  57.0   0.0\n",
      "\n",
      "\n",
      "Size of training set: 16000\n",
      "Size of test set: 4000\n",
      "\n",
      "\n",
      "Number of train observations, resampled by week: 2286\n",
      "Number of test observations, resampled by week: 573\n",
      "\n",
      "\n",
      "Training time series (first 10 values):\n",
      "[82.5        72.85714286 79.71428571 67.28571429 78.28571429 79.85714286\n",
      " 72.57142857 72.42857143 70.71428571 69.28571429]\n",
      "\n",
      "\n",
      "Testing time series (first 10 values):\n",
      "[88.         90.         90.14285714 97.14285714 96.71428571 90.14285714\n",
      " 85.57142857 85.71428571 92.28571429 97.71428571]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load in the dataset\n",
    "df_test = pd.read_csv('data/weather_test_data.csv')\n",
    "df_train = pd.read_csv('data/weather_train_data.csv')\n",
    "\n",
    "# Look at the test and training data\n",
    "print(\"Original Training/Testing dataframes: \\n\")\n",
    "print(df_test.head())\n",
    "print(df_train.head())\n",
    "print('\\n')\n",
    "\n",
    "print(f'Size of training set: {len(df_train)}')\n",
    "print(f'Size of test set: {len(df_test)}')\n",
    "print('\\n')\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "df_test['Date'] = pd.to_datetime(df_test['Date'])\n",
    "df_train['Date'] = pd.to_datetime(df_train['Date'])\n",
    "\n",
    "# Resample by week and take the mean\n",
    "df_train = df_train.resample('W', on='Date').mean()\n",
    "df_test = df_test.resample('W', on='Date').mean()\n",
    "print(f'Number of train observations, resampled by week: {len(df_train)}')\n",
    "print(f'Number of test observations, resampled by week: {len(df_test)}')\n",
    "print('\\n')\n",
    "\n",
    "y_train = df_train['tmax'].values\n",
    "# t_train = df_train.index\n",
    "# y_train = y_train.reshape((len(y_train), 1))\n",
    "\n",
    "y_test = df_test['tmax'].values\n",
    "# t_test = df_test.index\n",
    "# y_test = y_test.reshape((len(y_test), 1))\n",
    "\n",
    "print(\"Training time series (first 10 values):\")\n",
    "print(y_train[:10])\n",
    "print('\\n')\n",
    "\n",
    "print(\"Testing time series (first 10 values):\")\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the benchmark data and ensure that the `test` column contains the same values as processed from above. The additional columns are predictions from the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>seasonal_arima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>86.502254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>86.914775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>90.142857</td>\n",
       "      <td>86.378422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>97.142857</td>\n",
       "      <td>86.677825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>96.714286</td>\n",
       "      <td>87.537957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           test  seasonal_arima\n",
       "2286  88.000000       86.502254\n",
       "2287  90.000000       86.914775\n",
       "2288  90.142857       86.378422\n",
       "2289  97.142857       86.677825\n",
       "2290  96.714286       87.537957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the `predcitions.csv` data in `benchmarks`\n",
    "\n",
    "benchmarks = pd.read_csv('predictions.csv', index_col=0)\n",
    "\n",
    "benchmarks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow these steps to establish the forecasts. \n",
    "\n",
    "<img src='img/llmtime.png' width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Data Preparation for LLM Analysis\n",
    "\n",
    "#### Step 1: Normalization: \n",
    "\n",
    "Scale the numbers in the time series. The authors propose to utilize Min-Max scaling with a twist.\n",
    "\n",
    "**Procedure**:\n",
    "  - Allow \\(\\alpha\\) percentile of numbers in the training time series to be below 1.\n",
    "  - Optionally, offset the time series by \\(\\beta\\) times the range of the series before scaling.\n",
    "  - Choose \\(\\alpha\\) and \\(\\beta\\) as hyperparameters.\n",
    "\n",
    "#### Step 2: Serializing\n",
    "Standardize the decimal precision of numbers.\n",
    "\n",
    "**Procedure**:\n",
    "  - Set `prec` as the number of digits allowed after the decimal.\n",
    "  - Note: The number of digits before the decimal is not fixed.\n",
    "\n",
    "#### Step 3: Truncation/Pre-processing the Input\n",
    "Adapt the time series for the LLM's input constraints.\n",
    "\n",
    "**Procedure**:\n",
    "  - Form a time series string from the processed numbers.\n",
    "  - If the string exceeds the model's context limit, truncate it appropriately.\n",
    "  - Iteratively determine the optimal input string length for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take sometime to go through the following classes and the `get_scaler` function to understand the scaling scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/ngruver/llmtime/blob/main/models/llmtime.py\n",
    "\n",
    "@dataclass\n",
    "class SerializerSettings:\n",
    "    \"\"\"\n",
    "    Settings for serialization of numbers. \n",
    "    \"\"\"\n",
    "    prec: int = 3 # number of digits after the decimal\n",
    "    base: int = 10 \n",
    "    signed: bool = True # Whether the inputs to the LLM indicate sign of the numbers.\n",
    "    max_val: bool = 1e7 # This is used in deciding the number of digits before the decimal point\n",
    "    time_sep: str = ' ,' # How to delimit each time step\n",
    "    bit_sep: str = ' ' # How to separate each digit. It depends on the tokenizer.\n",
    "    plus_sign: str = '' \n",
    "    minus_sign: str = ' -'\n",
    "    missing_str: str = ' Nan' # How to represent missing entries\n",
    "    half_bin_correction: bool = True # To adapt the discrete distribution to continuos distribution, we do bin correction such that if the prediction is 0.12, corrected version will be 0.125. \n",
    "        \n",
    "@dataclass\n",
    "class Scaler:\n",
    "    \"\"\"\n",
    "    Represents a data scaler with transformation and inverse transformation functions.\n",
    "\n",
    "    Attributes:\n",
    "        transform (callable): Function to apply transformation.\n",
    "        inv_transform (callable): Function to apply inverse transformation.\n",
    "    \"\"\"\n",
    "    transform: callable = lambda x: x\n",
    "    inv_transform: callable = lambda x: x    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/ngruver/llmtime/blob/main/models/llmtime.py\n",
    "\n",
    "def get_scaler(time_series: np.array, scaler_type: str = 'advanced', alpha: float = 0.95, beta: float = 0.3):\n",
    "    \"\"\"\n",
    "    Generates a Sclaer object based on the values in time_series.\n",
    "    \n",
    "    Args:\n",
    "        time_series: 1D array Data to base scaling on.\n",
    "        scaler_type: Type of scaler to be used.\n",
    "        alpha: Quantile for scaling.\n",
    "        beta: Shift parameter.\n",
    "    \"\"\"\n",
    "    if scaler_type == 'gaussian':\n",
    "        mean = time_series.mean()\n",
    "        std = time_series.std()\n",
    "\n",
    "        def transform(x):\n",
    "            return (x - mean)/std\n",
    "        \n",
    "        def inv_transform(x):\n",
    "            return std * x + mean\n",
    "\n",
    "    \n",
    "    elif scaler_type == 'basic':\n",
    "        # Time series is scaled by the alpha quantile of absolute values\n",
    "        data = time_series[~np.isnan(time_series)]\n",
    "        q = np.maximum(np.quantile(np.abs(data), alpha), 0.01)\n",
    "\n",
    "        def transform(x):\n",
    "            return x / q\n",
    "\n",
    "        def inv_transform(x):\n",
    "            return q * x\n",
    "\n",
    "    elif scaler_type == 'advanced':\n",
    "        # Time series is shifted by beta * range and scaled by the alpha quantile\n",
    "        min_ = np.min(time_series) - beta*(np.max(time_series)-np.min(time_series))\n",
    "        q = np.quantile(time_series-min_, alpha)\n",
    "        if q == 0:\n",
    "            q = 1\n",
    "\n",
    "        def transform(x):\n",
    "            return (x - min_) / q\n",
    "\n",
    "        def inv_transform(x):\n",
    "            return x * q + min_\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized `scaler_type`: {scaler_type}.')\n",
    "    \n",
    "    return Scaler(transform=transform, inv_transform=inv_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92696629 0.81861958 0.89566613 0.75601926 0.87961477 0.89727127\n",
      " 0.81540931 0.81380417 0.79454254 0.77849117]\n"
     ]
    }
   ],
   "source": [
    "# basic scaler should have alpha quantile as 1.0 (assuming all values are positive)\n",
    "val = y_train\n",
    "assert np.all(val > 0), \"Some values are negative\"\n",
    "scaler = get_scaler(val, 'basic', alpha=0.90)\n",
    "assert np.quantile(scaler.transform(val), 0.90) == 1.0\n",
    "\n",
    "print(scaler.transform(val)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2bits(val: np.array, settings: SerializerSettings):\n",
    "    \"\"\"\n",
    "    Converts each value in a time series to it's bit representation.\n",
    "    \n",
    "    Args:\n",
    "        val: Series to be converted to string format.\n",
    "        settings: \n",
    "    \n",
    "    Returns:\n",
    "        np.array: A 2D array of with each row consisting of (max_bit_pos + prec) values\n",
    "    \n",
    "    Examples (assuming max_bit_pos=4 and settings.prec=3):\n",
    "        0.123 --> 0000123\n",
    "        1.23  --> 0001230\n",
    "        1.2345 -> 0001234\n",
    "    \"\"\"\n",
    "    base = float(settings.base)\n",
    "    max_bit_pos = int(np.ceil(np.log(settings.max_val) / np.log(base)))\n",
    "    \n",
    "    before_decimals = []\n",
    "    for i in range(max_bit_pos):\n",
    "        digit = (val / base**(max_bit_pos - i - 1)).astype(int)\n",
    "        before_decimals.append(digit)\n",
    "        val -= digit * base**(max_bit_pos - i - 1)\n",
    "\n",
    "    before_decimals = np.stack(before_decimals, axis=-1)\n",
    "    \n",
    "    if settings.prec > 0:\n",
    "        after_decimals = []\n",
    "        for i in range(settings.prec): \n",
    "            digit = (val / base**(-i - 1)).astype(int)\n",
    "            after_decimals.append(digit)\n",
    "            val -= digit * base**(-i - 1)\n",
    "\n",
    "        after_decimals = np.stack(after_decimals, axis=-1)\n",
    "        digits = np.concatenate([before_decimals, after_decimals], axis=-1)\n",
    "    \n",
    "    return digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with example\n",
    "settings = SerializerSettings()\n",
    "num2bits(np.array([0.123, 1.23, 1.2345], dtype=np.float32), settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_series(val: np.array, settings: SerializerSettings, max_bit_pos: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Serializes the series into strings to be used as an input to LLMs.\n",
    "    \n",
    "    Args:\n",
    "        val: 1D-array Time series to be serialized.\n",
    "        settings: Settings to use for serialization.\n",
    "        max_bit_pos: Maximum number of bits before the decimal in bit representation of numbers.\n",
    "    \"\"\"\n",
    "    assert val.ndim == 1, f'Expected 1D array, but got {val.ndim}D-array.'\n",
    "    \n",
    "    signs = 1 * (val >= 0) - 1 * (val < 0)\n",
    "    is_missing = np.isnan(val)\n",
    "\n",
    "    y_train_bits = num2bits(np.abs(val), settings)\n",
    "   \n",
    "    bit_strs = []\n",
    "    for sign, missing, digit in zip(signs, is_missing, y_train_bits):\n",
    "\n",
    "        # remove zeros\n",
    "        nonzero_indices = np.where(digit != 0)[0]\n",
    "        if len(nonzero_indices) == 0:\n",
    "            digit = np.array([0])\n",
    "        else:\n",
    "            digit = digit[nonzero_indices[0]: ]\n",
    "\n",
    "        # make a string for this digit by adding bit_sep and concatenating\n",
    "        digit = ''.join([settings.bit_sep + str(b) for b in digit])\n",
    "\n",
    "        # append the sign (+ or -)\n",
    "        sign_sep = settings.plus_sign if sign == 1 else settings.minus_sign\n",
    "\n",
    "        # missing\n",
    "        if missing:\n",
    "            bit_strs.append(settings.missing_str)\n",
    "        else:\n",
    "            bit_strs.append(sign_sep + digit)\n",
    "    \n",
    "    series = settings.time_sep.join(bit_strs) + settings.time_sep\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series: [0.90410959 0.79843444 0.87358121 0.73737769 0.85792564 0.87514677\n",
      " 0.79530333 0.79373777 0.77495108 0.7592955 ]\n",
      "String:  9 0 4 , 7 9 8 , 8 7 3 , 7 3 7 , 8 5 7 , 8 7 5 , 7 9 5 , 7 9 3 , 7 7 4 , 7 5 9 , 6 6 2 , 6 4 3 , 6 7\n"
     ]
    }
   ],
   "source": [
    "time_series = y_train\n",
    "scaler = get_scaler(time_series, 'basic')\n",
    "vals = scaler.transform(time_series)\n",
    "series_string = serialize_series(vals, settings)\n",
    "\n",
    "print(\"Series:\", vals[:10])\n",
    "print(\"String:\", series_string[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Truncation/Pre-processing the Input\n",
    "\n",
    "In this section of the tutorial, we focus on ensuring that the input number of tokens and the ***expected number of generated tokens*** do not exceed the model's context length. Given that time series representations can be extensive, the authors employ a strategy to manage this effectively:\n",
    "1. **Tokenizer Access**\n",
    "   - Access to a tokenizer is crucial to understand how our prompt will be tokenized.\n",
    "   - Note: GPT-3.5/4 does not provide direct access to the tokenizer. We can use existing tokenizers as a fallback for approximation.\n",
    "   - We will use `tiktoken` library for this purpose\n",
    "\n",
    "\n",
    "2. **Iterative Selection for Input Series**\n",
    "   - We will iteratively select the number of entries in the input time series.\n",
    "   - This ensures that the output has the required number of entries and fits within the context length.\n",
    "\n",
    "This approach allows us to tailor our time series data to fit within the constraints of the LLM, ensuring both the input and its generated output remain within the permissible token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Tokenizes a given `string` as per the model specified by `model_name`.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return encoding.encode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokes in the series_string: 9259. Context length: 4097\n",
      "First 10 tokens: [860, 657, 604, 837, 767, 860, 807, 837, 807, 767]\n"
     ]
    }
   ],
   "source": [
    "series_tokens = tokenize(series_string, MODEL_NAME)\n",
    "print(f\"Total number of tokes in the series_string: {len(series_tokens)}. Context length: {CONTEXT_LENGTH}\")\n",
    "print(f\"First 10 tokens: {series_tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `series_string` is represented in 9K tokens, clearly we need to truncate it to fit it well within the context limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate input \n",
    "def truncate_input(series_string: str, time_series: np.array, settings: SerializerSettings, \n",
    "                   context_length: int, n_steps_to_predict: int = 12, \n",
    "                   step_multiplier: float = 1.2, model_name: str = 'text-davinci-003', verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Truncates the string such that the total number of tokens in the input and the expected output does not exceed context length. \n",
    "    \n",
    "    Args:\n",
    "        series_string: String for the time series as serialized by `serialize_series`.\n",
    "        time_series: Original time series.\n",
    "        settings: Settings for serialization of time series.\n",
    "        context_length: Maximum number of tokens allowed in LLM.\n",
    "        n_steps_to_predict: number of time steps in the future to predict.\n",
    "        step_multiplier: A multiplier for estimation of tokens per time step in the output\n",
    "        model_name: LLM to use.\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        series_str (str): A string format of the truncated time series \n",
    "        series (np.array): A truncated time series that will be used as an input\n",
    "        avg_token_per_chunk (float): Number of tokens per time step\n",
    "    \"\"\"\n",
    "    series_chunks = series_string.split(settings.time_sep)\n",
    "\n",
    "    for i in range(len(series_chunks)):\n",
    "        truncated_series_str = settings.time_sep.join(series_chunks[i:])\n",
    "        if not truncated_series_str.endswith(settings.time_sep):\n",
    "            truncated_series_str += settings.time_sep\n",
    "\n",
    "        input_tokens = tokenize(truncated_series_str, model_name)\n",
    "        \n",
    "        num_input_tokens = len(input_tokens)\n",
    "        avg_token_per_chunk = num_input_tokens / (len(series_chunks) - i)\n",
    "        total_expected_output_tokens = n_steps_to_predict * avg_token_per_chunk * step_multiplier\n",
    "        \n",
    "        total_token_length = num_input_tokens + total_expected_output_tokens\n",
    "        if total_token_length < context_length:\n",
    "            truncated_time_series = time_series[i:]\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Discarding old {i} values in the series\")\n",
    "        print(f\"Number of entries in the final input time series: {len(truncated_time_series)} representable in {num_input_tokens} tokens\")\n",
    "        print(f\"Average tokens per time step: {avg_token_per_chunk}\")\n",
    "\n",
    "    return truncated_series_str, truncated_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding old 1299 values in the series\n",
      "Number of entries in the final input time series: 987 representable in 3999 tokens\n",
      "Average tokens per time step: 4.047570850202429\n"
     ]
    }
   ],
   "source": [
    "input_string, input_time_series = truncate_input(series_string, time_series, settings, CONTEXT_LENGTH, 20, 1.2, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing as a function\n",
    "\n",
    "Let's package all of the above in a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_series(series: np.array, scaler: Scaler, settings: SerializerSettings, truncate: bool,\n",
    "                      context_length: int = 4097, n_steps_to_predict: int = 10, step_multiplier: float = 1.2, \n",
    "                      model_name: str = 'text-davinci-003', verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Preprocesses the time series into the input for LLM.\n",
    "    \n",
    "    Args:\n",
    "        series: Numerical time series to preprocess into the LLM prompt.\n",
    "        scaler: Normalization function for the time series. \n",
    "        settings: Serializer settings for the normalized time series.\n",
    "        truncate: Whether to truncate the resulting string to fit the context length. \n",
    "        context_length: Maximum number of tokens allowed in an LLM. Used only when truncate=True\n",
    "        n_steps_to_predict: Number of future steps to predict. Used only when truncate=True.\n",
    "        step_multiplier: A factor to estimate expected output tokens. Used only when truncate=True.\n",
    "        model_name: LLM name.\n",
    "        verbose: \n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed string for the truncated time series\n",
    "        np.array: truncated time series\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the series\n",
    "    normalized_series = scaler.transform(series)\n",
    "    \n",
    "    # Serialize the series\n",
    "    series_str = serialize_series(normalized_series, settings)\n",
    "    \n",
    "    if not series_str.endswith(settings.time_sep):\n",
    "        series_str += settings.time_sep\n",
    "\n",
    "    # Truncate the series\n",
    "    if truncate:\n",
    "        truncated_series_str, truncated_series = truncate_input(series_str, series, settings, context_length, n_steps_to_predict, step_multiplier, model_name, verbose)\n",
    "        return truncated_series_str, truncated_series\n",
    "    \n",
    "    return series_str, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting LLM for Time Series Forecasting\n",
    "\n",
    "We are now ready with the optimized prompt for time series forecasting. We can prompt the LLM to do time series forecasting. \n",
    "\n",
    "For this purpose, we'll utilize the `completion/create` endpoint of OpenAI. For a comprehensive understanding of the various options available with this endpoint, please refer to the [OpenAI documentation](https://platform.openai.com/docs/api-reference/completions/create).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_series(model_name: str, series_string: str, settings: SerializerSettings, n_steps_to_predict: int = 12, temp: float = 0.7, n_samples: int = 5):\n",
    "    \"\"\"\n",
    "    Calls LLMs to complete the series specified by series_string.\n",
    "    \n",
    "    Args:\n",
    "       model_name: Type of LLM.\n",
    "       series_string: Time series as represented by a string.\n",
    "       settings: Serializer settings.\n",
    "       n_steps_to_predict: number of time steps to predict\n",
    "       temp: Temperature parameter for sampling the output. Refer to the API. \n",
    "       n_samples: Number of output samples. Refer to the API. \n",
    "       \n",
    "    Returns:\n",
    "        A list of `n_samples` number of strings sampled from the LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_token_per_step = len(tokenize(series_string, model_name)) / len(series_string.split(settings.time_sep))\n",
    "    to_avoid_falling_short_factor = 1.05\n",
    "    \n",
    "    # Wherever possible, suppress unwanted outputs by exclusively selecting the relevant tokens\n",
    "    # This is only possible in non-chat models such as GPT-3/GPT-2\n",
    "    logit_bias = {}\n",
    "    if model_name == 'text-davinci-003':\n",
    "        allowed_tokens = [settings.bit_sep + str(i) for i in range(settings.base)]\n",
    "        allowed_tokens += [settings.time_sep, settings.plus_sign, settings.minus_sign]\n",
    "        allowed_tokens = [t for t in allowed_tokens if len(t) > 0]\n",
    "        logit_bias = {tokenize(t, model_name)[0]: 30 for t in allowed_tokens} \n",
    "\n",
    "    response = client.completions.create(\n",
    "        model = model_name,\n",
    "        prompt = series_string,\n",
    "        max_tokens = int(avg_token_per_step * n_steps_to_predict * to_avoid_falling_short_factor),\n",
    "        temperature = temp,\n",
    "        logit_bias = logit_bias,\n",
    "        n = n_samples,\n",
    "        seed = 1234, # Used for reproducibility, but the results might still differ from one call to the other\n",
    "    )\n",
    "\n",
    "    return [choice.text for choice in response.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding old 1290 values in the series\n",
      "Number of entries in the final input time series: 996 representable in 4035 tokens\n",
      "Average tokens per time step: 4.0471414242728185\n",
      " 9 1 4 , 9 8 9 , 9 8 6 , 9 8 4 , 9 8 0 , 9 5 4 , 8 8 7 , 8 8 6 , 8 8 4 , 8 3 4 , 8 5 6 , 8 5 0 , 8 0\n"
     ]
    }
   ],
   "source": [
    "scaler = get_scaler(y_train, 'basic', alpha=0.95)\n",
    "settings = SerializerSettings()\n",
    "n_steps_to_predict = 12\n",
    "step_multiplier = 1.2\n",
    "\n",
    "input_series_str, input_series = preprocess_series(y_train, scaler, settings, True, CONTEXT_LENGTH, n_steps_to_predict, step_multiplier, MODEL_NAME)\n",
    "\n",
    "# input_string, input_time_series, avg_token_per_chunk = truncate_input(series_string, time_series, settings, CONTEXT_LENGTH, n_steps_to_predict, step_multiplier, MODEL_NAME)\n",
    "sample_completions = complete_series(MODEL_NAME, input_series_str, settings, n_steps_to_predict, temp=0.7, n_samples=1)\n",
    "print(sample_completions[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing LLM Outputs to Retrieve Numerical Time Series Data\n",
    "\n",
    "After receiving outputs from the LLM, it's crucial to convert these back into a usable numerical format. This involves several steps:\n",
    "\n",
    "1. **Deserializing/Preprocess the Output String**\n",
    "   - Transform the output string back into a series format.\n",
    "   - This involves parsing the string to isolate each data point.\n",
    "   - Convert each element in the series from string to its numerical representation.\n",
    "\n",
    "\n",
    "3. **Reversing the Scaling Process**\n",
    "   - Use the previously applied scaler to convert the data back to its original scale.\n",
    "   - This reverses the normalization or scaling process applied during the initial data preparation.\n",
    "\n",
    "Thus, we can effectively transform the LLM's string outputs back into meaningful numerical time series data.\n",
    "\n",
    "#### Deserializing/Preprocess the Output String\n",
    "\n",
    "Read through the following function that converts the LLM output to the numerical series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert these predctions back to normal strings\n",
    "def llm_output_to_series(bit_str: str, settings: SerializerSettings):\n",
    "    \"\"\"\n",
    "    Converts the LLM output string to the numerical series.\n",
    "    \n",
    "    Args:\n",
    "        bit_str: LLM's output\n",
    "        settings: Serializer settings.\n",
    "        n_steps_to_predict: number of time steps to predict.\n",
    "    \n",
    "    Returns:\n",
    "        list of numerical values.\n",
    "    \"\"\"\n",
    "    output_strs = bit_str.split(settings.time_sep)\n",
    "    \n",
    "    output_strs = [a for a in output_strs if len(a) > 0] # remove the empty ones\n",
    "    output_strs = output_strs[:-1] # ignore the last one just so that the LLM stopped generating before the last one could be completed.\n",
    "\n",
    "    signs, output_series = [], []\n",
    "    for output in output_strs:\n",
    "        \n",
    "        # extracting string bits per time step\n",
    "        if output.startswith(settings.minus_sign):\n",
    "            sign = -1\n",
    "            digit_str = output[len(settings.minus_sign):]\n",
    "        else:\n",
    "            sign = 1\n",
    "            digit_str = output[len(settings.plus_sign):]\n",
    "\n",
    "        # extract bits\n",
    "        if settings.bit_sep == '':\n",
    "            bits = [b for b in digit_str.strip()]\n",
    "        else:\n",
    "            bits = [b for b in digit_str.strip().split(settings.bit_sep)]\n",
    "\n",
    "        # convert string bits to digits\n",
    "        digits = [int(b) for b in bits]\n",
    "\n",
    "        # convert the bits into numerical value in that base\n",
    "        base = float(settings.base)\n",
    "        D = len(digits)\n",
    "        digits_flipped = np.flip(np.array(digits), axis=-1)\n",
    "        powers = -np.arange(-settings.prec, -settings.prec + D)\n",
    "        val = np.sum(digits_flipped/base**powers, axis=-1)\n",
    "\n",
    "        if settings.half_bin_correction:\n",
    "            val += 0.5/base**settings.prec\n",
    "\n",
    "        output_series.append(sign * val)\n",
    "    \n",
    "    return np.array(output_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9145, 0.9895, 0.9865, 0.9845, 0.9805, 0.9545, 0.8875, 0.8865,\n",
       "        0.8845, 0.8345, 0.8565, 0.8505])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_series = [llm_output_to_series(bit_str, settings) for bit_str in sample_completions]\n",
    "output_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverting LLM Numerical Outputs to Original Scale\n",
    "\n",
    "Once we have received the numerical output from the LLM, the next step is to transform this data back into the scale of the original time series.\n",
    "For this, we will use the `scaler.inv_transform` method to invert the scaling transformation applied during preprocessing.\n",
    "\n",
    "**Note on Sample Variability**: LLM outputs can vary in length for the final steps of the time series. To address this:\n",
    "   - We compare the lengths of the series from various LLM samples.\n",
    "   - We will retain only the shortest length across all samples to maintain a consistent series length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_series(predicted_series: List, scaler: Scaler):\n",
    "    \"\"\"\n",
    "    Rescales the series back to the original scale.\n",
    "    \n",
    "    Args:\n",
    "        predicted_series:\n",
    "        scaler:\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Final predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for a in output_series:\n",
    "        predictions.append(scaler.inv_transform(a))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([83.448125, 90.291875, 90.018125, 89.835625, 89.470625, 87.098125,\n",
       "        80.984375, 80.893125, 80.710625, 76.148125, 78.155625, 77.608125])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rescale_series(output_series, scaler)\n",
    "predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: How well did we do?\n",
    "\n",
    "To asses how good are our predictions, we will use Mean Absolute Error (MAE) metric against the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(predictions, truth):\n",
    "    \"\"\"\n",
    "    Computes MAE.\n",
    "    \"\"\"\n",
    "    n_steps = min([len(l) for l in predictions])\n",
    "    predictions = np.array([a[:n_steps] for a in predictions])\n",
    "    abs_errors = np.abs(predictions - truth[:n_steps])\n",
    "    return np.average(abs_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.422008928571426"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mae(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: How to select right parameters to prompt LLMs?\n",
    "\n",
    "We have the working pipeline to perform forecasting. However, we still need to decide some of the parameters that will boost our time series forecasting performance. \n",
    "As a result, when working with LLMs for time series forecasting, fine-tuning hyperparameters is a crucial step. Here's a walkthrough of how to approach this:\n",
    "\n",
    "1. **Choosing Hyperparameters**: \n",
    "   - We have the following hyperparameters: \n",
    "     - $\\alpha$: Scaling parameter for pre-processing time series.\n",
    "     - $\\beta$: Offset parameter for pre-processing time series.\n",
    "     - `prec`: String representation parameter. Higher precision restricts the input length of time series. \n",
    "     - `temp`: LLM sampling parameter. Alternatively, use `top_p`. Refer to the documentation for more details.\n",
    "\n",
    "\n",
    "2. **Setting a Benchmark**: \n",
    "   - We'll use the Negative Log-Likelihood per Dimension (NLL/D) as our evaluation metric, based on the likelihood of the validation series given the training data, as detailed in Appendix A.2 of the Gruver et al. 2023.\n",
    "\n",
    "\n",
    "3. **Process for Tuning**: \n",
    "   - Divide the training data into two parts: training and validation.\n",
    "   - Test different combinations of hyperparameters on this split data. Our goal is to identify the combination that minimizes the NLL/D.\n",
    "\n",
    "### Negative Log-likelihood per Dimension (NLL/D)\n",
    "\n",
    "Detailed steps for this calculation will be provided, including a function in the subsequent section for practical implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Val data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding old 1281 values in the series\n",
      "Number of entries in the final input time series: 975 representable in 3951 tokens\n",
      "Average tokens per time step: 4.048155737704918\n"
     ]
    }
   ],
   "source": [
    "scaler = get_scaler(y_train, 'advanced', alpha=0.95, beta=0.3)\n",
    "settings = SerializerSettings()\n",
    "step_multiplier = 1.2\n",
    "val_length = 30 # validation step requires predicting `val_length` steps into the future\n",
    "\n",
    "train_series = y_train[:-val_length]\n",
    "validation_series = y_train[-val_length:]\n",
    "\n",
    "train_series_str, train_series_arr = preprocess_series(train_series, scaler, settings, True, CONTEXT_LENGTH, val_length, step_multiplier)\n",
    "\n",
    "\n",
    "validation_series_str, _ =  preprocess_series(validation_series, scaler, settings, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying LLM to predict the likelihood of validation string conditioned on train string\n",
    "\n",
    "To make this query, we will need to specify the following\n",
    "1. `max_tokens = 0`: Don't generate new tokens\n",
    "2. `logprobs=5`: Send the log probability of top 5 tokens per predicted token\n",
    "3. `echo=True`: Returns the input prompt in the output.\n",
    "3. `temperature=1`: This is a hyperparameter to be selected.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the LLM to extract the logits for each token\n",
    "full_series = train_series_str + validation_series_str\n",
    "\n",
    "response = client.completions.create(\n",
    "    model = MODEL_NAME,\n",
    "    prompt = full_series,\n",
    "    logprobs = 5, \n",
    "    max_tokens = 0,\n",
    "    echo = True,\n",
    "    temperature = 1.0, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Negative Log-Likelihood per Dimension (NLL/D)\n",
    "\n",
    "The following two code blocks are there to help you understand the computation thorugh DIY. The entire computation is encapsulated in the function. One can refer to that function to check the right answers. \n",
    "\n",
    "##### Key Attributes in `response.choices[0].logprobs`\n",
    "1. **`token_logprobs`**: Contains log probabilities of predicted tokens.\n",
    "2. **`tokens`**: Tokens at each prediction step.\n",
    "3. **`echo=True`**:\n",
    "   - Requests the API to return the input prompt as tokenized by the LLM.\n",
    "   - As a result, log probability for each token is also returned, crucial for assessing the likelihood of observing the validation series.\n",
    "\n",
    "4. **`top_logprobs`**: \n",
    "   - Shows top log probabilities for tokens at each step.\n",
    "   - Useful for understanding possible variations in LLM outputs.\n",
    "\n",
    "Delve into these attributes for a deeper understanding of the LLM's functioning.\n",
    "\n",
    "##### Note on Time Steps\n",
    "- Since LLM outputs don’t explicitly mark time steps, you'll need to infer the start of the validation string. This is done by matching the cumulative count of `settings.time_sep` with the length of the input series.\n",
    "\n",
    "##### Computing Procedure\n",
    "1. **Extract Log Probabilities**: Focus on tokens that pertain to the validation series.\n",
    "2. **Probability Adjustment**: Account for extraneous tokens. This involves tweaking probabilities to reflect modifications (like logit_bias adjustments) made to the model.\n",
    "3. **Aggregate Log Probabilities**: Combine these to represent the overall probability of predicting the validation series.\n",
    "4. **Continuous Distribution Adjustment**: Uniformly distribute likelihood over the bins in predicted ranges.\n",
    "5. **Final Adjustment**: Convert this probability back to the scale of the original input.\n",
    "\n",
    "This approach ensures an efficient and accurate calculation of NLL/D, providing valuable insights into your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = np.array(response.choices[0].logprobs.token_logprobs, dtype=np.float32)\n",
    "output_string_arr = np.array(response.choices[0].logprobs.tokens)\n",
    "top5logprobs = response.choices[0].logprobs.top_logprobs\n",
    "\n",
    "# Compute the starting point for the output string \n",
    "seps = output_string_arr == settings.time_sep\n",
    "val_start = np.argmax(np.cumsum(seps) == len(train_series_arr)) + 1\n",
    "\n",
    "\n",
    "# We are only interested in computing likelihood for the tokens predicted for validation_series\n",
    "val_logprobs = logprobs[val_start:]\n",
    "val_top5logprobs = top5logprobs[val_start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Pay attention to how the log probability of the chosen token might differ from the one listed in `top_logprobs`. The `logprobs` represent the log probability of tokens in `output_string_arr`, which corresponds to the validation series itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' 6 2 8 , 6 2 8 , 6 2 8 , 6 2 8 , 6 3 0 , 6 1 5 , 5 0 1 , 6 4 5 , 8 2 0 , 5 0 8 , 6 3 0 , 5 7 8 , 6 1 3 , 6 3 3 , 6 9 8 , 7 7 0 , 9 2 5 , 9 2 8 , 7 3 0 , 7 8 5 , 7 8 8 , 7 8 5 , 8 2 5 , 7 8 3 , 8 3 5 , 9 7 8 , 8 5 5 , 9 9 5 , 1 0 5 5 , 9 2 2 ,',\n",
       " ' 6 2 8 , 6 2 8 , 6 2 8 , 6 2 8 , 6 3 0 , 6 1 5 , 5 0 1 , 6 4 5 , 8 2 0 , 5 0 8 , 6 3 0 , 5 7 8 , 6 1 3 , 6 3 3 , 6 9 8 , 7 7 0 , 9 2 5 , 9 2 8 , 7 3 0 , 7 8 5 , 7 8 8 , 7 8 5 , 8 2 5 , 7 8 3 , 8 3 5 , 9 7 8 , 8 5 5 , 9 9 5 , 1 0 5 5 , 9 2 2 ,',\n",
       " {' 8': -1.3373731,\n",
       "  ' 0': -1.1371111,\n",
       "  ' 5': -1.3132795,\n",
       "  ' 3': -1.9262632,\n",
       "  ' 1': -6.7107368},\n",
       " -1.3373731)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(output_string_arr[val_start:]), validation_series_str, val_top5logprobs[2], val_logprobs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.545105934123211\n"
     ]
    }
   ],
   "source": [
    "# Probability Adjustment: adjust logprobs by removing extraneous tokens\n",
    "# Note: This adjustment rewards the model to think solely in terms of alllowed tokens\n",
    "allowed_tokens = [settings.bit_sep + str(i) for i in range(settings.base)] \n",
    "allowed_tokens += [settings.time_sep, settings.plus_sign, settings.minus_sign, settings.bit_sep]\n",
    "allowed_tokens = {t for t in allowed_tokens if len(t) > 0}\n",
    "\n",
    "p_extra = []\n",
    "for i in range(len(val_top5logprobs)):\n",
    "    x = sum(np.exp(ll) for k,ll in val_top5logprobs[i].items() if not (k in allowed_tokens)) \n",
    "    p_extra.append(x)\n",
    "p_extra = np.array(p_extra)\n",
    "\n",
    "if settings.bit_sep == '':\n",
    "    p_extra = 0\n",
    "\n",
    "adjusted_val_logprobs = val_logprobs - np.log(1-p_extra)\n",
    "\n",
    "# Aggregate Log Probabilities: Compute total logprobs per dimension\n",
    "digit_bits = -adjusted_val_logprobs.sum()\n",
    "loglikelihood_per_dimension = digit_bits/len(validation_series)\n",
    "\n",
    "# Continuous Distribution Adjustment: Adjust the discrete likelihood to continuous distribution by assuming bin over the range (See Page 5, 1st Para)\n",
    "transformed_nll = loglikelihood_per_dimension - settings.prec * np.log(settings.base)\n",
    "\n",
    "# Final Adjustment: Adjust the likelihood for input scaling (See Page 5, 1st Para)\n",
    "avg_logdet_dydx = np.log(vmap(grad(scaler.transform))(validation_series)).mean()\n",
    "\n",
    "nlld = transformed_nll - avg_logdet_dydx\n",
    "print(nlld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nlld(model_name: str, train_series_str: str, len_train_series: int, validation_series_str: str, validation_series: int,\n",
    "                 settings: SerializerSettings, scaler: Scaler, val_length: int, temp: float):\n",
    "    \"\"\"\n",
    "    Computes NLL/D metric based on how likely is the target_series conditioned on the input_series.\n",
    "    \n",
    "    Args:\n",
    "        model_name: LLM to use\n",
    "        train_series_str: Serialized input_series\n",
    "        len_train_series: Number of time steps in the train time series.\n",
    "        validation_series: validation series to be used for transformation.\n",
    "        len_validation_series: Number of time steps in the validation time series\n",
    "        settings: Serialization settings.\n",
    "        scaler: Scaler to define transformation of data.\n",
    "        val_length: Total number of steps to predict and check against the validation series.\n",
    "        temp: Temperature parameter for sampling in LLM.\n",
    "    \n",
    "    Returns:\n",
    "        float: NLL/D value.\n",
    "    \"\"\"\n",
    "    # Use LLM for zero-shot predictions\n",
    "    full_series = train_series_str + validation_series_str\n",
    "    response = client.completions.create(\n",
    "        model = model_name,\n",
    "        prompt = full_series,\n",
    "        logprobs = 5, \n",
    "        max_tokens = 0,\n",
    "        echo = True, # Send back the input string too.\n",
    "        temperature = temp, \n",
    "        seed = 1234, # for reproducibility\n",
    "    )\n",
    "    \n",
    "    logprobs = np.array(response.choices[0].logprobs.token_logprobs, dtype=np.float32)\n",
    "    output_string_arr = np.array(response.choices[0].logprobs.tokens) # expect the full string since echo=True\n",
    "    top5logprobs = response.choices[0].logprobs.top_logprobs\n",
    "\n",
    "    # Compute the starting point for the output string \n",
    "    seps = output_string_arr == settings.time_sep\n",
    "    val_start = np.argmax(np.cumsum(seps) == len_train_series) + 1\n",
    "    \n",
    "    val_logprobs = logprobs[val_start:]\n",
    "    val_top5logprobs = top5logprobs[val_start:]\n",
    "\n",
    "    # adjust logprobs by removing extraneous tokens\n",
    "    ## Note: This adjustment rewards the model to think solely in terms of alllowed tokens\n",
    "    allowed_tokens = [settings.bit_sep + str(i) for i in range(settings.base)] \n",
    "    allowed_tokens += [settings.time_sep, settings.plus_sign, settings.minus_sign, settings.bit_sep]\n",
    "    allowed_tokens = {t for t in allowed_tokens if len(t) > 0}\n",
    "    \n",
    "    p_extra = np.array([sum(np.exp(ll) for k,ll in val_top5logprobs[i].items() if not (k in allowed_tokens)) for i in range(len(val_top5logprobs))])\n",
    "\n",
    "    if settings.bit_sep == '':\n",
    "        p_extra = 0\n",
    "        \n",
    "    adjusted_val_logprobs = val_logprobs - np.log(1-p_extra)\n",
    "    \n",
    "    # Compute total logprobs per dimension\n",
    "    digit_bits = -adjusted_val_logprobs.sum()\n",
    "    loglikelihood_per_dimension = digit_bits/len(validation_series)\n",
    "    \n",
    "    # Adjust the discrete likelihood to continuous distribution (See Page 5, 1st Para)\n",
    "    transformed_nll = loglikelihood_per_dimension - settings.prec * np.log(settings.base)\n",
    "    \n",
    "    # Adjust the likelihood for input scaling (See Page 5, 1st Para)\n",
    "    avg_logdet_dydx = np.log(vmap(grad(scaler.transform))(validation_series)).mean()\n",
    "    \n",
    "    return transformed_nll - avg_logdet_dydx       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.545565420390499\n"
     ]
    }
   ],
   "source": [
    "scaler = get_scaler(train_series, 'advanced', alpha=0.95, beta=0.3)\n",
    "settings = SerializerSettings(prec=3)\n",
    "temp = 1.0\n",
    "\n",
    "val_nll = compute_nlld(MODEL_NAME, train_series_str, len(train_series_arr),\n",
    "             validation_series_str, validation_series, \n",
    "             settings, scaler, val_length, temp)\n",
    "\n",
    "print(val_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: Selecting the best parameters based on NLL/D\n",
    "\n",
    "We will perform a grid search over the parameters. \n",
    "\n",
    "**Caveat:** Depending on the number of hyperparameters, the API will be called that many times. This step might end up consuming a lot of API's credits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.2, 0.5, 0.8],\n",
    "    'beta': [0, 0.15],\n",
    "    'prec': [2],\n",
    "    'temp': [1.0]\n",
    "}\n",
    "\n",
    "hyperparams = model_selection.ParameterGrid(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding old 1224 values in the series\n",
      "Number of entries in the final input time series: 1032 representable in 3958 tokens\n",
      "Average tokens per time step: 3.831558567279768\n",
      "Hyper: {'alpha': 0.2, 'beta': 0, 'prec': 2, 'temp': 1.0, 'scaler': Scaler(transform=<function get_scaler.<locals>.transform at 0x7f2ab03e4ca0>, inv_transform=<function get_scaler.<locals>.inv_transform at 0x7f2ab03ce790>)}. Val NLL: 3.059033605755867\n",
      "\n",
      "Discarding old 1224 values in the series\n",
      "Number of entries in the final input time series: 1032 representable in 3958 tokens\n",
      "Average tokens per time step: 3.831558567279768\n",
      "Hyper: {'alpha': 0.2, 'beta': 0.15, 'prec': 2, 'temp': 1.0, 'scaler': Scaler(transform=<function get_scaler.<locals>.transform at 0x7f2aa7e1c310>, inv_transform=<function get_scaler.<locals>.inv_transform at 0x7f2aa7e1c5e0>)}. Val NLL: 3.0363467565903157\n",
      "\n",
      "Discarding old 1130 values in the series\n",
      "Number of entries in the final input time series: 1126 representable in 3968 tokens\n",
      "Average tokens per time step: 3.520851818988465\n",
      "Hyper: {'alpha': 0.5, 'beta': 0, 'prec': 2, 'temp': 1.0}. Val NLL: 3.360265032386769\n",
      "\n",
      "Discarding old 1130 values in the series\n",
      "Number of entries in the final input time series: 1126 representable in 3968 tokens\n",
      "Average tokens per time step: 3.520851818988465\n",
      "Hyper: {'alpha': 0.5, 'beta': 0.15, 'prec': 2, 'temp': 1.0}. Val NLL: 3.0845264643046786\n",
      "\n",
      "Discarding old 1016 values in the series\n",
      "Number of entries in the final input time series: 1240 representable in 3979 tokens\n",
      "Average tokens per time step: 3.2062852538275584\n",
      "Hyper: {'alpha': 0.8, 'beta': 0, 'prec': 2, 'temp': 1.0}. Val NLL: 3.148022681774325\n",
      "\n",
      "Discarding old 1016 values in the series\n",
      "Number of entries in the final input time series: 1240 representable in 3979 tokens\n",
      "Average tokens per time step: 3.2062852538275584\n",
      "Hyper: {'alpha': 0.8, 'beta': 0.15, 'prec': 2, 'temp': 1.0}. Val NLL: 3.0392527763721167\n",
      "\n",
      "Best Hyperparameters: {'alpha': 0.2, 'beta': 0.15, 'prec': 2, 'temp': 1.0, 'scaler': Scaler(transform=<function get_scaler.<locals>.transform at 0x7f2aa7e1c310>, inv_transform=<function get_scaler.<locals>.inv_transform at 0x7f2aa7e1c5e0>)} Vall NLL: 3.0363467565903157\n"
     ]
    }
   ],
   "source": [
    "best_val_nll = float('inf')\n",
    "best_hypers = None\n",
    "hyper_performance = []\n",
    "\n",
    "for param in hyperparams:\n",
    "    scaler = get_scaler(train_series, 'advanced', alpha=param['alpha'], beta=param['beta'])\n",
    "    settings = SerializerSettings(prec=param['prec'])\n",
    "    \n",
    "    # serialize input_series\n",
    "    train_series_str, train_series_arr = preprocess_series(train_series, scaler, settings, True, CONTEXT_LENGTH, val_length, step_multiplier, MODEL_NAME)\n",
    "    validation_series_str, _ =  preprocess_series(validation_series, scaler, settings, False)\n",
    "    \n",
    "    val_nll = compute_nlld(MODEL_NAME, train_series_str, len(train_series_arr),\n",
    "                 validation_series_str, validation_series, \n",
    "                 settings, scaler, val_length, param['temp'])\n",
    "    \n",
    "    if val_nll < best_val_nll:\n",
    "        best_val_nll = val_nll\n",
    "        best_hypers = param\n",
    "        best_hypers['scaler'] = scaler\n",
    "        \n",
    "    hyper_performance.append((param, val_nll))\n",
    "    print(f'Hyper: {param}. Val NLL: {val_nll}\\n')\n",
    "\n",
    "\n",
    "print(f'Best Hyperparameters: {best_hypers} Vall NLL: {best_val_nll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = []\n",
    "for k, v in hyper_performance:\n",
    "    perf.append({\n",
    "        'alpha': k['alpha'],\n",
    "        'beta': k['beta'],\n",
    "        'prec': k['prec'],\n",
    "        'temp': k['temp'],\n",
    "        'nll': v\n",
    "    })\n",
    "perf = pd.DataFrame(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>prec</th>\n",
       "      <th>temp</th>\n",
       "      <th>nll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.741351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.856490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.939739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.990336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.008049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.143080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.035694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.039253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.059034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.084526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.145988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.360265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  beta  prec  temp       nll\n",
       "1     0.2  0.00     3   1.0  1.741351\n",
       "9     0.8  0.00     3   1.0  1.856490\n",
       "7     0.5  0.15     3   1.0  1.939739\n",
       "5     0.5  0.00     3   1.0  1.990336\n",
       "3     0.2  0.15     3   1.0  2.008049\n",
       "11    0.8  0.15     3   1.0  2.143080\n",
       "2     0.2  0.15     2   1.0  3.035694\n",
       "10    0.8  0.15     2   1.0  3.039253\n",
       "0     0.2  0.00     2   1.0  3.059034\n",
       "6     0.5  0.15     2   1.0  3.084526\n",
       "8     0.8  0.00     2   1.0  3.145988\n",
       "4     0.5  0.00     2   1.0  3.360265"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.sort_values(by='nll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predctions using the best hyperparameters\n",
    "\n",
    "Finally, we will use the best hyperparameters for time series forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding old 1253 values in the series\n",
      "Number of entries in the final input time series: 1033 representable in 3959 tokens\n",
      "Average tokens per time step: 3.8288201160541586\n",
      "\n",
      "Minimum steps predicted across samples: 30\n",
      "\n",
      "estimates:\n",
      " [87.5355     88.68171429 87.15342857 87.02607143 88.80907143 83.078\n",
      " 85.62514286 84.73364286 86.89871429 83.71478571 81.93178571 82.1865\n",
      " 78.74785714 76.71014286 76.45542857 74.29035714 76.32807143 74.41771429\n",
      " 74.67242857 75.43657143 71.99792857 73.01678571 69.45078571 69.96021429\n",
      " 66.52157143 63.84707143 65.50271429 66.90364286 64.61121429 70.34228571]\n",
      "\n",
      "\n",
      "MAE:  5.32518 \t Model: seasonal_arima\n",
      "MAE:  5.09745 \t Model: GPT-3\n"
     ]
    }
   ],
   "source": [
    "input_series = y_train\n",
    "n_steps_to_predict = 30\n",
    "step_multiplier = 1.2\n",
    "HYPERS = {\n",
    "    'alpha': 0.2,\n",
    "    'beta': 0.15,\n",
    "    'prec': 2,\n",
    "    'temp': 1.0\n",
    "}\n",
    "\n",
    "scaler = get_scaler(train_series, 'advanced', alpha=HYPERS['alpha'], beta=HYPERS['beta'])\n",
    "settings = SerializerSettings(prec=HYPERS['prec'])\n",
    "\n",
    "input_series_str, input_series_arr = preprocess_series(input_series, scaler, settings, True, CONTEXT_LENGTH, n_steps_to_predict, step_multiplier, MODEL_NAME)\n",
    "\n",
    "sample_completions = complete_series(MODEL_NAME, input_series_str, settings, n_steps_to_predict, temp=HYPERS['temp'], n_samples=10)\n",
    "\n",
    "output_series = [llm_output_to_series(bit_str, settings) for bit_str in sample_completions]\n",
    "predictions = rescale_series(output_series, scaler)\n",
    "\n",
    "minimum_steps_predicted = min([len(l) for l in predictions])\n",
    "print(\"\\nMinimum steps predicted across samples:\", minimum_steps_predicted)\n",
    "predictions = np.array([a[:minimum_steps_predicted] for a in predictions])\n",
    "\n",
    "point_estimates = np.median(predictions, axis=0)\n",
    "print(\"\\nestimates:\\n\", point_estimates)\n",
    "\n",
    "benchmarks = pd.read_csv('predictions.csv', index_col=0)[: minimum_steps_predicted]\n",
    "\n",
    "benchmarks['GPT-3'] = point_estimates\n",
    "\n",
    "print('\\n')\n",
    "for col in benchmarks.columns:\n",
    "    if col == 'test':\n",
    "        continue\n",
    "    MAE = np.average(np.abs(benchmarks[col] - benchmarks['test']))\n",
    "    print(f\"MAE: {MAE: 0.5f} \\t Model: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection and Inquiry\n",
    "\n",
    "1. **Variability of Seed Parameter**: Reflect on how the seed parameter impacts the call's output. What variations do you notice when the seed isn't set?\n",
    "\n",
    "2. **Temperature Parameter's Influence**: How does the temperature setting affect the model's estimates? What changes occur when the temperature is set very low?\n",
    "\n",
    "3. **Impact of `prec` on Forecasts**: Examine how the precision (`prec`), particularly the number of digits after the decimal point, influences the forecasts.\n",
    "\n",
    "4. **Cost-Effectiveness of NLL/D vs. MAE**: Why might NLL/D be a more cost-effective metric for validation compared to MAE?\n",
    "\n",
    "5. **Adaptations for Different Models**: Consider how you would adjust this pipeline for other models like GPT-3.5/4 or LLaMA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
